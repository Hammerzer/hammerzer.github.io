<!DOCTYPE html><html lang="zh-CN"><head><script src="https://lib.sinaapp.com/js/jquery/1.7.2/jquery.min.js"></script><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/180-180.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/32-32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/16-16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hammerzer.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CONTENT OUTLINE  关于Python学习，每遇未知即记录！ Pytorch官网 Pytorch中文教程 | 社区翻译 Pytorch官网英文文档 Pytorch中文文档 Browse State-of-the-Art ：数据集下载"><meta property="og:type" content="article"><meta property="og:title" content="Tool-Pytorch"><meta property="og:url" content="https://hammerzer.github.io/2023/07/21/Tool-Pytorch/index.html"><meta property="og:site_name" content="Moustache&#39;s Blog"><meta property="og:description" content="CONTENT OUTLINE  关于Python学习，每遇未知即记录！ Pytorch官网 Pytorch中文教程 | 社区翻译 Pytorch官网英文文档 Pytorch中文文档 Browse State-of-the-Art ：数据集下载"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-9-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-9-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-9-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-9-4.png"><meta property="og:image" content="https://hammerzer.github.io/.io//pytorch-9-5.png"><meta property="article:published_time" content="2023-07-21T09:19:48.000Z"><meta property="article:modified_time" content="2025-01-19T03:19:15.940Z"><meta property="article:author" content="Moustache"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hammerzer.github.io/.io//pytorch-1.png"><link rel="canonical" href="https://hammerzer.github.io/2023/07/21/Tool-Pytorch/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Tool-Pytorch | Moustache's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Moustache's Blog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Moustache's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">小胡子的私人空间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">78</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hammerzer.github.io/2023/07/21/Tool-Pytorch/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/180-180.png"><meta itemprop="name" content="Moustache"><meta itemprop="description" content="我是小胡子"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Moustache's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Tool-Pytorch</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-07-21 17:19:48" itemprop="dateCreated datePublished" datetime="2023-07-21T17:19:48+08:00">2023-07-21</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 11:19:15" itemprop="dateModified" datetime="2025-01-19T11:19:15+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a> </span></span><span id="/2023/07/21/Tool-Pytorch/" class="post-meta-item leancloud_visitors" data-flag-title="Tool-Pytorch" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> <span>℃</span> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2023/07/21/Tool-Pytorch/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2023/07/21/Tool-Pytorch/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>25k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>22 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CONTENT-OUTLINE"><a href="#CONTENT-OUTLINE" class="headerlink" title="CONTENT OUTLINE"></a>CONTENT OUTLINE</h2><blockquote><p><span style="background:#ff0"></span></p><p>关于Python学习，每遇未知即记录！</p><p><a target="_blank" rel="noopener" href="http://pytorch.p2hp.com/">Pytorch官网</a></p><p><a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/">Pytorch中文教程 | 社区翻译</a></p><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">Pytorch官网英文文档</a></p><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">Pytorch中文文档</a></p><p><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota">Browse State-of-the-Art ：数据集下载</a></p></blockquote><span id="more"></span><h2 id="〇、目录"><a href="#〇、目录" class="headerlink" title="〇、目录"></a>〇、目录</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">参考文档</a></p><p><code>Pytorch</code>常用的包：</p><ol><li><code>torch</code>：张量的有关运算。如创建、索引、链接、转置、加减乘除、切片等</li><li><code>torch.Storage</code>：跟绝大部分基于连续存储的数据结构有关</li><li><code>torch.nn</code>：包含搭建神经网络层的模块和一系列loss函数。如全连接、卷积、BN批处理、dropout、crossentryloss、mseloss等；<code>torch.nn.function</code>：常用的激活函数relu、leaky_relu、sigmoid等</li><li><code>torch.optim</code>：各种参数优化方法，例如SGW、AdaGrad、Adam、RMSProp等</li><li><code>torch.autogard</code>：提供tensor所有操作的自动求导方法</li><li><code>torch.utils.data</code>：用于加载数据。</li><li><code>torchvision</code>：是pytorch中专门用来处理图像的库。包中常用的模块有<ul><li><code>torchvision.datasets</code>：用来进行数据加载的</li><li><code>Torchvision.models</code>：为我们提供已经训练好的模型，让我们加载之后可以直接使用。包括AlexNet、VGG、ResNet等网络模型</li><li><code>Torchvision.transforms</code>：为我们提供了一般的图像转换操作类</li><li><code>Torchvision.utils</code>：将给定的tensor保存成image文件</li></ul></li><li><code>from PIL import Image</code>：pytorch中处理图像使用的这几种格式<ul><li>PIL：使用python自带图像处理库读取出来的图片格式。</li><li>Numpy：使用python-onpenCV库读取出来的图片格式</li><li>Tensor：pytorch中训练是所采用的向量格式（也可以说是图片）。<br>而 from PIL import Image是在进行PIL与tensor的转换，也就是图片格式的转换</li></ul></li><li><code>matplotlib</code>：这是pytorch的一个绘图库，是python中常用的可视化工具之一，可以方便创建2D图标和一些基本的3D图表。基本的方法有 ：<ul><li><code>plt.figure()</code>:调用figure创建一个绘图对象</li><li><code>Plt.plot</code>：调用plot函数在当前的绘图对象中绘图</li><li><code>Plt.xlable()/plt.ylable</code>：设置x/y轴的文字</li><li><code>Plt.title()</code>：设置图标的标题</li><li><code>Plt.xlim()/plt.ylim()</code>：设置变量的范围，[格式为x/y的起点，终点]</li><li><code>Plt.axis()</code>：同时设置两个变量的范围，格式为[x的起点，x的终点，y的起点，y的终点]</li><li><code>Plt.legend()</code>：显示label中标记的图示</li><li><code>Plt.show()</code>：以上参数设置完成后，必须使用该方法显示出创建的所有绘图对象</li></ul></li></ol><br><h2 id="一、Torch"><a href="#一、Torch" class="headerlink" title="一、Torch"></a>一、Torch</h2><p>包 <code>torch</code> 包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p><p>它有CUDA 的对应实现，可以在NVIDIA GPU上进行张量运算(计算能力&gt;=2.0)。</p><h3 id="1-张量Tensors"><a href="#1-张量Tensors" class="headerlink" title="1 张量Tensors"></a>1 张量Tensors</h3><h4 id="1-1-torch-numel"><a href="#1-1-torch-numel" class="headerlink" title="1.1 torch.numel"></a>1.1 <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/#tensors">torch.numel</a></h4><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.numel<span class="function"><span class="params">(input)</span>-&gt;</span>int</span><br></pre></td></tr></table></figure><p>返回<code>input</code> 张量中的元素个数</p><ul><li>参数: input (<a target="_blank" rel="noopener" href="http://pytorch.org/docs/tensors.html#torch.Tensor"><em>Tensor</em></a>) – 输入张量</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">120</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.zeros(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.numel(a)</span><br><span class="line"><span class="number">16</span></span><br></pre></td></tr></table></figure><br><h4 id="1-2-torch-is-tensor"><a href="#1-2-torch-is-tensor" class="headerlink" title="1.2 torch.is_tensor"></a>1.2 torch.is_tensor</h4><figure class="highlight ceylon"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="keyword">is</span><span class="number">_</span>tensor(obj)</span><br></pre></td></tr></table></figure><p>如果<em>obj</em> 是一个pytorch张量，则返回True</p><ul><li>参数： obj (Object) – 判断对象</li></ul><br><h3 id="2-创建操作-Creation-Ops"><a href="#2-创建操作-Creation-Ops" class="headerlink" title="2 创建操作 Creation Ops"></a>2 创建操作 Creation Ops</h3><h4 id="2-1-torch-arange"><a href="#2-1-torch-arange" class="headerlink" title="2.1 torch.arange"></a>2.1 <strong>torch.arange</strong></h4><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="keyword">start</span>, <span class="keyword">end</span>, step=<span class="number">1</span>, <span class="keyword">out</span>=<span class="keyword">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个1维张量，长度为 <code>floor((end−start)/step)</code>。包含从<code>start</code>到<code>end</code>，以<code>step</code>为步长的一组序列值(默认步长为1)。</p><p>参数:</p><ul><li>start (float) – 序列的起始点</li><li>end (float) – 序列的终止点</li><li>step (float) – 相邻点的间隔大小</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"> <span class="number">1</span></span><br><span class="line"> <span class="number">2</span></span><br><span class="line"> <span class="number">3</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"> <span class="number">1.0000</span></span><br><span class="line"> <span class="number">1.5000</span></span><br><span class="line"> <span class="number">2.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">3</span>]</span><br></pre></td></tr></table></figure><br><h4 id="2-2-torch-ones"><a href="#2-2-torch-ones" class="headerlink" title="2.2 torch.ones"></a>2.2 torch.ones</h4><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.ones(*sizes, <span class="keyword">out</span>=<span class="keyword">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个全为1 的张量，形状由可变参数<code>sizes</code>定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状</li><li>out (Tensor, optional) – 结果张量 例子:</li></ul><figure class="highlight tap"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; torch.ones(2, 3)</span><br><span class="line"></span><br><span class="line"><span class="number"> 1 </span><span class="number"> 1 </span> 1</span><br><span class="line"><span class="number"> 1 </span><span class="number"> 1 </span> 1</span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure><br><h4 id="2-3-torch-zeros"><a href="#2-3-torch-zeros" class="headerlink" title="2.3 torch.zeros"></a>2.3 torch.zeros</h4><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(*sizes, <span class="keyword">out</span>=<span class="keyword">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个全为标量 0 的张量，形状由可变参数<code>sizes</code> 定义。</p><p>参数:</p><ul><li>sizes (int…) – 整数序列，定义了输出形状</li><li>out (<a target="_blank" rel="noopener" href="http://pytorch.org/docs/tensors.html#torch.Tensor">Tensor</a>, <em>optional</em>) – 结果张量</li></ul><blockquote><p>从 0.4 开始，此函数不支持 <code>out</code> 关键字。作为替代方案，旧的 <code>torch.zeros_like(input, out=output)</code> 等效于 <code>torch.zeros(input.size(), out=output)</code> 。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.empty(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><br><br><br><br><br><br><br><h3 id="3-随机抽样-Random-sampling"><a href="#3-随机抽样-Random-sampling" class="headerlink" title="3 随机抽样 Random sampling"></a>3 随机抽样 Random sampling</h3><h4 id="3-1-torch-normal"><a href="#3-1-torch-normal" class="headerlink" title="3.1 torch.normal()"></a>3.1 <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/#random-sampling">torch.normal()</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(means, std, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>返回一个张量，包含从给定参数<code>means</code>,<code>std</code>的离散正态分布中抽取随机数。 均值<code>means</code>是一个张量，包含每个输出元素相关的正态分布的均值。 <code>std</code>是一个张量，包含每个输出元素相关的正态分布的标准差。 均值和标准差的形状不须匹配，但每个张量的元素个数须相同。</p><p>参数:</p><ul><li>means (Tensor) – 均值</li><li>std (Tensor) – 标准差【沐神说的是方差，有出入，<span style="color:#b3de4b">待补充</span>】</li><li>out (Tensor) – 可选的输出张量</li></ul><blockquote><p>已用例子中，【见<em>线性回归的从零实现</em>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(均值，标准差，（样本数量 ，样本的长度为w的长度）)</span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></blockquote><h3 id="4-数学操作Math-operations"><a href="#4-数学操作Math-operations" class="headerlink" title="4 数学操作Math operations"></a>4 数学操作Math operations</h3><h4 id="4-1-Pointwise-Ops【逐点操作】"><a href="#4-1-Pointwise-Ops【逐点操作】" class="headerlink" title="4.1 Pointwise Ops【逐点操作】"></a>4.1 Pointwise Ops【逐点操作】</h4><h5 id="4-1-1-torch-mul"><a href="#4-1-1-torch-mul" class="headerlink" title="4.1.1 torch.mul"></a>4.1.1 torch.mul</h5><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/didi_ya/article/details/121158666">torch.matmul()</a></p><p><code>pytorch</code>中两个张量的乘法可以分为两种：</p><ul><li>两个张量对应元素相乘，在<code>PyTorch</code>中可以通过<code>torch.mul</code>函数（或<code>*</code>运算符）实现；</li><li>两个张量矩阵相乘，在<code>PyTorch</code>中可以通过<code>torch.matmul</code>函数实现；</li></ul><p><code>torch.matmul(input, other) → Tensor</code>：计算两个张量<code>input</code>和<code>other</code>的矩阵乘积【注意】：<code>matmul</code>函数没有强制规定维度和大小，可以用利用广播机制进行不同维度的相乘操作。</p><p><code>torch.matmul()</code>也是一种类似于矩阵相乘操作的<code>tensor</code>连乘操作。但是它可以利用<code>python</code>中的广播机制，处理一些维度不同的<code>tensor</code>结构进行相乘操作。这也是该函数与<code>torch.bmm()</code>区别所在。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mul(<span class="built_in">input</span>, value, out=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>用标量值<code>value</code>乘以输入<code>input</code>的每个元素，并返回一个新的结果张量。 <code>out=tensor∗value</code></p><p>如果输入是FloatTensor or DoubleTensor类型，则<code>value</code> 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，<code>value</code>取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 乘到每个元素的数</li><li>out (Tensor, optional) – 输出张量</li></ul><p><strong>两张量相乘</strong></p><p>两个张量<code>input</code>,<code>other</code>按元素进行相乘，并返回到输出张量。即计算outi=inputi∗otheri</p><p>两计算张量形状不须匹配，但总元素数须一致。 <strong>注意</strong>：当形状不匹配时，<code>input</code>的形状作为输入张量的形状。</p><p>参数：</p><ul><li>input (Tensor) – 第一个相乘张量</li><li>other (Tensor) – 第二个相乘张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line">-<span class="number">0.7280</span>  <span class="number">0.0598</span> -<span class="number">1.4327</span> -<span class="number">0.5825</span></span><br><span class="line">-<span class="number">0.1427</span> -<span class="number">0.0690</span>  <span class="number">0.0821</span> -<span class="number">0.3270</span></span><br><span class="line">-<span class="number">0.9241</span>  <span class="number">0.5110</span>  <span class="number">0.4070</span> -<span class="number">1.1188</span></span><br><span class="line">-<span class="number">0.8308</span>  <span class="number">0.7426</span> -<span class="number">0.6240</span> -<span class="number">1.1582</span></span><br><span class="line">[torch.FloatTensor of size 4x4]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">2</span>, <span class="number">8</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"></span><br><span class="line"> <span class="number">0.0430</span> -<span class="number">1.0775</span>  <span class="number">0.6015</span>  <span class="number">1.1647</span> -<span class="number">0.6549</span>  <span class="number">0.0308</span> -<span class="number">0.1670</span>  <span class="number">1.0742</span></span><br><span class="line">-<span class="number">1.2593</span>  <span class="number">0.0292</span> -<span class="number">0.0849</span>  <span class="number">0.4530</span>  <span class="number">1.2404</span> -<span class="number">0.4659</span> -<span class="number">0.1840</span>  <span class="number">0.5974</span></span><br><span class="line">[torch.FloatTensor of size 2x8]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mul(a, b)</span><br><span class="line"></span><br><span class="line">-<span class="number">0.0313</span> -<span class="number">0.0645</span> -<span class="number">0.8618</span> -<span class="number">0.6784</span></span><br><span class="line"> <span class="number">0.0934</span> -<span class="number">0.0021</span> -<span class="number">0.0137</span> -<span class="number">0.3513</span></span><br><span class="line"> <span class="number">1.1638</span>  <span class="number">0.0149</span> -<span class="number">0.0346</span> -<span class="number">0.5068</span></span><br><span class="line">-<span class="number">1.0304</span> -<span class="number">0.3460</span>  <span class="number">0.1148</span> -<span class="number">0.6919</span></span><br><span class="line">[torch.FloatTensor of size 4x4]</span><br></pre></td></tr></table></figure><br><h5 id="4-1-2-torch-cos"><a href="#4-1-2-torch-cos" class="headerlink" title="4.1.2 torch.cos"></a>4.1.2 torch.cos</h5><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cos(<span class="keyword">input</span>, <span class="keyword">out</span>=<span class="keyword">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回一个新张量，包含输入<code>input</code>张量每个元素的余弦。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>out (Tensor, optional) – 输出张量</li></ul><br><h5 id="4-1-3-torch-div"><a href="#4-1-3-torch-div" class="headerlink" title="4.1.3 torch.div()"></a>4.1.3 torch.div()</h5><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.div(<span class="keyword">input</span>, <span class="keyword">value</span>, <span class="keyword">out</span>=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>将<code>input</code>逐元素除以标量值<code>value</code>，并返回结果到输出张量<code>out</code>。 即 <code>out=tensor/value</code></p><p>如果输入是FloatTensor or DoubleTensor类型，则参数 <code>value</code> 必须为实数，否则须为整数。【译注：似乎并非如此，无关输入类型，<code>value</code>取整数、实数皆可。】</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>value (Number) – 除数</li><li>out (Tensor, optional) – 输出张量</li></ul><p><strong>对于两张量相除</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.div(<span class="keyword">input</span>, other, <span class="keyword">out</span>=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>两张量<code>input</code>和<code>other</code>逐元素相除，并将结果返回到输出。即， <code>outi=inputi/otheri</code></p><p>两张量形状不须匹配，但元素数须一致。</p><p>注意：当形状不匹配时，<code>input</code>的形状作为输出张量的形状。</p><p>参数：</p><ul><li>input (Tensor) – 张量(分子)</li><li>other (Tensor) – 张量(分母)</li><li>out (Tensor, optional) – 输出张量</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line">-<span class="number">0.1810</span>  <span class="number">0.4017</span>  <span class="number">0.2863</span> -<span class="number">0.1013</span></span><br><span class="line"> <span class="number">0.6183</span>  <span class="number">2.0696</span>  <span class="number">0.9012</span> -<span class="number">1.5933</span></span><br><span class="line"> <span class="number">0.5679</span>  <span class="number">0.4743</span> -<span class="number">0.0117</span> -<span class="number">0.1266</span></span><br><span class="line">-<span class="number">0.1213</span>  <span class="number">0.9629</span>  <span class="number">0.2682</span>  <span class="number">1.5968</span></span><br><span class="line">[torch.FloatTensor of size 4x4]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.randn(<span class="number">8</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"></span><br><span class="line"> <span class="number">0.8774</span>  <span class="number">0.7650</span></span><br><span class="line"> <span class="number">0.8866</span>  <span class="number">1.4805</span></span><br><span class="line">-<span class="number">0.6490</span>  <span class="number">1.1172</span></span><br><span class="line"> <span class="number">1.4259</span> -<span class="number">0.8146</span></span><br><span class="line"> <span class="number">1.4633</span> -<span class="number">0.1228</span></span><br><span class="line"> <span class="number">0.4643</span> -<span class="number">0.6029</span></span><br><span class="line"> <span class="number">0.3492</span>  <span class="number">1.5270</span></span><br><span class="line"> <span class="number">1.6103</span> -<span class="number">0.6291</span></span><br><span class="line">[torch.FloatTensor of size 8x2]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.div(a, b)</span><br><span class="line"></span><br><span class="line">-<span class="number">0.2062</span>  <span class="number">0.5251</span>  <span class="number">0.3229</span> -<span class="number">0.0684</span></span><br><span class="line">-<span class="number">0.9528</span>  <span class="number">1.8525</span>  <span class="number">0.6320</span>  <span class="number">1.9559</span></span><br><span class="line"> <span class="number">0.3881</span> -<span class="number">3.8625</span> -<span class="number">0.0253</span>  <span class="number">0.2099</span></span><br><span class="line">-<span class="number">0.3473</span>  <span class="number">0.6306</span>  <span class="number">0.1666</span> -<span class="number">2.5381</span></span><br><span class="line">[torch.FloatTensor of size 4x4]</span><br></pre></td></tr></table></figure><br><h4 id="4-2-Reduction-Ops"><a href="#4-2-Reduction-Ops" class="headerlink" title="4.2 Reduction Ops"></a>4.2 Reduction Ops</h4><h5 id="4-2-1-torch-norm"><a href="#4-2-1-torch-norm" class="headerlink" title="4.2.1 torch.norm()"></a>4.2.1 <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/#torchnorm">torch.norm()</a></h5><blockquote><p>【L2范数】<code>||x||2</code> = 对向量每个元素的平方求和，再开根号</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(<span class="built_in">input</span>, p=<span class="number">2</span>) → <span class="built_in">float</span></span><br></pre></td></tr></table></figure><p>返回输入张量<code>input</code> 的p 范数。</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>p (float,optional) – 范数计算中的幂指数值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line">-<span class="number">0.4376</span> -<span class="number">0.5328</span>  <span class="number">0.9547</span></span><br><span class="line">[torch.FloatTensor of size 1x3]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(a, <span class="number">3</span>)</span><br><span class="line"><span class="number">1.0338925067372466</span></span><br></pre></td></tr></table></figure><br><h5 id="4-2-2-torch-mean"><a href="#4-2-2-torch-mean" class="headerlink" title="4.2.2 torch.mean"></a>4.2.2 torch.mean</h5><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch<span class="selector-class">.mean</span>(<span class="selector-tag">input</span>) → <span class="attribute">float</span></span><br></pre></td></tr></table></figure><p>返回输入张量所有元素的均值。</p><p>参数： input (Tensor) – 输入张量</p><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line">-<span class="number">0.2946</span> -<span class="number">0.9143</span>  <span class="number">2.1809</span></span><br><span class="line">[torch.FloatTensor of size 1x3]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mean(a)</span><br><span class="line"><span class="number">0.32398951053619385</span></span><br><span class="line">torch.mean(<span class="built_in">input</span>, dim, out=<span class="literal">None</span>) → Tensor</span><br></pre></td></tr></table></figure><p>返回输入张量给定维度<code>dim</code>上每行的均值。</p><p>输出形状与输入相同，除了给定维度上为1.</p><p>参数：</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – the dimension to reduce</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line">-<span class="number">1.2738</span> -<span class="number">0.3058</span>  <span class="number">0.1230</span> -<span class="number">1.9615</span></span><br><span class="line"> <span class="number">0.8771</span> -<span class="number">0.5430</span> -<span class="number">0.9233</span>  <span class="number">0.9879</span></span><br><span class="line"> <span class="number">1.4107</span>  <span class="number">0.0317</span> -<span class="number">0.6823</span>  <span class="number">0.2255</span></span><br><span class="line">-<span class="number">1.3854</span>  <span class="number">0.4953</span> -<span class="number">0.2160</span>  <span class="number">0.2435</span></span><br><span class="line">[torch.FloatTensor of size 4x4]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mean(a, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">-<span class="number">0.8545</span></span><br><span class="line"> <span class="number">0.0997</span></span><br><span class="line"> <span class="number">0.2464</span></span><br><span class="line">-<span class="number">0.2157</span></span><br><span class="line">[torch.FloatTensor of size 4x1]</span><br></pre></td></tr></table></figure><br><h3 id="5-比较操作-Comparison-Ops"><a href="#5-比较操作-Comparison-Ops" class="headerlink" title="5 比较操作 Comparison Ops"></a>5 比较操作 Comparison Ops</h3><h4 id="5-1-torch-max"><a href="#5-1-torch-max" class="headerlink" title="5.1 torch.max"></a>5.1 torch.max</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#D</span></span><br><span class="line">torch.<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure><p>返回输入张量所有元素的最大值。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li></ul><p>例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"></span><br><span class="line"> <span class="number">0.4729</span> -<span class="number">0.2266</span> -<span class="number">0.2085</span></span><br><span class="line">[torch.FloatTensor of size 1x3]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">max</span>(a)</span><br><span class="line"><span class="number">0.4729</span></span><br></pre></td></tr></table></figure><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">max</span>(input, <span class="built_in">dim</span>, <span class="built_in">max</span>=<span class="keyword">None</span>, max_indices=<span class="keyword">None</span>) -&gt; (Tensor, LongTensor)</span><br></pre></td></tr></table></figure><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。</p><p>输出形状中，将<code>dim</code>维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>dim (int) – 指定的维度</li><li>max (Tensor, optional) – 结果张量，包含给定维度上的最大值</li><li>max_indices (LongTensor, optional) – 结果张量，包含给定维度上每个最大值的位置索引</li></ul><p>例子：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; <span class="selector-tag">a</span> = torch<span class="selector-class">.randn</span>(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">&gt;&gt; <span class="selector-tag">a</span></span><br><span class="line"></span><br><span class="line"><span class="number">0.0692</span>  <span class="number">0.3142</span>  <span class="number">1.2513</span> -<span class="number">0.5428</span></span><br><span class="line"><span class="number">0.9288</span>  <span class="number">0.8552</span> -<span class="number">0.2073</span>  <span class="number">0.6409</span></span><br><span class="line"><span class="number">1.0695</span> -<span class="number">0.0101</span> -<span class="number">2.4507</span> -<span class="number">1.2230</span></span><br><span class="line"><span class="number">0.7426</span> -<span class="number">0.7666</span>  <span class="number">0.4862</span> -<span class="number">0.6628</span></span><br><span class="line">torch<span class="selector-class">.FloatTensor</span> of size <span class="number">4</span>x4]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; torch<span class="selector-class">.max</span>(<span class="selector-tag">a</span>, <span class="number">1</span>)</span><br><span class="line">(</span><br><span class="line"> <span class="number">1.2513</span></span><br><span class="line"> <span class="number">0.9288</span></span><br><span class="line"> <span class="number">1.0695</span></span><br><span class="line"> <span class="number">0.7426</span></span><br><span class="line"><span class="selector-attr">[torch.FloatTensor of size 4x1]</span></span><br><span class="line">,</span><br><span class="line"> <span class="number">2</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"> <span class="number">0</span></span><br><span class="line"><span class="selector-attr">[torch.LongTensor of size 4x1]</span></span><br><span class="line">)</span><br><span class="line">torch<span class="selector-class">.max</span>(<span class="selector-tag">input</span>, other, out=None) → Tensor</span><br></pre></td></tr></table></figure><p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引。 即，<code>outi=max(inputi,otheri)</code></p><p>输出形状中，将<code>dim</code>维设定为1，其它与输入形状保持一致。</p><p>参数:</p><ul><li>input (Tensor) – 输入张量</li><li>other (Tensor) – 输出张量</li><li>out (Tensor, optional) – 结果张量</li></ul><p>例子：</p><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">a = torch.randn(<span class="number">4</span>)</span></span><br><span class="line"><span class="meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">a</span></span><br><span class="line"></span><br><span class="line"> 1.3869</span><br><span class="line"> 0.3912</span><br><span class="line">-0.8634</span><br><span class="line">-0.5468</span><br><span class="line">[torch.FloatTensor of size 4]</span><br><span class="line"></span><br><span class="line"><span class="meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">b = torch.randn(<span class="number">4</span>)</span></span><br><span class="line"><span class="meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">b</span></span><br><span class="line"></span><br><span class="line"> 1.0067</span><br><span class="line">-0.8010</span><br><span class="line"> 0.6258</span><br><span class="line"> 0.3627</span><br><span class="line">[torch.FloatTensor of size 4]</span><br><span class="line"></span><br><span class="line"><span class="meta prompt_">&gt;&gt;&gt;</span> <span class="language-python">torch.<span class="built_in">max</span>(a, b)</span></span><br><span class="line"></span><br><span class="line"> 1.3869</span><br><span class="line"> 0.3912</span><br><span class="line"> 0.6258</span><br><span class="line"> 0.3627</span><br><span class="line">[torch.FloatTensor of size 4]</span><br></pre></td></tr></table></figure><br><h3 id="6-序列化-Serialization"><a href="#6-序列化-Serialization" class="headerlink" title="6 序列化 Serialization"></a>6 序列化 Serialization</h3><h4 id="6-1-torch-saves-source"><a href="#6-1-torch-saves-source" class="headerlink" title="6.1 torch.saves[source]"></a>6.1 torch.saves[<a target="_blank" rel="noopener" href="http://pytorch.org/docs/_modules/torch/serialization.html#save">source]</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(obj, f, pickle_module=&lt;module <span class="string">&#x27;pickle&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;/home/jenkins/miniconda/lib/python3.5/pickle.py&#x27;</span>&gt;, pickle_protocol=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>保存一个对象到一个硬盘文件上 参考: <a target="_blank" rel="noopener" href="http://pytorch.org/docs/notes/serialization.html#recommend-saving-models">Recommended approach for saving a model</a> 参数：</p><ul><li>obj – 保存对象</li><li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>pickle_module – 用于pickling元数据和对象的模块</li><li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li></ul><br><h4 id="6-2-torch-load-source"><a href="#6-2-torch-load-source" class="headerlink" title="6.2 torch.load[source]"></a>6.2 torch.load[<a target="_blank" rel="noopener" href="http://pytorch.org/docs/_modules/torch/serialization.html#load">source]</a></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.load(f, map_location=<span class="literal">None</span>, pickle_module=&lt;module <span class="string">&#x27;pickle&#x27;</span> <span class="keyword">from</span> <span class="string">&#x27;/home/jenkins/miniconda/lib/python3.5/pickle.py&#x27;</span>&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example</span></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br></pre></td></tr></table></figure><p>从磁盘文件中读取一个通过<code>torch.save()</code>保存的对象。 <code>torch.load()</code> 可通过参数<code>map_location</code> 动态地进行内存重映射，使其能从不动设备中读取文件。一般调用时，需两个参数: storage 和 location tag. 返回不同地址中的storage，或着返回None (此时地址可以通过默认方法进行解析). 如果这个参数是字典的话，意味着其是从文件的地址标记到当前系统的地址标记的映射。 默认情况下， location tags中 “cpu”对应host tensors，‘cuda:device_id’ (e.g. ‘cuda:2’) 对应cuda tensors。 用户可以通过register_package进行扩展，使用自己定义的标记和反序列化方法。</p><p>参数:</p><ul><li>f – 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li><li>map_location – 一个函数或字典规定如何remap存储位置</li><li>pickle_module – 用于unpickling元数据和对象的模块 (必须匹配序列化文件时的pickle_module )</li></ul><p>例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>)</span><br><span class="line"><span class="comment"># Load all tensors onto the CPU</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line"><span class="comment"># Map tensors from GPU 1 to GPU 0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure><br><h3 id="7-索引-切片-连接-换位…"><a href="#7-索引-切片-连接-换位…" class="headerlink" title="7 索引 切片 连接 换位…"></a>7 索引 切片 连接 换位…</h3><h4 id="7-1-torch-stack"><a href="#7-1-torch-stack" class="headerlink" title="7.1 torch.stack"></a>7.1 <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/#torchstacksource">torch.stack</a></h4><figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.stack(<span class="keyword">sequence</span>, <span class="built_in">dim</span>=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状。</p><p>参数:</p><ul><li>sqequence (Sequence) – 待连接的张量序列</li><li>dim (int) – 插入的维度。必须介于 0 与 待连接的张量序列数之间。</li></ul><br><br><br><br><br><h2 id="二、Torch-Storage"><a href="#二、Torch-Storage" class="headerlink" title="二、Torch.Storage"></a>二、Torch.Storage</h2><p><code>torch.Storage</code> 跟绝大部分基于连续存储的数据结构类似，本质上是一个单一数据类型的一维连续数组(array)。</p><p>每一个 <code>torch.Tensor</code> 都有一个与之相对应的<code>torch.Storage</code>对象，两者存储数据的数据类型(data type)保持一致。</p><figure class="highlight axapta"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.FloatStorage</span><br></pre></td></tr></table></figure><h3 id="1、基本操作"><a href="#1、基本操作" class="headerlink" title="1、基本操作"></a>1、基本操作</h3><p><strong>byte()</strong></p><p>将此存储转为byte类型</p><p><strong>char()</strong></p><p>将此存储转为char类型</p><p><strong>clone()</strong></p><p>返回此存储的一个副本</p><br><h3 id="2、特别指出"><a href="#2、特别指出" class="headerlink" title="2、特别指出"></a>2、特别指出</h3><h4 id="2-1-类型转换"><a href="#2-1-类型转换" class="headerlink" title="2.1 类型转换"></a>2.1 类型转换</h4><p>type(<em>new_type=None, async=False</em>)</p><p>将此对象转为指定类型。<br>如果已经是正确类型，不会执行复制操作，直接返回原对象。</p><p><strong>参数：</strong></p><ul><li><strong>new_type</strong> (<em><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">type</a> or <a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">string</a></em>) -需要转成的类型</li><li><strong>async</strong> (<em><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">bool</a></em>) -如果值为True，且源在锁定内存中而目标在GPU中——或正好相反，则复制操作相对于宿主异步执行。否则此参数不起效果。</li></ul><br><h2 id="三、Torch-nn"><a href="#三、Torch-nn" class="headerlink" title="三、Torch.nn"></a>三、Torch.nn</h2><p><code>nn</code>是<code>Neural Network</code>的简称，是专门为神经网络设计的模块化接口，帮助便于执行如下的与神经网络相关的行为：</p><ul><li><p>创建神经网络</p></li><li><p>训练神经网络</p></li><li><p>保存神经网络</p></li><li><p>恢复神经网络</p></li></ul><p>nn构建与<code>autograd</code>之上，可以用来定义和运行神经网络：</p><ul><li>nn.Parameter</li><li>nn.Linear</li><li>nn.functional</li><li>nn.Module</li><li>nn.Sequential</li></ul><blockquote><p>参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/HiWangWenBing/article/details/120614234">神经网络基础 - torch.nn库五大基本功能</a></p></blockquote><br><h3 id="1、nn-Parameter"><a href="#1、nn-Parameter" class="headerlink" title="1、nn.Parameter"></a>1、nn.Parameter</h3><p><code>torch.nn.Parameter</code>是继承自<code>torch.Tensor</code>的子类，其主要作用是作为<code>nn.Module</code>中的可训练参数使用。</p><p>它与<code>torch.Tensor</code>的区别就是<code>nn.Parameter</code>会自动被认为是<code>module</code>的可训练参数，即加入到<code>parameter()</code>这个迭代器中去；而<code>module</code>中非<code>nn.Parameter()</code>的普通<code>tensor</code>是不在<code>parameter</code>中的。</p><p>注意到，<code>nn.Parameter</code>的对象的<code>requires_grad</code>属性的默认值是<code>True</code>，即是可被训练的，这与<code>torch.Tensor</code>对象的默认值相反。在<code>nn.Module</code>类中，<code>Pytorch</code>也是使用<code>nn.Parameter</code>来对每一个module的参数进行初始化的。</p><hr><h4 id="1-1-nn-Parameter概述"><a href="#1-1-nn-Parameter概述" class="headerlink" title="1.1 nn.Parameter概述"></a>1.1 nn.Parameter概述</h4><p><code>Parameter</code>实际上也是<code>Tensor</code>，也就是说是一个多维矩阵，是<code>Variable</code>类中的一个特殊类。</p><p>当我们创建一个<code>model</code>时，<code>nn</code>会自动创建相应的参数<code>parameter</code>，并会自动累加到模型的<code>Parameter</code> 成员列表中。</p><blockquote><p><strong>Parameters VS buffers</strong></p><ul><li>一种是反向传播需要被<code>optimizer</code>更新的，我们称之为<code>parameter</code><ul><li><code>self.register_parameter(&quot;param&quot;, param)</code></li><li><code>self.param = nn.Parameter(torch.randn(1))</code></li></ul></li><li>一种是反向传播不需要被<code>optimizer</code>更新的，我们称之为<code>buffer</code><ul><li><code>self.register_buffer(&quot;my_buffer&quot;, torch.randn(1))</code></li></ul></li></ul></blockquote><h4 id="1-2-单个全连接层中参数的个数"><a href="#1-2-单个全连接层中参数的个数" class="headerlink" title="1.2 单个全连接层中参数的个数"></a>1.2 单个全连接层中参数的个数</h4><p><code>in_features</code>的数量，决定了参数的个数：<code>Y = WX + b</code>。其中，<code>X</code>的维度就是<code>in_features</code>，<code>X</code>的维度决定的<code>W</code>的维度， 总的参数个数 = <code>in_features + 1</code></p><p><code>out_features</code>的数量，决定了全连接层中神经元的个数，因为每个神经元只有一个输出。有多少个输出，就需要多少个神经元。</p><ul><li><p>总的<code>W</code>参数的个数 = <code>in_features * out_features</code></p></li><li><p>总的<code>b</code>参数的个数 = <code>1 * out_features</code></p></li><li><p>总的参数<code>（W/B）</code>的个数 = <code>(in_features + 1) * out_features</code></p></li></ul><br><h4 id="1-3-使用参数创建全连接层代码案例"><a href="#1-3-使用参数创建全连接层代码案例" class="headerlink" title="1.3 使用参数创建全连接层代码案例"></a>1.3 使用参数创建全连接层代码案例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.functional.linear( )</span></span><br><span class="line">x_input = torch.Tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input.shape:&quot;</span>, x_input.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x_input      :&quot;</span>, x_input)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights1 = nn.Parameter(torch.rand(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Bias1 = nn.Parameter(torch.rand(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias.shape:&quot;</span>, Bias1.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Bias      :&quot;</span>, Bias1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">Weights2 = nn.Parameter(torch.Tensor(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights.shape:&quot;</span>, Weights2.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Weights      :&quot;</span>, Weights2)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nfull_connect_layer&quot;</span>)</span><br><span class="line">full_connect_layer = nn.functional.linear(x_input, Weights1)</span><br><span class="line"><span class="built_in">print</span>(full_connect_layer)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    x_input.shape: torch.Size([<span class="number">3</span>])</span><br><span class="line">    x_input      : tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line">    Weights.shape: torch.Size([<span class="number">3</span>])</span><br><span class="line">    Weights      : Parameter containing:</span><br><span class="line">    tensor([<span class="number">0.3339</span>, <span class="number">0.7027</span>, <span class="number">0.9703</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    Bias.shape: torch.Size([<span class="number">1</span>])</span><br><span class="line">    Bias      : Parameter containing:</span><br><span class="line">    tensor([<span class="number">0.4936</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    Weights.shape: torch.Size([<span class="number">3</span>])</span><br><span class="line">    Weights      : Parameter containing:</span><br><span class="line">    tensor([<span class="number">0.0000e+00</span>, <span class="number">1.8980e+01</span>, <span class="number">1.1210e-44</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    full_connect_layer</span><br><span class="line">    tensor(<span class="number">2.0068</span>, grad_fn=&lt;DotBackward&gt;)</span><br></pre></td></tr></table></figure><br><h3 id="2、nn-Linear"><a href="#2、nn-Linear" class="headerlink" title="2、nn.Linear"></a>2、nn.Linear</h3><br><h3 id="3、nn-functional"><a href="#3、nn-functional" class="headerlink" title="3、nn.functional"></a>3、nn.functional</h3><p>在<code>PyTorch</code>中，还有一个库为<code>nn.functional</code>，同样也提供了很多网络层与函数功能，但与<code>nn.Module</code>不同的是，利用<code>nn.functional</code>定义的网络层不可自动学习参数，还需要使用<code>nn.Parameter</code>封装。</p><p><code>nn.functional</code>的设计初衷是对于一些不需要学习参数的层，如激活函数层、<code>BN(Batch Normalization)</code>层，可以使用<code>nn.functional</code>，这样这些层就不需要在<code>nn.Module</code>中定义了</p><h4 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h4><p><strong>nn.functional</strong></p><ul><li>包含<code>torch.nn</code>库所有函数，包含大量<code>Loss</code>和<code>Activation function</code><ul><li>如 <code>torch.nn.functional.conv2d(...)</code></li></ul></li><li><code>nn.functional.XX</code> 是函数接口</li><li><code>nn.functional.XX</code> 无法与<code>nn.Sequential</code>结合使用</li><li>没有学习参数的【损失函数、激活函数等】根据个人选择使用<code>nn.functional.XX</code>或<code>nn.XX</code></li><li>需要特别注意<code>dropout</code>层</li></ul><p><strong>分类</strong>：包括神经网络<strong>前向</strong>和<strong>后向</strong>处理所需要到的常见函数。</p><ul><li><strong>神经元处理函数</strong></li><li><strong>激活函数</strong></li></ul><br><h4 id="3-2-激活函数"><a href="#3-2-激活函数" class="headerlink" title="3.2 激活函数"></a>3.2 激活函数</h4><h5 id="3-2-1-relu案例"><a href="#3-2-1-relu案例" class="headerlink" title="3.2.1 relu案例"></a>3.2.1 relu案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.functional.relu( )</span></span><br><span class="line"><span class="built_in">print</span>(y_output)</span><br><span class="line">out = nn.functional.relu(y_output)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    tensor([[ <span class="number">0.1023</span>,  <span class="number">0.7831</span>, -<span class="number">0.2368</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">    torch.Size([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    tensor([[<span class="number">0.1023</span>, <span class="number">0.7831</span>, <span class="number">0.0000</span>]], grad_fn=&lt;ReluBackward0&gt;)</span><br></pre></td></tr></table></figure><h5 id="3-2-2-sigmoid"><a href="#3-2-2-sigmoid" class="headerlink" title="3.2.2 sigmoid"></a>3.2.2 sigmoid</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nn.functional.sigmoid( )</span></span><br><span class="line"><span class="built_in">print</span>(y_output)</span><br><span class="line">out = nn.functional.sigmoid(y_output)</span><br><span class="line"><span class="built_in">print</span>(out.shape)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[ <span class="number">0.1023</span>,  <span class="number">0.7831</span>, -<span class="number">0.2368</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br><span class="line">    torch.Size([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    tensor([[<span class="number">0.5255</span>, <span class="number">0.6863</span>, <span class="number">0.4411</span>]], grad_fn=&lt;SigmoidBackward&gt;)</span><br></pre></td></tr></table></figure><br><h4 id="3-3-nn-xxx和nn-functional-xxx比较"><a href="#3-3-nn-xxx和nn-functional-xxx比较" class="headerlink" title="3.3 nn.xxx和nn.functional.xxx比较"></a>3.3 nn.xxx和nn.functional.xxx比较</h4><br><h3 id="4、nn-Module"><a href="#4、nn-Module" class="headerlink" title="4、nn.Module"></a>4、nn.Module</h3><br><h3 id="5、nn-Sequential"><a href="#5、nn-Sequential" class="headerlink" title="5、nn.Sequential"></a>5、nn.Sequential</h3><p><code>nn.Sequential</code>【继承自<code>nn.Module</code>类】是一个有序的容器，该类将按照传入构造器的顺序，依次创建相应的函数，并记录在<code>Sequential</code>类对象的数据结构中，同时以神经网络模块为元素的有序字典也可以作为传入参数。</p><p>因此，<code>Sequential</code>可以看成是有多个函数运算对象，串联成的神经网络，其返回的是<code>Module</code>类型的神经网络对象。</p><h4 id="5-1-以列表的形式，串联函数运算，构建串行执行的神经网络"><a href="#5-1-以列表的形式，串联函数运算，构建串行执行的神经网络" class="headerlink" title="5.1 以列表的形式，串联函数运算，构建串行执行的神经网络"></a>5.1 以列表的形式，串联函数运算，构建串行执行的神经网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line"><span class="comment"># A sequential container. Modules will be added to it in the order they are passed in the constructor. </span></span><br><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">32</span>), nn.ReLU(), nn.Linear(<span class="number">32</span>, <span class="number">10</span>), nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(model_c)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model_c.parameters)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>],-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">利用系统提供的神经网络模型类：Sequential,以参数列表的方式来实例化神经网络模型对象</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">3</span>): Softmax(dim=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">显示网络模型参数</span><br><span class="line">&lt;bound method Module.parameters of Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">3</span>): Softmax(dim=<span class="number">1</span>)</span><br><span class="line">)&gt;</span><br><span class="line"></span><br><span class="line">定义神经网络样本输入</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">使用神经网络进行预测</span><br><span class="line">tensor([[-<span class="number">0.1526</span>,  <span class="number">0.0437</span>, -<span class="number">0.1685</span>,  <span class="number">0.0034</span>, -<span class="number">0.0675</span>,  <span class="number">0.0423</span>,  <span class="number">0.2807</span>,  <span class="number">0.0527</span>,</span><br><span class="line">         -<span class="number">0.1710</span>,  <span class="number">0.0668</span>],</span><br><span class="line">        [-<span class="number">0.1820</span>,  <span class="number">0.0860</span>,  <span class="number">0.0174</span>,  <span class="number">0.0883</span>,  <span class="number">0.2046</span>, -<span class="number">0.1609</span>,  <span class="number">0.0165</span>, -<span class="number">0.2392</span>,</span><br><span class="line">         -<span class="number">0.2348</span>,  <span class="number">0.1697</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><br><h4 id="5-2-以字典的形式，串联函数运算，构建串行执行的神经网络"><a href="#5-2-以字典的形式，串联函数运算，构建串行执行的神经网络" class="headerlink" title="5.2 以字典的形式，串联函数运算，构建串行执行的神经网络"></a>5.2 以字典的形式，串联函数运算，构建串行执行的神经网络</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象&quot;</span>)</span><br><span class="line">model = nn.Sequential(OrderedDict([(<span class="string">&#x27;h1&#x27;</span>, nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">32</span>)),</span><br><span class="line">                                     (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">                                     (<span class="string">&#x27;out&#x27;</span>, nn.Linear(<span class="number">32</span>, <span class="number">10</span>)),</span><br><span class="line">                                     (<span class="string">&#x27;softmax&#x27;</span>, nn.Softmax(dim=<span class="number">1</span>))]))</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n显示网络模型参数&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.parameters)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n定义神经网络样本输入&quot;</span>)</span><br><span class="line">x_input = torch.randn(<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x_input.shape)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n使用神经网络进行预测&quot;</span>)</span><br><span class="line">y_pred = model.forward(x_input.view(x_input.size()[<span class="number">0</span>],-<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(y_pred)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">利用系统提供的神经网络模型类：Sequential,以字典的方式来实例化神经网络模型对象</span><br><span class="line">Sequential(</span><br><span class="line">  (h1): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (relu1): ReLU()</span><br><span class="line">  (out): Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (softmax): Softmax(dim=<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">显示网络模型参数</span><br><span class="line">&lt;bound method Module.parameters of Sequential(</span><br><span class="line">  (h1): Linear(in_features=<span class="number">784</span>, out_features=<span class="number">32</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (relu1): ReLU()</span><br><span class="line">  (out): Linear(in_features=<span class="number">32</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (softmax): Softmax(dim=<span class="number">1</span>)</span><br><span class="line">)&gt;</span><br><span class="line"></span><br><span class="line">定义神经网络样本输入</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">使用神经网络进行预测</span><br><span class="line">tensor([[<span class="number">0.1249</span>, <span class="number">0.1414</span>, <span class="number">0.0708</span>, <span class="number">0.1031</span>, <span class="number">0.1080</span>, <span class="number">0.1351</span>, <span class="number">0.0859</span>, <span class="number">0.0947</span>, <span class="number">0.0753</span>,</span><br><span class="line">         <span class="number">0.0607</span>],</span><br><span class="line">        [<span class="number">0.0982</span>, <span class="number">0.1102</span>, <span class="number">0.0929</span>, <span class="number">0.0855</span>, <span class="number">0.0848</span>, <span class="number">0.1076</span>, <span class="number">0.1077</span>, <span class="number">0.0949</span>, <span class="number">0.1153</span>,</span><br><span class="line">         <span class="number">0.1029</span>]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br></pre></td></tr></table></figure><br><h4 id="5-3-举例"><a href="#5-3-举例" class="headerlink" title="5.3 举例"></a>5.3 举例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of using Sequential</span></span><br><span class="line">model_c = nn.Sequential(</span><br><span class="line">            nn.conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">            nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example of using Sequential with OrderedDict</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;conv1&#x27;</span>, nn.conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">            (<span class="string">&#x27;conv2&#x27;</span>, nn.conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure><br><h2 id="四、Torch-optim"><a href="#四、Torch-optim" class="headerlink" title="四、Torch.optim"></a>四、Torch.optim</h2><br><h2 id="五、Torch-autogard"><a href="#五、Torch-autogard" class="headerlink" title="五、Torch.autogard"></a>五、Torch.autogard</h2><br><h2 id="六、Torch-utils"><a href="#六、Torch-utils" class="headerlink" title="六、Torch.utils"></a>六、Torch.utils</h2><br><h2 id="七、torchvision"><a href="#七、torchvision" class="headerlink" title="七、torchvision"></a>七、torchvision</h2><p><strong>torchvision是pytorch的一个图形库，它服务于PyTorch深度学习框架的，主要用来构建计算机视觉模型。</strong></p><p><code>torchvision</code>的构成如下：</p><ul><li><strong>torchvision.datasets:</strong> 一些加载数据的函数及常用的数据集接口；</li><li><strong>torchvision.models:</strong> 包含常用的模型结构（含预训练模型），例如AlexNet、VGG、ResNet等；</li><li><strong>torchvision.transforms:</strong> 常用的图片变换，例如裁剪、旋转等；</li><li><strong>torchvision.utils:</strong> 其他的一些有用的方法。</li></ul><h3 id="1、Torchvision-transforms-解析"><a href="#1、Torchvision-transforms-解析" class="headerlink" title="1、Torchvision.transforms 解析"></a>1、Torchvision.transforms <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/476220305?utm_id=0">解析</a></h3><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">transforms.CenterCrop 对图片中心进行裁剪 </span><br><span class="line">transforms.ColorJitter 对图像颜色的对比度、饱和度和零度进行变换</span><br><span class="line">transforms.FiveCrop  对图像四个角和中心进行裁剪得到五分图像</span><br><span class="line">transforms.Grayscale  对图像进行灰度变换</span><br><span class="line">transforms.Pad  使用固定值进行像素填充</span><br><span class="line">transforms.RandomAffine  随机仿射变换 </span><br><span class="line">transforms.RandomCrop  随机区域裁剪</span><br><span class="line">transforms.RandomHorizontalFlip  随机水平翻转</span><br><span class="line">transforms.RandomRotation  随机旋转</span><br><span class="line">transforms.RandomVerticalFlip  随机垂直翻转</span><br></pre></td></tr></table></figure><p><strong>常见操作：</strong></p><ul><li><p><strong>class torchvision.transforms.CenterCrop(size)</strong> 将给定的<code>PIL.Image</code>进行中心切割，得到给定的size，<code>size</code>可以是<code>tuple</code>，<code>(target_height, target_width)</code>。<code>size</code>也可以是一个<code>Integer</code>，在这种情况下，切出来的图片的形状是正方形。</p></li><li><p><strong>class torchvision.transforms.RandomCrop(size, padding=0)</strong> 切割中心点的位置随机选取。<code>size</code>可以是<code>tuple</code>也可以是<code>Integer</code>。</p></li><li><p><strong>class torchvision.transforms.RandomHorizontalFlip</strong> 随机水平翻转给定的<code>PIL.Image</code>，概率为0.5。即：一半的概率翻转，一半的概率不翻转。</p></li><li><p><strong>class torchvision.transforms.RandomSizedCrop(size, interpolation=2)</strong> 先将给定的<code>PIL.Image</code>随机切，然后再<code>resize</code>成给定的<code>size</code>大小。</p></li><li><p><strong>class torchvision.transforms.Pad(padding, fill=0)</strong> 将给定的<code>PIL.Image</code>的所有边用给定的<code>pad value</code>填充。【<code>padding</code>：要填充多少像素 <code>fill</code>：用什么值填充】</p></li></ul><p><strong>举例：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">padding_img = transforms.Pad(padding=<span class="number">10</span>, fill=<span class="number">0</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;C:/Users/Stell/Desktop/2.png&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))</span><br><span class="line"><span class="built_in">print</span>(img.size)</span><br><span class="line"> </span><br><span class="line">padded_img = padding_img(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(padded_img))</span><br><span class="line"><span class="built_in">print</span>(padded_img.size)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">        &lt;<span class="keyword">class</span> <span class="string">&#x27;PIL.PngImagePlugin.PngImageFile&#x27;</span>&gt;</span><br><span class="line">        (<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        &lt;<span class="keyword">class</span> <span class="string">&#x27;PIL.Image.Image&#x27;</span>&gt;</span><br><span class="line">        (<span class="number">30</span>, <span class="number">30</span>)</span><br></pre></td></tr></table></figure><br><h4 id="1-1-transforms-Compose-类"><a href="#1-1-transforms-Compose-类" class="headerlink" title="1.1  transforms.Compose()类"></a>1.1 transforms.Compose()类</h4><p>这个类的主要作用是串联多个图片变换的操作。</p><p>这个类的构造很简单：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class torchvision.transforms.Compose(transforms):</span><br><span class="line"> # Composes several transforms together.</span><br><span class="line"> # Parameters: transforms (list of Transform objects) – list of transforms to compose.</span><br><span class="line"> </span><br><span class="line"># Example</span><br><span class="line"># 可以看出Compose里面的参数实际上就是个列表，而这个列表里面的元素就是你想要执行的transform操作</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; transforms.Compose([</span><br><span class="line">&gt;&gt;&gt;     transforms.CenterCrop(10),</span><br><span class="line">&gt;&gt;&gt;     transforms.ToTensor(),])</span><br></pre></td></tr></table></figure><p>事实上，<code>Compose()</code>类会将<code>transforms</code>列表里面的<code>transform</code>操作进行遍历。实现的代码很简单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 这里对源码进行了部分截取。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, img</span>):</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="variable language_">self</span>.transforms:   </span><br><span class="line">        img = t(img)</span><br><span class="line">    <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure><p><strong>接下来举例说明transforms.Compose：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">             transforms.RandomHorizontalFlip(),</span><br><span class="line">                    transforms.ToTensor(),</span><br><span class="line">                    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])</span><br></pre></td></tr></table></figure><p>具体是对图像进行各种转换操作，并用函数<code>compose</code>将这些转换操作组合起来；</p><p>先读取一张图片：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">r&quot;C:\Users\Stell\Desktop\2.png&quot;</span>)</span><br></pre></td></tr></table></figure><p>**transforms.RandomResizedCrop(224)**将给定图像随机裁剪为不同的大小和宽高比，然后缩放所裁剪得到的图像为制定的大小；（即先随机采集，然后对裁剪得到的图像缩放为同一大小），默认<code>scale=(0.08, 1.0)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;原图大小：&quot;</span>,img.size)</span><br><span class="line"></span><br><span class="line">data1 = transforms.RandomResizedCrop(<span class="number">224</span>)(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;随机裁剪后的大小:&quot;</span>,data1.size)</span><br><span class="line">data2 = transforms.RandomResizedCrop(<span class="number">224</span>)(img)</span><br><span class="line">data3 = transforms.RandomResizedCrop(<span class="number">224</span>)(img)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),plt.imshow(img),plt.title(<span class="string">&quot;原图0&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),plt.imshow(data1),plt.title(<span class="string">&quot;转换后的图1&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>),plt.imshow(data2),plt.title(<span class="string">&quot;转换后的图2&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>),plt.imshow(data3),plt.title(<span class="string">&quot;转换后的图3&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    原图大小： (<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">    随机裁剪后的大小: (<span class="number">224</span>, <span class="number">224</span>)</span><br></pre></td></tr></table></figure><p><img src="/.io//pytorch-1.png"></p><blockquote><p>该操作的含义在于：即使只是该物体的一部分，我们也认为这是该类物体</p></blockquote><p><strong>transforms.RandomHorizontalFlip()：以给定的概率随机水平旋转给定的PIL的图像，默认为0.5</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">r&quot;C:\Users\Stell\Desktop\2.png&quot;</span>)</span><br><span class="line">img1 = transforms.RandomHorizontalFlip()(img)</span><br><span class="line">img2 = transforms.RandomHorizontalFlip()(img)</span><br><span class="line">img3 = transforms.RandomHorizontalFlip()(img)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>),plt.imshow(img),plt.title(<span class="string">&quot;原图0&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>), plt.imshow(img1), plt.title(<span class="string">&quot;变换后的图1&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>), plt.imshow(img2), plt.title(<span class="string">&quot;变换后的图2&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>), plt.imshow(img3), plt.title(<span class="string">&quot;变换后的图3&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/.io//pytorch-2.png"></p><p><strong>transforms.ToTensor() ：将给定图像转为Tensor</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">r&quot;C:\Users\Stell\Desktop\2.png&quot;</span>)</span><br><span class="line">img = transforms.ToTensor()(img)</span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment">#3通道</span></span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9725</span>, <span class="number">0.9843</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9765</span>, <span class="number">0.9725</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9765</span>, <span class="number">0.9725</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9647</span>, <span class="number">0.9412</span>, <span class="number">0.9412</span>, <span class="number">0.9725</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9882</span>, <span class="number">0.9529</span>, <span class="number">1.0000</span>, <span class="number">0.9725</span>, <span class="number">0.9725</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9804</span>, <span class="number">0.9529</span>, <span class="number">1.0000</span>, <span class="number">0.9765</span>, <span class="number">0.9725</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.9529</span>, <span class="number">0.9412</span>, <span class="number">0.9412</span>, <span class="number">0.9529</span>, <span class="number">0.9961</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.6902</span>, <span class="number">0.8314</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.5843</span>, <span class="number">0.1373</span>, <span class="number">0.1373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.8745</span>, <span class="number">0.4000</span>, <span class="number">1.0000</span>, <span class="number">0.6902</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7843</span>, <span class="number">0.4000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.4000</span>, <span class="number">0.1373</span>, <span class="number">0.1373</span>, <span class="number">0.4000</span>, <span class="number">0.9608</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.6902</span>, <span class="number">0.8314</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.5843</span>, <span class="number">0.1373</span>, <span class="number">0.1373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.8745</span>, <span class="number">0.4000</span>, <span class="number">1.0000</span>, <span class="number">0.6902</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.7843</span>, <span class="number">0.4000</span>, <span class="number">1.0000</span>, <span class="number">0.7373</span>, <span class="number">0.6902</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">0.4000</span>, <span class="number">0.1373</span>, <span class="number">0.1373</span>, <span class="number">0.4000</span>, <span class="number">0.9608</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">         [<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>,</span><br><span class="line">          <span class="number">1.0000</span>, <span class="number">1.0000</span>]]])</span><br></pre></td></tr></table></figure><p><strong>transforms.Normalize(）：归一化处理</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">r&quot;C:\Users\Stell\Desktop\2.png&quot;</span>)</span><br><span class="line">img = transforms.ToTensor()(img)</span><br><span class="line">img = transforms.Normalize(mean=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>], std=[<span class="number">0.5</span>,<span class="number">0.5</span>,<span class="number">0.5</span>])(img)</span><br><span class="line"><span class="built_in">print</span>(img)</span><br></pre></td></tr></table></figure><br><h4 id="1-2-transforms-Normalize-）"><a href="#1-2-transforms-Normalize-）" class="headerlink" title="1.2 transforms.Normalize(）"></a>1.2 transforms.Normalize(）</h4><p>简单来说就是将数据按通道进行计算，将每一个通道的数据<strong>先计算出其方差与均值</strong>，然后再将其<strong>每一个通道内的每一个数据减去均值，再除以方差，得到归一化后的结果。</strong></p><p>在深度学习图像处理中，标准化处理之后，可以使数据更好的响应激活函数，提高数据的表现力，减少梯度爆炸和梯度消失的出现，加快模型的收敛速度。</p><br><h2 id="八、处理图像"><a href="#八、处理图像" class="headerlink" title="八、处理图像"></a>八、处理图像</h2><br><h2 id="九、matplotlib绘图库"><a href="#九、matplotlib绘图库" class="headerlink" title="九、matplotlib绘图库"></a>九、<code>matplotlib</code>绘图库</h2><h3 id="9-1-基础用法"><a href="#9-1-基础用法" class="headerlink" title="9.1 基础用法"></a>9.1 基础用法</h3><h4 id="9-1-1-起步"><a href="#9-1-1-起步" class="headerlink" title="9.1.1 起步"></a>9.1.1 起步</h4><p>plot函数是Python中一个非常常用的画图函数，通常用于绘制二维图形。它的最基础用法是传入一个列表或数组，plot函数会自动将这些数据点连接起来绘制成一条折线图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#在这个例子里，我们使用numpy库生成了100个在0到10之间均匀分布的数据点作为x轴坐标</span></span><br><span class="line"><span class="comment">#然后通过numpy库的sin函数生成对应的y轴坐标。最后用plot函数将这些点连接成折线图并展现</span></span><br></pre></td></tr></table></figure><p><img src="/.io//pytorch-9-1.png"></p><h4 id="9-1-2-自定义颜色、线型和点型"><a href="#9-1-2-自定义颜色、线型和点型" class="headerlink" title="9.1.2 自定义颜色、线型和点型"></a>9.1.2 自定义颜色、线型和点型</h4><p>除了默认的蓝色实线，plot函数还有很多自定义的参数可以使用。其中<code>color</code>、<code>linestyle</code>和<code>marker</code>分别控制线条的<strong>颜色</strong>、<strong>线型</strong>和<strong>点型</strong>。可以在函数调用时通过指定这些参数实现自定义。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#在这个例子里，我们使用了红色的虚线和圆形点。</span></span><br></pre></td></tr></table></figure><p><img src="/.io//pytorch-9-2.png"></p><h4 id="9-1-3-设置坐标轴范围和标签"><a href="#9-1-3-设置坐标轴范围和标签" class="headerlink" title="9.1.3 设置坐标轴范围和标签"></a>9.1.3 设置坐标轴范围和标签</h4><p>除了线条和点的自定义，<code>plot</code>函数还可以通过设置<code>xlim</code>、<code>ylim</code>和<code>xlabel</code>、<code>ylabel</code>等来控制<strong>坐标轴的范围</strong>和<strong>标签</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">10</span>, <span class="number">100</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.xlim(<span class="number">0</span>, <span class="number">11</span>)</span><br><span class="line">plt.ylim(-<span class="number">1.5</span>, <span class="number">1.5</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X Axis&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y Axis&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在这个例子里，我们限制了x轴范围在0到11之间，y轴范围在-1.5到1.5之间，并设置了x轴和y轴的标签</span></span><br></pre></td></tr></table></figure><p><img src="/.io//pytorch-9-3.png"></p><h4 id="9-1-4-绘制多个图形"><a href="#9-1-4-绘制多个图形" class="headerlink" title="9.1.4 绘制多个图形"></a>9.1.4 绘制多个图形</h4><p>plot函数可以同时展示多个图形。只需要在调用函数时，将不同的数据和属性传入即可。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 10, 100)</span><br><span class="line">y1 = np.sin(x)</span><br><span class="line">y2 = np.cos(x)</span><br><span class="line"></span><br><span class="line">plt.plot(x, y1, <span class="attribute">color</span>=<span class="string">&#x27;red&#x27;</span>, <span class="attribute">linestyle</span>=<span class="string">&#x27;--&#x27;</span>, <span class="attribute">marker</span>=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.plot(x, y2, <span class="attribute">color</span>=<span class="string">&#x27;blue&#x27;</span>, <span class="attribute">linestyle</span>=<span class="string">&#x27;-.&#x27;</span>, <span class="attribute">marker</span>=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">plt.xlim(0, 11)</span><br><span class="line">plt.ylim(-1.5, 1.5)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;X Axis&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Y Axis&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;sin(x)&#x27;</span>, <span class="string">&#x27;cos(x)&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>在这个例子里，我们同时绘制了sin(x)和cos(x)的图像，并设置了图例。</p><p><img src="/.io//pytorch-9-4.png"></p><h4 id="9-1-5-其他用法"><a href="#9-1-5-其他用法" class="headerlink" title="9.1.5 其他用法"></a>9.1.5 其他用法</h4><p>除了以上常用的基础用法之外，plot函数还有很多其他用法。比如绘制散点图、直方图、面积图等等。还可以使用<code>subplot</code>函数<strong>创建子图</strong>，使用<code>plt.savefig</code>函数<strong>保存图像</strong>等等。更多plot函数的用法可以参考官方文档。</p><p>以下是一个简单的例子，展现如何绘制散点图。</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="title">x</span> = np.random.randn(<span class="number">100</span>)</span><br><span class="line"><span class="title">y</span> = np.random.randn(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="title">plt</span>.scatter(x, y)</span><br><span class="line"><span class="title">plt</span>.show()</span><br></pre></td></tr></table></figure><p>在这个例子里，我们随机生成了100个点作为x轴和y轴的坐标，并用scatter函数将它们绘制成散点图。</p><p><img src="/.io//pytorch-9-5.png"></p><br><h3 id="9-2"><a href="#9-2" class="headerlink" title="9.2"></a>9.2</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/545234640">plt.figsize</a></p><br><br><br><br><br><br><br><br><br><br><br><h2 id="X、Sundry"><a href="#X、Sundry" class="headerlink" title="X、Sundry"></a>X、Sundry</h2><h3 id="1、数据集下载-amp-构建数据集"><a href="#1、数据集下载-amp-构建数据集" class="headerlink" title="1、数据集下载 &amp; 构建数据集"></a>1、数据集下载 &amp; 构建数据集</h3><blockquote><p><a target="_blank" rel="noopener" href="https://paperswithcode.com/sota">Browse State-of-the-Art ：数据集下载</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/GODXML/article/details/125065304">手动数据集下载教程</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42360017/article/details/123774892">pytorch入门6：学会如何下载数据集，并通过tensorboard查看下载的数据集</a></p></blockquote><p>PyTorch领域库提供了一些预加载的数据集（如FashionMNIST），这些数据集是<code>torch.utils.data.Dataset</code>的子类，并实现特定数据的功能。它们可以被用来为你的模型制作原型和基准。你可以找到它们这里：<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">Image Datasets</a>、<a target="_blank" rel="noopener" href="https://pytorch.org/text/stable/datasets.html">Text Datasets</a>，和<a target="_blank" rel="noopener" href="https://pytorch.org/audio/stable/datasets.html">Audio Datasets</a></p><br><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Moustache 微信支付"><p>微信支付</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Moustache</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hammerzer.github.io/2023/07/21/Tool-Pytorch/" title="Tool-Pytorch">https://hammerzer.github.io/2023/07/21/Tool-Pytorch/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2023/07/21/Tool-Numpy/" rel="prev" title="Tool-Numpy"><i class="fa fa-chevron-left"></i> Tool-Numpy</a></div><div class="post-nav-item"><a href="/2023/08/10/convolutional-neural-network/" rel="next" title="动手学深度学习-卷积神经网络">动手学深度学习-卷积神经网络 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CONTENT-OUTLINE"><span class="nav-number">1.</span> <span class="nav-text">CONTENT OUTLINE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%87%E3%80%81%E7%9B%AE%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">〇、目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Torch"><span class="nav-number">3.</span> <span class="nav-text">一、Torch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%BC%A0%E9%87%8FTensors"><span class="nav-number">3.1.</span> <span class="nav-text">1 张量Tensors</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-torch-numel"><span class="nav-number">3.1.1.</span> <span class="nav-text">1.1 torch.numel</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-torch-is-tensor"><span class="nav-number">3.1.2.</span> <span class="nav-text">1.2 torch.is_tensor</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%88%9B%E5%BB%BA%E6%93%8D%E4%BD%9C-Creation-Ops"><span class="nav-number">3.2.</span> <span class="nav-text">2 创建操作 Creation Ops</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-torch-arange"><span class="nav-number">3.2.1.</span> <span class="nav-text">2.1 torch.arange</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-torch-ones"><span class="nav-number">3.2.2.</span> <span class="nav-text">2.2 torch.ones</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-torch-zeros"><span class="nav-number">3.2.3.</span> <span class="nav-text">2.3 torch.zeros</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7-Random-sampling"><span class="nav-number">3.3.</span> <span class="nav-text">3 随机抽样 Random sampling</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-torch-normal"><span class="nav-number">3.3.1.</span> <span class="nav-text">3.1 torch.normal()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E6%95%B0%E5%AD%A6%E6%93%8D%E4%BD%9CMath-operations"><span class="nav-number">3.4.</span> <span class="nav-text">4 数学操作Math operations</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Pointwise-Ops%E3%80%90%E9%80%90%E7%82%B9%E6%93%8D%E4%BD%9C%E3%80%91"><span class="nav-number">3.4.1.</span> <span class="nav-text">4.1 Pointwise Ops【逐点操作】</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-1-torch-mul"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">4.1.1 torch.mul</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-2-torch-cos"><span class="nav-number">3.4.1.2.</span> <span class="nav-text">4.1.2 torch.cos</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-1-3-torch-div"><span class="nav-number">3.4.1.3.</span> <span class="nav-text">4.1.3 torch.div()</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-Reduction-Ops"><span class="nav-number">3.4.2.</span> <span class="nav-text">4.2 Reduction Ops</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-torch-norm"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">4.2.1 torch.norm()</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-torch-mean"><span class="nav-number">3.4.2.2.</span> <span class="nav-text">4.2.2 torch.mean</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-%E6%AF%94%E8%BE%83%E6%93%8D%E4%BD%9C-Comparison-Ops"><span class="nav-number">3.5.</span> <span class="nav-text">5 比较操作 Comparison Ops</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-torch-max"><span class="nav-number">3.5.1.</span> <span class="nav-text">5.1 torch.max</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-%E5%BA%8F%E5%88%97%E5%8C%96-Serialization"><span class="nav-number">3.6.</span> <span class="nav-text">6 序列化 Serialization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-torch-saves-source"><span class="nav-number">3.6.1.</span> <span class="nav-text">6.1 torch.saves[source]</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-torch-load-source"><span class="nav-number">3.6.2.</span> <span class="nav-text">6.2 torch.load[source]</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87-%E8%BF%9E%E6%8E%A5-%E6%8D%A2%E4%BD%8D%E2%80%A6"><span class="nav-number">3.7.</span> <span class="nav-text">7 索引 切片 连接 换位…</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-torch-stack"><span class="nav-number">3.7.1.</span> <span class="nav-text">7.1 torch.stack</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Torch-Storage"><span class="nav-number">4.</span> <span class="nav-text">二、Torch.Storage</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="nav-number">4.1.</span> <span class="nav-text">1、基本操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E7%89%B9%E5%88%AB%E6%8C%87%E5%87%BA"><span class="nav-number">4.2.</span> <span class="nav-text">2、特别指出</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2"><span class="nav-number">4.2.1.</span> <span class="nav-text">2.1 类型转换</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Torch-nn"><span class="nav-number">5.</span> <span class="nav-text">三、Torch.nn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81nn-Parameter"><span class="nav-number">5.1.</span> <span class="nav-text">1、nn.Parameter</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-nn-Parameter%E6%A6%82%E8%BF%B0"><span class="nav-number">5.1.1.</span> <span class="nav-text">1.1 nn.Parameter概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E5%8D%95%E4%B8%AA%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B8%AD%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="nav-number">5.1.2.</span> <span class="nav-text">1.2 单个全连接层中参数的个数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E4%BD%BF%E7%94%A8%E5%8F%82%E6%95%B0%E5%88%9B%E5%BB%BA%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%BB%A3%E7%A0%81%E6%A1%88%E4%BE%8B"><span class="nav-number">5.1.3.</span> <span class="nav-text">1.3 使用参数创建全连接层代码案例</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81nn-Linear"><span class="nav-number">5.2.</span> <span class="nav-text">2、nn.Linear</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81nn-functional"><span class="nav-number">5.3.</span> <span class="nav-text">3、nn.functional</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E6%A6%82%E8%BF%B0"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.1 概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.2 激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-1-relu%E6%A1%88%E4%BE%8B"><span class="nav-number">5.3.2.1.</span> <span class="nav-text">3.2.1 relu案例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-2-sigmoid"><span class="nav-number">5.3.2.2.</span> <span class="nav-text">3.2.2 sigmoid</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-nn-xxx%E5%92%8Cnn-functional-xxx%E6%AF%94%E8%BE%83"><span class="nav-number">5.3.3.</span> <span class="nav-text">3.3 nn.xxx和nn.functional.xxx比较</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81nn-Module"><span class="nav-number">5.4.</span> <span class="nav-text">4、nn.Module</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81nn-Sequential"><span class="nav-number">5.5.</span> <span class="nav-text">5、nn.Sequential</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E4%BB%A5%E5%88%97%E8%A1%A8%E7%9A%84%E5%BD%A2%E5%BC%8F%EF%BC%8C%E4%B8%B2%E8%81%94%E5%87%BD%E6%95%B0%E8%BF%90%E7%AE%97%EF%BC%8C%E6%9E%84%E5%BB%BA%E4%B8%B2%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.5.1.</span> <span class="nav-text">5.1 以列表的形式，串联函数运算，构建串行执行的神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E4%BB%A5%E5%AD%97%E5%85%B8%E7%9A%84%E5%BD%A2%E5%BC%8F%EF%BC%8C%E4%B8%B2%E8%81%94%E5%87%BD%E6%95%B0%E8%BF%90%E7%AE%97%EF%BC%8C%E6%9E%84%E5%BB%BA%E4%B8%B2%E8%A1%8C%E6%89%A7%E8%A1%8C%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">5.5.2.</span> <span class="nav-text">5.2 以字典的形式，串联函数运算，构建串行执行的神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-%E4%B8%BE%E4%BE%8B"><span class="nav-number">5.5.3.</span> <span class="nav-text">5.3 举例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Torch-optim"><span class="nav-number">6.</span> <span class="nav-text">四、Torch.optim</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81Torch-autogard"><span class="nav-number">7.</span> <span class="nav-text">五、Torch.autogard</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81Torch-utils"><span class="nav-number">8.</span> <span class="nav-text">六、Torch.utils</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81torchvision"><span class="nav-number">9.</span> <span class="nav-text">七、torchvision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Torchvision-transforms-%E8%A7%A3%E6%9E%90"><span class="nav-number">9.1.</span> <span class="nav-text">1、Torchvision.transforms 解析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-transforms-Compose-%E7%B1%BB"><span class="nav-number">9.1.1.</span> <span class="nav-text">1.1 transforms.Compose()类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-transforms-Normalize-%EF%BC%89"><span class="nav-number">9.1.2.</span> <span class="nav-text">1.2 transforms.Normalize(）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81%E5%A4%84%E7%90%86%E5%9B%BE%E5%83%8F"><span class="nav-number">10.</span> <span class="nav-text">八、处理图像</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81matplotlib%E7%BB%98%E5%9B%BE%E5%BA%93"><span class="nav-number">11.</span> <span class="nav-text">九、matplotlib绘图库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95"><span class="nav-number">11.1.</span> <span class="nav-text">9.1 基础用法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-1-%E8%B5%B7%E6%AD%A5"><span class="nav-number">11.1.1.</span> <span class="nav-text">9.1.1 起步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-2-%E8%87%AA%E5%AE%9A%E4%B9%89%E9%A2%9C%E8%89%B2%E3%80%81%E7%BA%BF%E5%9E%8B%E5%92%8C%E7%82%B9%E5%9E%8B"><span class="nav-number">11.1.2.</span> <span class="nav-text">9.1.2 自定义颜色、线型和点型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-3-%E8%AE%BE%E7%BD%AE%E5%9D%90%E6%A0%87%E8%BD%B4%E8%8C%83%E5%9B%B4%E5%92%8C%E6%A0%87%E7%AD%BE"><span class="nav-number">11.1.3.</span> <span class="nav-text">9.1.3 设置坐标轴范围和标签</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-4-%E7%BB%98%E5%88%B6%E5%A4%9A%E4%B8%AA%E5%9B%BE%E5%BD%A2"><span class="nav-number">11.1.4.</span> <span class="nav-text">9.1.4 绘制多个图形</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-5-%E5%85%B6%E4%BB%96%E7%94%A8%E6%B3%95"><span class="nav-number">11.1.5.</span> <span class="nav-text">9.1.5 其他用法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2"><span class="nav-number">11.2.</span> <span class="nav-text">9.2</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#X%E3%80%81Sundry"><span class="nav-number">12.</span> <span class="nav-text">X、Sundry</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD-amp-%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">12.1.</span> <span class="nav-text">1、数据集下载 &amp; 构建数据集</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Moustache" src="/images/180-180.png"><p class="site-author-name" itemprop="name">Moustache</p><div class="site-description" itemprop="description">我是小胡子</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">78</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/hammerzer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hammerzer" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:stellar_lzu@163.com" title="E-Mail → mailto:stellar_lzu@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Chase</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">29:01</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.4" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JRehDoQ6pHXV1zKg09AMNLFt-gzGzoHsz',
      appKey     : 'cRAt4W15KiQdrIuHlQrRrtIl',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>