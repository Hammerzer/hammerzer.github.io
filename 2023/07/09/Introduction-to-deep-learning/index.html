<!DOCTYPE html><html lang="zh-CN"><head><script src="https://lib.sinaapp.com/js/jquery/1.7.2/jquery.min.js"></script><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/180-180.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/32-32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/16-16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hammerzer.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CONTENT OUTLINE深度学习入门 -⭐️ 环境相关 &amp; 预备知识 -⭐️ 线性回归 &#x2F; Softmax回归 -⭐️ 多层感知机 &#x2F; 模型相关基础"><meta property="og:type" content="article"><meta property="og:title" content="入门深度学习"><meta property="og:url" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/index.html"><meta property="og:site_name" content="Moustache&#39;s Blog"><meta property="og:description" content="CONTENT OUTLINE深度学习入门 -⭐️ 环境相关 &amp; 预备知识 -⭐️ 线性回归 &#x2F; Softmax回归 -⭐️ 多层感知机 &#x2F; 模型相关基础"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/5-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/6-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/6-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/6-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/6-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-5.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-6.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-7.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-8.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-9.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-10.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-11.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-12.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/8-13.jpg"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-5.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-6.jpg"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-7.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-8.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-9.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/9-9.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-5.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-6.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-17.jpg"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-18.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-19.jpg"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-20.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-21.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-22.jpg"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-7.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-8.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-9.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-10.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-11.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-12.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-13.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-14.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-15.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/10-16.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-5.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-6.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-7.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-8.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-9.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-10.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-11.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-12.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-13.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/11-14.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-2-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-2-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/12-5.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/13-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/13-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/13-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/13-4.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/14-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/14-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/14-3.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/15-1.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/15-2.png"><meta property="og:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/17-1.png"><meta property="article:published_time" content="2023-07-09T01:59:39.000Z"><meta property="article:modified_time" content="2025-01-19T03:19:16.613Z"><meta property="article:author" content="Moustache"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/5-1.png"><link rel="canonical" href="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>入门深度学习 | Moustache's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Moustache's Blog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Moustache's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">小胡子的私人空间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">36</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">79</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/180-180.png"><meta itemprop="name" content="Moustache"><meta itemprop="description" content="我是小胡子"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Moustache's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">入门深度学习</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-07-09 09:59:39" itemprop="dateCreated datePublished" datetime="2023-07-09T09:59:39+08:00">2023-07-09</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 11:19:16" itemprop="dateModified" datetime="2025-01-19T11:19:16+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a> </span></span><span id="/2023/07/09/Introduction-to-deep-learning/" class="post-meta-item leancloud_visitors" data-flag-title="入门深度学习" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> <span>℃</span> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2023/07/09/Introduction-to-deep-learning/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2023/07/09/Introduction-to-deep-learning/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>64k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>59 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CONTENT-OUTLINE"><a href="#CONTENT-OUTLINE" class="headerlink" title="CONTENT OUTLINE"></a>CONTENT OUTLINE</h2><p><span style="background:#ff0"><i class="fas fa-star-half-alt" style="margin-right:10px"></i>深度学习入门</span></p><p><span style="background:#f0a1a8">-⭐️</span> 环境相关 &amp; 预备知识</p><p><span style="background:#b3de4b">-⭐️</span> 线性回归 / Softmax回归</p><p><span style="background:#51c7db">-⭐️</span> 多层感知机 / 模型相关基础</p><span id="more"></span><h2 id="〇、目录"><a href="#〇、目录" class="headerlink" title="〇、目录"></a>〇、目录</h2><ul><li>安装虚拟机</li><li><code>Ubuntu</code>问题</li><li><code>Anaconda</code>相关</li><li>关于Github</li><li>跟沐神学搭建环境</li><li>数据操作</li><li>数据预处理</li><li>线性回归</li><li>Softmax 回归</li><li>多层感知机</li><li>模型选择与过/欠拟合</li><li>权重衰退</li><li>丢弃法</li><li>数值稳定性、模型初始化、激活函数</li></ul><br><h2 id="一、安装虚拟机"><a href="#一、安装虚拟机" class="headerlink" title="一、安装虚拟机"></a>一、安装虚拟机</h2><blockquote><p>安装虚拟机：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38797088">参考原文</a> 虚拟机下载VM16【公众号：软件科技汇】</p></blockquote><ul><li><p>下载<code>VMware Workstation Pro 16</code></p></li><li><p>虚拟机<code>ubuntu</code>镜像，我这里下载的是<code>Ubuntu 18.04</code><a target="_blank" rel="noopener" href="https://ubuntu.com/download/desktop">下载地址</a></p></li><li><p>在<code>VMware</code>中创建虚拟机【详细步骤见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/38797088">参考原文</a>】</p></li></ul><blockquote><p><code>Ubuntu.vmdk</code>文件：虚拟机磁盘文件</p></blockquote><h3 id="0、关于VMware"><a href="#0、关于VMware" class="headerlink" title="0、关于VMware"></a>0、关于VMware</h3><p>0.1 相关组合键</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">Ctrl</span> <span class="operator">+</span> <span class="variable">Alt</span> <span class="operator">+</span><span class="built_in">Enter</span> 虚拟机全屏</span><br></pre></td></tr></table></figure><h3 id="1、解决虚拟启动问题【关于BIOS设置】"><a href="#1、解决虚拟启动问题【关于BIOS设置】" class="headerlink" title="1、解决虚拟启动问题【关于BIOS设置】"></a>1、解决虚拟启动问题【关于BIOS设置】</h3><p>一般在<code>Advanced、Security、BIOS Features、Configuration</code>下面，找到<code>Intel Virtualization Technology</code>按回车键选择Enabled，表示开启。</p><p>进入<code>BIOS</code>之后选择某个选项按<code>Enter</code>是进入，按<code>Esc</code>是返回，按<code>F10</code>或<code>F4</code>保存。</p><blockquote><p>联想拯救者怎么进入<code>bios</code>？</p><p>1、联想拯救者在开机或重启状态下，在开机等待界面，连续按快捷键<code>F2</code>（或是<code>Fn+F2</code>）即可进入<code>bios</code>。</p><p>2、<code>nova</code>孔是联想拯救者快捷进入bios的方式之一，而且不仅是联想拯救者系列，联想旗下多个系列的产品都设置有该功能。用户在笔记本侧边找到<code>nova</code>孔，用针或牙签刺入，即可调出快捷菜单。选择<code>bios</code>进入即可。【右侧<code>USB</code>孔旁】</p></blockquote><br><h2 id="二、Ubuntu问题"><a href="#二、Ubuntu问题" class="headerlink" title="二、Ubuntu问题"></a>二、Ubuntu问题</h2><h3 id="0、小问题汇总"><a href="#0、小问题汇总" class="headerlink" title="0、小问题汇总"></a>0、小问题汇总</h3><h4 id="0-1-屏幕显示大小调整"><a href="#0-1-屏幕显示大小调整" class="headerlink" title="0.1 屏幕显示大小调整"></a>0.1 屏幕显示大小调整</h4><blockquote><p>屏幕显示大小调整： 设置→<code>Devices</code>→<code>Displays</code>→<code>Resolution</code></p></blockquote><h4 id="0-2-初始su密码错误"><a href="#0-2-初始su密码错误" class="headerlink" title="0.2 初始su密码错误"></a>0.2 初始su密码错误</h4><blockquote><p><a target="_blank" rel="noopener" href="https://www.yii666.com/article/605608.html?action=onAll">原文</a>解决</p></blockquote><p><code>Ubuntu</code>刚安装后，在终端中运行<code>su</code>命令要求输入密码，出现密码错误。</p><p>原因：<code>root</code>没有默认密码，需要手动设定。以安装<code>ubuntu</code>时输入的用户名登陆，该用户在<code>admin</code>组中，有权限给<code>root</code>设定密码</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo passwd [root] 或者 sudo passwd</span><br><span class="line"><span class="comment">#Enter后输入原始密码，新密码和确认密码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#退出root</span></span><br><span class="line"><span class="keyword">exit</span>  |  Ctrl+D</span><br></pre></td></tr></table></figure><h4 id="0-3-Ubuntu上修改主机名"><a href="#0-3-Ubuntu上修改主机名" class="headerlink" title="0.3 Ubuntu上修改主机名"></a>0.3 Ubuntu上修改主机名</h4><blockquote><p><a target="_blank" rel="noopener" href="https://www.python100.com/html/75774.html">原文</a></p></blockquote><ul><li><p>方法一：修改文件【需要先获取root权限】</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gedit /etc/hostname</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gedit /etc/hosts</span></span><br></pre></td></tr></table></figure></li><li><p>方法二：hostname文件的修改可使用命令完成【需要先获取root权限】</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">hostnamectl</span> <span class="built_in">set-hostname</span> <span class="string">new-hostname</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="0-4-Ubuntu安装一直卡黑屏"><a href="#0-4-Ubuntu安装一直卡黑屏" class="headerlink" title="0.4 Ubuntu安装一直卡黑屏"></a>0.4 Ubuntu安装一直卡黑屏</h4><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42081389/article/details/104072902">解决原文</a></p></blockquote><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Please <span class="built_in">remove</span> the installation medium then reboot</span><br><span class="line"><span class="comment">#Ubuntu 18.04 一直卡在黑屏</span></span><br><span class="line"><span class="comment">#解决：编辑虚拟机设置 → CD/DVD...关闭启动时连接</span></span><br><span class="line"><span class="comment">#注意：首次安装需打开以上设置</span></span><br></pre></td></tr></table></figure><h4 id="0-5-修改最佳apt下载服务器"><a href="#0-5-修改最佳apt下载服务器" class="headerlink" title="0.5 修改最佳apt下载服务器"></a>0.5 修改最佳<code>apt</code>下载服务器</h4><blockquote><p>可参考<a target="_blank" rel="noopener" href="https://jingyan.baidu.com/article/c1a3101eee72e79f646deb3d.html">此</a></p></blockquote><ul><li><code>Ubuntu18.04</code>：打开<code>Software and Update</code>，直接<code>stop</code>其自动检查，就会出现设置</li></ul><h4 id="0-6-错误：无法得到锁"><a href="#0-6-错误：无法得到锁" class="headerlink" title="0.6 错误：无法得到锁"></a>0.6 错误：无法得到锁</h4><p><strong>问题：</strong>新搭建的<code>Ubuntu</code>虚拟机，执行apt安装命令是报错<code>could not get lock /var/lib/dpkg/lock-frontend</code></p><p><strong>问题原因：</strong>存在另外一个进程在使用这个目录：<code>/var/lib/dpkg/lock-frontend</code>，【比如，正在更新软件<code>Cache</code>】。所以先确定是否还有其他人在使用软件管理程序或者在使用<code>apt</code>进行软件的安装，如果没有的话，可以直接删除/重命名被锁定的文件。</p><p><strong>解决：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> /var/lib/dpkg/lock-frontend</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">rm</span> /var/lib/dpkg/lock</span><br></pre></td></tr></table></figure><h4 id="0-7-Ubuntu打开终端时自动退出base环境"><a href="#0-7-Ubuntu打开终端时自动退出base环境" class="headerlink" title="0.7 Ubuntu打开终端时自动退出base环境"></a>0.7 Ubuntu打开终端时自动退出base环境</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda<span class="built_in"> config </span>--<span class="built_in">set</span> auto_activate_base <span class="literal">false</span></span><br></pre></td></tr></table></figure><h3 id="1、Ubuntu-分区号Sda的解释"><a href="#1、Ubuntu-分区号Sda的解释" class="headerlink" title="1、Ubuntu 分区号Sda的解释"></a>1、Ubuntu 分区号Sda的解释</h3><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Ftworld21/article/details/15707417?locationNum=8">参考原文</a></p></blockquote><p><strong>主分区</strong>：一块物理硬盘上可以被独立使用的一部分，一个硬盘最多可以有<strong>4个主分区</strong></p><p><strong>扩展分区</strong>：为了突破一个物理硬盘只能有4个分区的限制，引入了扩展分区。扩展分区和主分区的地位相当，但是扩展分区本身不能被直接使用，然而可以被继续划分成多个逻辑分区。</p><p><strong>逻辑分区</strong>：逻辑分区可以有任意多个，但是不能独立存在，多个连续的逻辑分区可做为一个扩展分区。一个硬盘只能有一个扩展分区。</p><p><strong>总结</strong>：也就是说，在一个物理硬盘上主分区和扩展分区加在一起<strong>最多仍然只有4个</strong>。但是扩展分区可以继续被划分成逻辑分区，而对多数用户而言，其实主分区和逻辑分区在使用上是没什么区别的。这样就达到了一快硬盘几乎可以有无限个分区的目的。</p><p><strong><code>LINUX</code>下分区实例分析：</strong></p><p>现在电脑上有一个<code>SCSI</code>硬盘，这时查看设备<code>ls /dev</code>，会发现有一个<code>sda</code>，如果是<code>IDE</code>硬盘，就是<code>hda</code>。</p><ul><li><p>分区方案一：4个主分区【这时候能看到：<code>sda,sda1,sda2,sda3,dsa4</code>】</p></li><li><p>分区方案二：一个主分区然后一个逻辑分区【这时候能看到：<code>sda,sda1,sda2,sda5</code>】</p><p>这里<code>sda</code>是物理硬盘，<code>sda1</code>是主分区，<code>sda2</code>是扩展分区，<code>sda5</code>是逻辑分区（正是因为必须保留4个数字给主分区和扩展分区使用，所以逻辑分区的数字必须从5开始）。</p></li><li><p>分区方案三：一个逻辑分区<br>这里能看到：sda，sda1，sda5<br>见到这些数字不要害怕，这样一解释就很容易理解了。如果有多块物理硬盘就会出现sdb，sdc</p></li></ul><h3 id="2、Ubuntu快捷键"><a href="#2、Ubuntu快捷键" class="headerlink" title="2、Ubuntu快捷键"></a>2、Ubuntu快捷键</h3><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Ctrl</span> + Alt + T 快速打开终端</span><br><span class="line"><span class="symbol">Ctrl</span> + Shift + W 关闭终端</span><br><span class="line"><span class="symbol">Ctrl</span> + L 快速清除终端屏幕</span><br><span class="line"></span><br><span class="line"><span class="symbol">Alt</span> + Tab 切换程序</span><br><span class="line"><span class="symbol">Alt</span> + Tab + Shift 逆向切换程序</span><br><span class="line"><span class="symbol">Alt</span> +<span class="built_in">F4</span> 关闭窗口</span><br><span class="line"></span><br><span class="line"><span class="symbol">Alt</span> + <span class="built_in">F2</span> 打开命令窗口，可输入命令启动应用程序，如gedit（文本编辑器）    </span><br><span class="line"><span class="symbol">Ctrl</span> + Alt + <span class="built_in">F7</span> 切换到命令行界面</span><br><span class="line"><span class="symbol">Ctrl</span> + Alt + <span class="built_in">F2</span> 切换到图形界面</span><br><span class="line"></span><br><span class="line"><span class="symbol">Ctrl</span> + Shift +C/V 终端的复制粘贴</span><br></pre></td></tr></table></figure><blockquote><p><code>vim</code>或命令行下：鼠标左键选中即为复制， 按下中键（鼠标滚轮）即为粘贴</p></blockquote><h3 id="3、终端命令"><a href="#3、终端命令" class="headerlink" title="3、终端命令"></a>3、终端命令</h3><blockquote><p><code>Ubuntu</code>为<code>Linux</code>内核的，故<code>Linux</code>命令均可用于<code>Ubuntu</code></p></blockquote><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">chase@chaseVM:~S   ----- <span class="comment">#用户名@计算机名:绝对路径$</span></span><br><span class="line"></span><br><span class="line">su ----- 获取root权限【<span class="keyword">exit</span>推出 | Ctrl+D退出】</span><br><span class="line">sudo xxx ----- 以root权限执行</span><br><span class="line"></span><br><span class="line">dpkg -s &lt;软件包名&gt; ----- 验证某个软件包是否已经安装【dpkg -s vim】</span><br><span class="line">apt list --installed ----- 列出apt已安装的程序及依赖</span><br><span class="line">apt list --installed | grep program_name ----- 使用grep进行过滤</span><br><span class="line"></span><br></pre></td></tr></table></figure><br><h2 id="三、Ananconda相关"><a href="#三、Ananconda相关" class="headerlink" title="三、Ananconda相关"></a>三、Ananconda相关</h2><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_74055982/article/details/140220250">conda环境变量+常用操作+配置镜像源</a></p></blockquote><h3 id="1-Anaconda修改国内镜像源"><a href="#1-Anaconda修改国内镜像源" class="headerlink" title="1 Anaconda修改国内镜像源"></a>1 Anaconda修改国内镜像源</h3><h4 id="1-1-通过-conda-config-命令生成配置文件"><a href="#1-1-通过-conda-config-命令生成配置文件" class="headerlink" title="1.1 通过 conda config 命令生成配置文件"></a>1.1 通过 conda config 命令生成配置文件</h4><blockquote><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/042fd657e2d4?tdsourcetag=s_pcqq_aiomsg">原文1</a> <a target="_blank" rel="noopener" href="https://mirrors4.tuna.tsinghua.edu.cn/help/anaconda/">原文2 Nice</a></p></blockquote><p>这里，我们使用清华的<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/">镜像</a></p><p>首先，在CMD命令行输入以下两条命令：</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda config --<span class="keyword">add</span> channels https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line"></span><br><span class="line">conda config --<span class="keyword">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><h4 id="1-2-修改配置文件"><a href="#1-2-修改配置文件" class="headerlink" title="1.2 修改配置文件"></a>1.2 修改配置文件</h4><p>删除上述配置文件 .condarc 中的第三行，然后保存，最终版本文件如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">show_channel_urls: <span class="literal">true</span></span><br></pre></td></tr></table></figure><h4 id="1-3-查看是否生效"><a href="#1-3-查看是否生效" class="headerlink" title="1.3 查看是否生效"></a>1.3 查看是否生效</h4><p>通过命令 conda info 查看当前配置信息，内容如下，即修改成功，关注 channel URLs 字段内容</p><figure class="highlight csharp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;conda info</span><br><span class="line">Current conda install:</span><br><span class="line"></span><br><span class="line">               platform : win<span class="number">-32</span></span><br><span class="line">          conda version : <span class="number">4.3</span><span class="number">.22</span></span><br><span class="line">       conda <span class="keyword">is</span> <span class="keyword">private</span> : False</span><br><span class="line">      conda-env version : <span class="number">4.3</span><span class="number">.22</span></span><br><span class="line">       requests version : <span class="number">2.12</span><span class="number">.4</span></span><br><span class="line">           channel URLs : https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/win-32</span></span><br><span class="line">                          https:<span class="comment">//mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/noarch</span></span><br></pre></td></tr></table></figure><h3 id="2-Anaconda常用命令小结"><a href="#2-Anaconda常用命令小结" class="headerlink" title="2 Anaconda常用命令小结"></a>2 Anaconda常用命令小结</h3><blockquote><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/509771770">Conda相关指令1.2</a></p></blockquote><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">##查看当前conda工具版本号</span><br><span class="line">conda --version</span><br><span class="line"></span><br><span class="line">##查看包括版本的更多信息</span><br><span class="line">conda info</span><br><span class="line"></span><br><span class="line">##更新conda至最新版本</span><br><span class="line">conda update conda</span><br><span class="line"></span><br><span class="line">##查看conda帮助信息</span><br><span class="line">conda -h</span><br></pre></td></tr></table></figure><h4 id="2-1-环境管理相关"><a href="#2-1-环境管理相关" class="headerlink" title="2.1 环境管理相关"></a>2.1 环境管理相关</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看conda环境管理命令帮助信息</span></span><br><span class="line">conda create --help</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建出来的虚拟环境所在的位置为conda路径下的env/文件下,默认创建和当前python版本一致的环境</span></span><br><span class="line">conda create --name envname</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建新环境时指定python版本为3.6，环境名称为python36</span></span><br><span class="line">conda create --name python36 <span class="attribute">python</span>=3.6</span><br><span class="line"></span><br><span class="line"><span class="comment">#切换到环境名为python36的环境（默认是base环境），切换后可通过python -V查看是否切换成功</span></span><br><span class="line">conda activate python36</span><br><span class="line"></span><br><span class="line"><span class="comment">#返回前一个python环境</span></span><br><span class="line">conda deactivate</span><br><span class="line"></span><br><span class="line"><span class="comment">#显示已创建的环境，会列出所有的环境名和对应路径</span></span><br><span class="line">conda <span class="built_in">info</span> -e</span><br><span class="line">conda env list</span><br><span class="line"></span><br><span class="line"><span class="comment">#删除虚拟环境</span></span><br><span class="line">conda <span class="built_in">remove</span> --name envname --all</span><br><span class="line">conda env <span class="built_in">remove</span> -n [envname]</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定python版本,以及多个包</span></span><br><span class="line">conda create -n envname <span class="attribute">python</span>=3.4 <span class="attribute">scipy</span>=0.15.0 astroib numpy</span><br></pre></td></tr></table></figure><p>查看当前环境安装的包</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda list   ##获取当前环境中已安装的包</span><br><span class="line">conda list -n python36   ##获取指定环境中已安装的包</span><br><span class="line">conda list xxx ##查看某个包是否被安装</span><br></pre></td></tr></table></figure><p>克隆一个环境</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># clone_env 代指克隆得到的新环境的名称</span><br><span class="line"># envname 代指被克隆的环境的名称</span><br><span class="line">conda create --name clone_env --clone envname</span><br><span class="line"></span><br><span class="line">#查看conda环境信息</span><br><span class="line">conda info --envs</span><br></pre></td></tr></table></figure><p>构建相同的conda环境(不通过克隆的方法)</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 查看包信息</span><br><span class="line">conda list --explicit</span><br><span class="line"></span><br><span class="line"># 导出包信息到当前目录, spec-file.txt为导出文件名称,可以自行修改名称</span><br><span class="line">conda list --explicit &gt; spec-file.txt</span><br><span class="line"></span><br><span class="line"># 使用包信息文件建立和之前相同的环境</span><br><span class="line">conda create --name newenv --file spec-file.txt</span><br><span class="line"></span><br><span class="line"># 使用包信息文件向一个已经存在的环境中安装指定包</span><br><span class="line">conda install --name newenv --file spec-file.txt</span><br></pre></td></tr></table></figure><p>查找包</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#模糊查找，即模糊匹配，只要含py字符串的包名就能匹配到</span><br><span class="line">conda search py   </span><br><span class="line"></span><br><span class="line">##查找包，--full-name表示精确查找，即完全匹配名为python的包</span><br><span class="line">conda search --full-name python</span><br></pre></td></tr></table></figure><p>安装更新删除包</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">##在当前环境中安装包</span><br><span class="line">conda install scrapy  </span><br><span class="line"></span><br><span class="line">##在指定环境中安装包</span><br><span class="line">conda install -n python36 scrapy</span><br><span class="line"></span><br><span class="line">##在当前环境中更新包  </span><br><span class="line">conda update scrapy   </span><br><span class="line"></span><br><span class="line">##在指定环境中更新包</span><br><span class="line">conda update -n python36 scrapy  </span><br><span class="line"></span><br><span class="line">##更新当前环境所有包</span><br><span class="line">conda update --all   </span><br><span class="line"></span><br><span class="line">##在当前环境中删除包</span><br><span class="line">conda remove scrapy   </span><br><span class="line"></span><br><span class="line">##在指定环境中删除包</span><br><span class="line">conda remove -n python2 scrapy</span><br></pre></td></tr></table></figure><h4 id="2-2-Python管理相关"><a href="#2-2-Python管理相关" class="headerlink" title="2.2 Python管理相关"></a>2.2 Python管理相关</h4><blockquote><p><a target="_blank" rel="noopener" href="https://www.yutu.cn/article/news_43906.html">卸载python</a></p><p>卸载本地<code>python</code>后，依然可以直接使用来自<code>Anaconda</code>的<code>python</code>命令</p></blockquote><p>查找可以安装的python</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 查找所有名称包含python的包</span><br><span class="line">conda search python</span><br><span class="line"></span><br><span class="line"># 查找全名为python的包</span><br><span class="line">conda search --full-name python</span><br></pre></td></tr></table></figure><p>安装不同版本的Python</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#在不影响当前版本的情况下,新建环境并安装不同版本的python</span><br><span class="line">#新建一个Python版本为3.6 名称为 py36 的环境</span><br><span class="line"></span><br><span class="line">conda create -n py36 python=3.6 anaconda</span><br><span class="line"></span><br><span class="line">#注:将py36替换为您要创建的环境的名称。 anaconda是元数据包，带这个会把base的基础包一起安装，不带的话新环境只包含python3.6相关的包。 python = 3.6是您要在此新环境中安装的软件包和版本。 这可以是任何包，例如numpy = 1.7，或多个包。</span><br><span class="line">#然后激活想要使用的环境即可</span><br><span class="line">conda activate py36</span><br><span class="line">#更新Python</span><br><span class="line"># 普通的更新python</span><br><span class="line">conda update python</span><br><span class="line"></span><br><span class="line"># 将python更新到另外一个版本/安装指定版本的python</span><br><span class="line">conda install python=3.6</span><br></pre></td></tr></table></figure><h3 id="3-分享环境"><a href="#3-分享环境" class="headerlink" title="3 分享环境"></a>3 分享环境</h3><p>如果你想把你当前的环境配置与别人分享，这样ta可以快速建立一个与你一模一样的环境（同一个版本的python及各种包）来共同开发/进行新的实验。一个分享环境的快速方法就是给ta一个你的环境的<code>.yml</code>文件</p><p>首先通过activate <code>target_env</code>要分享的环境<code>target_env</code>，然后输入下面的命令会在当前工作目录下生成一个<code>environment.yml</code>文件</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env export &gt; environment.yml</span><br></pre></td></tr></table></figure><p>小伙伴拿到<code>environment.yml</code>文件后，将该文件放在工作目录下，可以通过以下命令从该文件创建环境</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f environment.yml</span><br></pre></td></tr></table></figure><h3 id="4-Anaconda-卸载-重新安装"><a href="#4-Anaconda-卸载-重新安装" class="headerlink" title="4 Anaconda 卸载+重新安装"></a>4 Anaconda 卸载+重新安装</h3><blockquote><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/623680942?utm_id=0&wd=&eqid=d6ccf802000123510000000664786f76">原文</a></p><p><a target="_blank" rel="noopener" href="https://learning.anaconda.cloud/get-started-with-anaconda">官方入门论坛</a></p><p><a target="_blank" rel="noopener" href="https://www.anaconda.com/installation-success?source=installer">Welcome to Anaconda</a></p></blockquote><p>1、彻底卸载<code>conda</code></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">conda</span> <span class="literal">info</span></span><br><span class="line"><span class="comment">#第一步查看conda 中envs文件夹和pkgs文件夹位置</span></span><br><span class="line">conda install anaconda-clean</span><br><span class="line"><span class="comment">#第二部下载官方卸载工具</span></span><br><span class="line">anaconda-clean --<span class="literal">yes</span></span><br><span class="line"><span class="comment">#第三步使用工具进行卸载和清理</span></span><br></pre></td></tr></table></figure><p>执行上述命令之后通过anaconda本地的卸载程序，删除<code>envs</code>文件夹和<code>pkgs</code>文件夹</p><p>点击<code>windows</code>开始按钮，找到<code>Anaconda</code>，右击选择卸载，在程序和功能中找到他卸载</p><p>2、安装<code>Anaconda</code></p><p>在<a target="_blank" rel="noopener" href="https://www.anaconda.com/">官网</a>中下载软件后正常安装</p><p>3、设置<code>windows</code>虚拟环境</p><p>安装好<code>Anaconda</code>后，在开始里面找到<code>Anaconda</code>文件夹下的<code>Anaconda Prompt</code>，左边显示<code>base</code>，说明是<code>base</code>环境，点击<code>Ctrl+z</code>退出，输入 <code>conda create -n AIE31</code>，即设置名为<code>AIE31</code>的虚拟环境，回车进行安装。</p><p>输入<code>conda activate AIE31</code>，进入虚拟环境</p><p>4、安装numpy</p><p>输入<code>conda install numpy</code> 安装<code>numpy</code>包，回车。出现以下显示，表示安装成功。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># All requested packages already installed</span><br></pre></td></tr></table></figure><p>5、<code>conda</code>命令环境变量</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#将以下环境变量，相应用户的目录可能会有所不同，但是后面的路径名相同。</span></span><br><span class="line"><span class="symbol">D:</span>\anaconda</span><br><span class="line"><span class="symbol">D:</span>\anaconda\Scripts</span><br><span class="line"><span class="symbol">D:</span>\anaconda\Library\bin</span><br><span class="line"><span class="symbol">D:</span>\anaconda\Library\mingw-w64\bin</span><br></pre></td></tr></table></figure><blockquote><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/cenariusxz/p/10600703.html">anaconda的python版本与本地python版本不同时的问题</a></p></blockquote><h3 id="5-Jupter-Notebook"><a href="#5-Jupter-Notebook" class="headerlink" title="5 Jupter Notebook"></a>5 Jupter Notebook</h3><blockquote><p>Python相关的编辑器：</p><ul><li>Jupyter：基于<code>iPython</code></li><li>Thonny：逐步执行 ，显示变量变化情况</li><li>Spyder：基于<code>iPython</code><span style="background:#ff0">【在cmd中输入 <code>idle</code>，可直接调处<code>ipython</code>】</span></li></ul></blockquote><h4 id="5-1-下载安装"><a href="#5-1-下载安装" class="headerlink" title="5.1 下载安装"></a>5.1 下载安装</h4><p>安装 Anaconda 全家桶，或者直接下载 Jupyter Notebook</p><p>在命令行输入该命令下载 Jupyter Netbook</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> jupyter</span><br></pre></td></tr></table></figure><p>如果下载失败可以添加下载镜像</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyter -<span class="selector-tag">i</span> https:<span class="comment">//pypi.tuna.tsinghua.edu.cn/simple</span></span><br></pre></td></tr></table></figure><p>使用 pip 安装之后，可以在指定文件夹中使用命令开启 Jupyter</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">jupyter notebook</span></span><br></pre></td></tr></table></figure><h4 id="5-2-使用Tips"><a href="#5-2-使用Tips" class="headerlink" title="5.2 使用Tips"></a>5.2 使用Tips</h4><h5 id="5-2-0-A-Few"><a href="#5-2-0-A-Few" class="headerlink" title="5.2.0 A Few"></a>5.2.0 A Few</h5><ul><li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Q4411H7fJ/?spm_id_from=333.337.search-card.all.click&vd_source=ad866fe26d18693e4132a3c33f8fba36">Jupyter操作入门-blibili</a> | <a target="_blank" rel="noopener" href="https://github.com/TommyZihao/zihaopython/blob/master/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%EF%BC%9A%E5%AD%A6%E4%B9%A0%E6%97%B6%E9%97%B4%E4%B8%8E%E6%88%90%E7%BB%A9%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%88%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%89/Jupyter%20notebook%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B.md">Jupyter快速上手-Github</a>：Jupyter上手</p></li><li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Q4411H7fJ/?p=2&spm_id_from=pageDriver&vd_source=ad866fe26d18693e4132a3c33f8fba36">数据分析可视化案例-线性回归-bilibili</a>：使用Excel进行简单的数据分析 | 使用Python进行机器学</p></li></ul><h5 id="5-2-1-在特定文件夹运行"><a href="#5-2-1-在特定文件夹运行" class="headerlink" title="5.2.1 在特定文件夹运行"></a>5.2.1 在特定文件夹运行</h5><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以E盘文件为例</span></span><br><span class="line">E：</span><br><span class="line">cd E:<span class="string">\About</span> Chase<span class="string">\Art</span> <span class="keyword">of</span> Programming-<span class="number">2023</span><span class="string">\d2l-zh\pytorch</span></span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><blockquote><p>更改Jupyter默认存储位置：参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/yellow_hill/article/details/130538583">原文</a></p></blockquote><h5 id="5-2-2-魔法函数"><a href="#5-2-2-魔法函数" class="headerlink" title="5.2.2 魔法函数"></a>5.2.2 魔法函数</h5><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib <span class="keyword">inline</span> : 使得matplotlib生成的图可以嵌入或内联显示</span><br></pre></td></tr></table></figure><h5 id="5-2-3-数学公式之LaTex"><a href="#5-2-3-数学公式之LaTex" class="headerlink" title="5.2.3 数学公式之LaTex"></a>5.2.3 数学公式之LaTex</h5><blockquote><p><a target="_blank" rel="noopener" href="https://www.latexlive.com/">LaTex在线编辑器</a></p></blockquote><ul><li><p>单<code>$</code>符号不对齐，为内联公式块</p></li><li><p>双<code>$</code>符号居中，为单行公式块</p></li></ul><br><h4 id="5-3-在-Jupyter-中切换使用-conda-虚拟环境"><a href="#5-3-在-Jupyter-中切换使用-conda-虚拟环境" class="headerlink" title="5.3 在 Jupyter 中切换使用 conda 虚拟环境"></a>5.3 在 <code>Jupyter</code> 中切换使用 <code>conda</code> 虚拟环境</h4><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014264373/article/details/119390267?spm=1001.2014.3001.5506">参考此处</a></p></blockquote><p>服务器上配置有多个 <code>conda</code> 虚拟环境，在使用<code>Jupyter notebook</code>时需要使用其中的一个环境，但是其默认还是使用 <code>base</code> 环境</p><p><code>Jupyter</code> 在一个名为 <code>kernel</code> 的单独进程中运行用户的代码。<code>kernel</code> 可以是不同的 <code>Python</code> 安装在不同的 <code>conda</code> 环境或虚拟环境，甚至可以是不同语言（例如 Julia 或 R）的解释器。</p><p>简而言之，如何使用 conda 环境和 Jupyter 有三种选择：</p><ul><li>在 <code>conda</code> 环境中运行 Jupyter 服务器和内核</li><li>为 <code>conda</code> 环境创建特殊内核</li><li>使用 <code>nb_conda_kernels</code> 添加所有环境</li></ul><h5 id="5-3-1-在-conda-环境中运行-Jupyter-服务器和内核"><a href="#5-3-1-在-conda-环境中运行-Jupyter-服务器和内核" class="headerlink" title="5.3.1 在 conda 环境中运行 Jupyter 服务器和内核"></a>5.3.1 在 <code>conda</code> 环境中运行 <code>Jupyter</code> 服务器和内核</h5><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda create -n my-conda-env   # creates new virtual env</span><br><span class="line">conda activate my-conda-env    # activate environment <span class="keyword">in</span> terminal</span><br><span class="line">conda install jupyter     # install jupyter + notebook</span><br><span class="line">jupyter notebook       # <span class="built_in">start</span> server + kernel</span><br></pre></td></tr></table></figure><p>【推荐指数⭐️⭐️】</p><p>这种方法就是为每一个 <code>conda</code> 环境 都安装 <code>jupyter</code>。</p><p><code>Jupyter</code> 将完全安装在 <code>conda</code> 环境中。不同版本的 <code>Jupyter</code> 可用于不同的 <code>conda</code> 环境，但此选项可能有点矫枉过正</p><p>在环境中包含内核就足够了，内核是运行代码的封装 <code>Python</code> 的组件。<code>Jupyter notebook</code> 的其余部分可以被视为编辑器或查看器，并且没有必要为每个环境单独安装它并将其包含在每个 env.yml 文件中。因此，接下来的两个选项之一可能更可取，但这是最简单的一个，绝对没问题。</p><h5 id="5-3-2-为-conda-环境创建特殊内核"><a href="#5-3-2-为-conda-环境创建特殊内核" class="headerlink" title="5.3.2 为 conda 环境创建特殊内核"></a>5.3.2 为 <code>conda</code> 环境创建特殊内核</h5><p>【推荐指数 ⭐️⭐️⭐️⭐️】</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda create -n my-conda-env    # creates new virtual env</span><br><span class="line">conda activate my-conda-env     # activate environment <span class="keyword">in</span> terminal</span><br><span class="line">conda install ipykernel      # install Python kernel <span class="keyword">in</span> new conda env</span><br><span class="line">ipython kernel install --user --name=my-conda-env-kernel  # configure Jupyter to use Python kernel</span><br><span class="line">jupyter notebook      # run jupyter from system</span><br></pre></td></tr></table></figure><p>只有 Python 内核会在 conda 环境中运行，系统中的 Jupyter 或不同的 conda 环境将被使用——它没有安装在 conda 环境中。</p><p>通过调用<code>ipython kernel install</code>将 jupyter 配置为使用 conda 环境作为内核.</p><blockquote><p>具体的操作 ：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u014264373/article/details/86541767">windows/mac/linux jupyter notebook 切换默认环境</a></p></blockquote><h5 id="5-3-3-使用-nb-conda-kernels-添加所有环境"><a href="#5-3-3-使用-nb-conda-kernels-添加所有环境" class="headerlink" title="5.3.3 使用 nb_conda_kernels 添加所有环境"></a>5.3.3 使用 <code>nb_conda_kernels</code> 添加所有环境</h5><p>【推荐指数 ⭐️⭐️⭐️⭐️⭐️】</p><p>第二种方法其实也挺不错的。有个缺点是，你新建一个环境，就要重复操作一次。而这个方法就是一键添加所有 conda 环境，且不妙哉！</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">conda activate my-conda-env    # this is the environment <span class="keyword">for</span> your project and code</span><br><span class="line">conda install ipykernel</span><br><span class="line">conda deactivate</span><br><span class="line"></span><br><span class="line">conda activate base      # could be also some other environment</span><br><span class="line">conda install nb_conda_kernels</span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong>：这里的 <code>conda install nb_conda_kernels</code> 是在 <code>base</code> 环境下操作的</p><p><strong>警告</strong>：只能在<code>base</code>环境下执行上操作，否则会报 <code>Internet Error 500</code>，且此错误未解决</p></blockquote><p>安装好后，打开 <code>jupyter notebook</code> 就会显示所有的 conda 环境啦，点击随意切换【问题是切换即新建空文档】</p><br><h3 id="6-conda-install-与-pip-install-的区别"><a href="#6-conda-install-与-pip-install-的区别" class="headerlink" title="6 conda install 与 pip install 的区别"></a>6 <a target="_blank" rel="noopener" href="https://www.cnblogs.com/chentiao/p/17356549.html">conda install 与 pip install 的区别</a></h3><p><code>conda install</code>和<code>pip install</code>一般被认为是几乎相同的，但这两个工具虽然功能存在部分重叠，但其设计的目的是不同的。</p><h4 id="6-1-使用区别"><a href="#6-1-使用区别" class="headerlink" title="6.1 使用区别"></a>6.1 使用区别</h4><p><strong>支持语言：</strong></p><ul><li><code>pip</code> 是 <code>python</code> 官方推荐的包下载工具，但是只能安装python包</li><li><code>conda</code> 是一个跨平台（支持<code>linux, mac, win</code>）的通用包和环境管理器，它除了支持<code>python</code>外，还能安装各种其他语言的包，例如 C/C++, R语言等</li></ul><p><strong>Repo源：</strong></p><ul><li><code>pip</code> 从<code>PyPI</code>（<code>Python Package Index</code>）上拉取数据。上面的<strong>数据更新更及时，涵盖的内容也更加全面</strong></li><li><code>conda</code> 从 <code>Anaconda.org</code> 上拉取数据。虽然<code>Anaconda</code>上有一些主流<code>Python</code>包，但在数量级上明显少于<code>PyPI</code>，缺少一些小众的包</li></ul><p><strong>包的内容</strong></p><ul><li><code>pip</code> 里的软件包为wheel版或源代码发行版。<code>wheel</code>属于已编译发新版的一种，下载好后可以直接使用；而源代码发行版必须要经过编译生成可执行程序后才能使用，编译的过程是在用户的机子上进行的，<strong>需要注意的是源码格式的包通常需要系统内安装了兼容的编译工具和所需要的库文件。</strong></li><li>conda 里的软件包都是<strong>二进制文件</strong>，下载后即可使用，<strong>不需要经过编译</strong></li></ul><p><strong>环境隔离</strong></p><ul><li><code>pip</code> 没有内置支持环境隔离，只能借助其他工具例如<code>virtualenv</code> or <code>venv</code>实现环境隔离</li><li><code>conda</code> 有能力直接创建隔离的环境</li></ul><p><strong>依赖关系</strong></p><ul><li><code>pip</code>安装包时，尽管也对当前包的依赖做检查，但是并不保证当前环境的所有包的所有依赖关系都同时满足。当某个环境所安装的包越来越多，产生冲突的可能性就越来越大。</li><li><code>pip install</code>通过循环递归的方式安装依赖，这一点我们可以理解为只要碰到没有安装的依赖包就进行安装，直到安装完毕，很粗暴简单，但是随之带来的问题就是安装过程不稳定，比如安装过程中先安装的软件包的依赖和后安装的软件包的依赖产生冲突，整个安装过程就会崩坏。</li><li><code>conda</code>会检查当前环境下所有包之间的依赖关系，保证当前环境里的所有包的所有依赖都会被满足</li></ul><h4 id="6-2-库的储存位置"><a href="#6-2-库的储存位置" class="headerlink" title="6.2 库的储存位置"></a>6.2 库的储存位置</h4><ul><li>在<code>conda</code>虚拟环境下使用 <code>pip install</code> 安装的库： 如果使用系统的的python，则库会被保存在 <code>~/.local/lib/python3.x/site-packages</code> 文件夹中；如果使用的是conda内置的python，则会被保存到 <code>anaconda3/envs/current_env/lib/site-packages</code>中。</li><li><code>conda install</code> 安装的库都会放在<code>anaconda3/pkgs</code>目录下。这样的好处就是，当在某个环境下已经下载好了某个库，再在另一个环境中还需要这个库时，就可以直接从<code>pkgs</code>目录下将该库复制至新环境而<strong>不用重复下载</strong></li></ul><h4 id="6-3-使用总结"><a href="#6-3-使用总结" class="headerlink" title="6.3 使用总结"></a>6.3 使用总结</h4><p>推荐使用conda创建虚拟环境，能用conda安装的就先用conda（conda下载是真的慢，就算换源了还是慢，下大点的文件还容易失败），不行再使用pip安装。</p><br><h3 id="7-错误解决"><a href="#7-错误解决" class="headerlink" title="7 错误解决"></a>7 错误解决</h3><h4 id="7-1-conda命令不能使用"><a href="#7-1-conda命令不能使用" class="headerlink" title="7.1 conda命令不能使用"></a>7.1 conda命令不能使用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">C:\Users\Stell&gt;conda activate d2l-zh</span></span><br><span class="line"></span><br><span class="line">CommandNotFoundError: Your shell has not been properly configured to use &#x27;conda activate&#x27;.</span><br><span class="line">If using &#x27;conda activate&#x27; from a batch script, change your</span><br><span class="line">invocation to &#x27;CALL conda.bat activate&#x27;.</span><br></pre></td></tr></table></figure><p>原因：其实就是指令没找到，出现错误。要么是conda环境安装有问题，要么是没有在conda的环境下去使用“conda activate”这个命令。安装一般是没有问题的，那么在shell环境下应该要先启动conda的base环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">一种方式</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">C:\Users\Stell&gt;activate</span></span><br><span class="line"><span class="meta prompt_">&gt; </span><span class="language-bash">(base) C:\Users\Stell&gt;</span></span><br></pre></td></tr></table></figure><p>解决：使用<code>Anacodna</code>提供的 <code>conda prompt</code> 解释器来执行命令</p><br><h4 id="7-2-Conda-启动报错-Invoke-Expression"><a href="#7-2-Conda-启动报错-Invoke-Expression" class="headerlink" title="7.2 Conda 启动报错 Invoke-Expression"></a>7.2 Conda 启动报错 Invoke-Expression</h4><p>第一次启动的时候报错【win11】</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-Expression : 所在位置 行:1 字符: 1097 + ... \Installation\mingw64\bin;C:\Installation\node_js;&quot;C:\Program Files\J ... + ~~~~~~~~~~ 表达式或语句中包含意外的标记“C:\Program”。 所在位置 行:1 字符: 1172 + ... k-1.8\bin;C:\Program Files\Java\jdk-1.8\jre\bin;&quot;;&quot;C:\Program Files\M ... + ~~~~~~~~~~ 表达式或语句中包含意外的标记“C:\Program”。 所在位置 行:1 字符: 1219 + ... ;&quot;;&quot;C:\Program Files\MySQL\MySQL Server 8.0\bin;&quot;;&quot;C:\Program Files\J ... + ~~~~~~~~~~ 表达式或语句中包含意外的标记“C:\Program”。 所在位置 C:\ProgramData\miniconda3\shell\condabin\Conda.psm1:76 字符: 9 + Invoke-Expression -Command $activateCommand; + ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : ParserError: (:) [Invoke-Expression], ParseException + FullyQualifiedErrorId : UnexpectedToken,Microsoft.PowerShell.Commands.InvokeExpressionCommand</span><br></pre></td></tr></table></figure><p>原因：之前配置 java 环境变量的时候，在一行里写了两个路径，用了分号隔开，导致 conda 启动的时候解析环境变量的时候出错。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">%</span><span class="language-bash">JAVA_HOME%\bin;%JAVA_HOME%\jre\bin;</span></span><br><span class="line">// 环境变量分行写即可解决</span><br></pre></td></tr></table></figure><br><h2 id="四、关于Github"><a href="#四、关于Github" class="headerlink" title="四、关于Github"></a>四、关于Github</h2><br><h2 id="五、跟沐神学搭建环境"><a href="#五、跟沐神学搭建环境" class="headerlink" title="五、跟沐神学搭建环境"></a>五、跟沐神学搭建环境</h2><blockquote><p>课程中使用云服务器中进行演示，使用ubuntu</p><p>问：建议使用win开发，还是下虚拟机，在ubuntu中开发？</p></blockquote><h3 id="0、Little-Problems"><a href="#0、Little-Problems" class="headerlink" title="0、Little Problems"></a>0、Little Problems</h3><h4 id="0-1-pip安装模块报错"><a href="#0-1-pip安装模块报错" class="headerlink" title="0.1 pip安装模块报错"></a>0.1 pip安装模块报错</h4><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#错误</span></span><br><span class="line">ERRPR:pip install Could not fetch URL</span><br><span class="line"><span class="comment">#原因：被墙</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#解决1</span></span><br><span class="line">pip install xxx -i http:<span class="regexp">//</span>pypi.douban.com/simple --trusted-host pypi.douban.com</span><br><span class="line"></span><br><span class="line"><span class="comment">#解决2</span></span><br><span class="line">pip install xxx -i http:<span class="regexp">//</span>pypi.douban.com/simple</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">豆瓣(douban) http:<span class="regexp">//</span>pypi.douban.com<span class="regexp">/simple/</span></span><br><span class="line">阿里云 http:<span class="regexp">//mi</span>rrors.aliyun.com<span class="regexp">/pypi/</span>simple/</span><br><span class="line">清华大学 https:<span class="regexp">//</span>pypi.tuna.tsinghua.edu.cn<span class="regexp">/simple/</span></span><br><span class="line">中国科学技术大学 http:<span class="regexp">//</span>pypi.mirrors.ustc.edu.cn<span class="regexp">/simple/</span></span><br></pre></td></tr></table></figure><h4 id="0-2-安装torch时进程被杀"><a href="#0-2-安装torch时进程被杀" class="headerlink" title="0.2 安装torch时进程被杀"></a>0.2 安装torch时进程被杀</h4><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#错误：pip install torch 时，进程直接被killed</span></span><br><span class="line"><span class="comment">#原因：torch已经被安装过了</span></span><br><span class="line"><span class="comment">#解决:</span></span><br><span class="line"><span class="attribute">pip</span> install torch --<span class="literal">no</span>-cache-dir</span><br></pre></td></tr></table></figure><h4 id="0-3-Ubuntu无法安装-git"><a href="#0-3-Ubuntu无法安装-git" class="headerlink" title="0.3 Ubuntu无法安装 git"></a>0.3 Ubuntu无法安装 git</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(base) chase@chase-virtual-machine:~$ <span class="built_in">sudo</span> apt install git</span><br><span class="line">[<span class="built_in">sudo</span>] password <span class="keyword">for</span> chase: </span><br><span class="line">Reading package lists... Done</span><br><span class="line">Building dependency tree       </span><br><span class="line">Reading state information... Done</span><br><span class="line">Package git is not available, but is referred to by another package.</span><br><span class="line">This may mean that the package is missing, has been obsoleted, or</span><br><span class="line">is only available from another <span class="built_in">source</span></span><br><span class="line"></span><br><span class="line">E: Package <span class="string">&#x27;git&#x27;</span> has no installation candidate</span><br></pre></td></tr></table></figure><p>原因：没有找到对应软件包<code>git</code></p><ul><li><p>解决一：<code>apt update</code>【可能apt未更新】</p></li><li><p>解决二：更改一下<code>Install</code>的安装源地址【详见：修改最佳<code>apt</code>下载服务器】</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">System settings -&gt; software&amp;Updates</span><br><span class="line">勾选“Source <span class="keyword">code</span>”</span><br><span class="line">在Download <span class="keyword">from</span>选择other -&gt; reload</span><br></pre></td></tr></table></figure></li></ul><h4 id="0-4-进入conda环境出错"><a href="#0-4-进入conda环境出错" class="headerlink" title="0.4 进入conda环境出错"></a>0.4 进入conda环境出错</h4><p><img src="./5-1.png"></p><p><strong>解决：</strong>在<code>cmd</code>中使用 <code>conda.bat activate envname</code></p><br><h3 id="2、操作"><a href="#2、操作" class="headerlink" title="2、操作"></a>2、操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##对于新的Ubuntu环境</span></span><br><span class="line"><span class="comment">##更新</span></span><br><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="comment">##安装编译器gcc等</span></span><br><span class="line"><span class="built_in">sudo</span> apt install build-essential</span><br><span class="line"><span class="comment">##安装python</span></span><br><span class="line"><span class="built_in">sudo</span> apt install python3.8</span><br><span class="line"><span class="comment">##安装miniConda 1、下载 2、使用bash 安装刚下载的文件,装入根目录下</span></span><br><span class="line">wget xxxxxxxxx[miniconda的下载地址]</span><br><span class="line">bash Miniconda3-Latest-Linux-x86_64.sh</span><br><span class="line"><span class="comment">##进入conda的base环境</span></span><br><span class="line">bash</span><br><span class="line">-----------------------------------------------------快照1</span><br><span class="line"></span><br><span class="line"><span class="comment">##【创建独立环境】</span></span><br><span class="line">conda create --name python38 python=3.8</span><br><span class="line"><span class="comment">#激活</span></span><br><span class="line">conda activate python36</span><br><span class="line"><span class="comment">#退出</span></span><br><span class="line">conda deactivate</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------</span><br><span class="line"><span class="comment">##安装jupyter、d2l、torch、torchvision</span></span><br><span class="line">pip install jupyter d2l torch torchvision</span><br><span class="line"><span class="comment">##下载记事本文件【动手学深度学习 → jupyter记事本】</span></span><br><span class="line">wget https://zh-v2.d2l.ai/d2l-zh.zip</span><br><span class="line"><span class="comment">##安装解压软件zip</span></span><br><span class="line"><span class="built_in">sudo</span> apt install zip</span><br><span class="line"><span class="comment">##解压 → 三个版本【mxnet | pytorch√ | tensorflow】</span></span><br><span class="line"><span class="built_in">ls</span></span><br><span class="line">unzip d2l-zh.zip</span><br><span class="line"></span><br><span class="line"><span class="comment">##pytorch版本</span></span><br><span class="line"><span class="built_in">cd</span> pytorch/</span><br><span class="line"><span class="built_in">ls</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment">##安装git</span></span><br><span class="line"><span class="built_in">sudo</span> apt install git</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/d2l-ai/d2l-zh-pytorch-slides</span><br><span class="line"><span class="comment">#运行</span></span><br><span class="line">jupyter notebook</span><br><span class="line">_________________对于远程服务器，需要将远端端口映射到本地8888端口 </span><br><span class="line">[本地]:ssh -L8888:localhsot:8888 ubuntu@[远程ip]</span><br><span class="line">--------------------------------------------------------</span><br><span class="line"></span><br><span class="line"><span class="comment">##幻灯片插件【用于jupyter】【或使用课程中COLAB】</span></span><br><span class="line">【如https://zh.d2l.ai/chapter_linear-networks/linear-regression.html】</span><br><span class="line">pip install rise</span><br><span class="line"></span><br><span class="line">-------------------------------------------end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#使用conda/miniconda环境</span></span><br><span class="line"><span class="comment">#添加-y参数跳过确认步骤</span></span><br><span class="line"><span class="comment">#[SOFTWARE]=[xx.xx]，安装指定版本的软件，不加=[xx,xx]默认安装最新版本的软件</span></span><br><span class="line">conda <span class="built_in">env</span> remove d2l-zh</span><br><span class="line">conda create -n -y d2l-zh python=3.8 pip </span><br><span class="line">conda create -n d2l-zh -y python=3.8 pip</span><br><span class="line">conda activate d2l-zh</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装需要的包</span></span><br><span class="line">pip install -y jupyter d2l torch torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载代码并执行</span></span><br><span class="line">wget https://zh-v2.d2l.ai/d2l-zh.zip</span><br><span class="line">unzip d2l-zh.zip</span><br><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure><h4 id="1-1-关于build-essential"><a href="#1-1-关于build-essential" class="headerlink" title="1.1 关于build-essential"></a>1.1 关于<code>build-essential</code></h4><p>**<code>build-essential</code> **：编译程序必须的软件包</p><p>查看该软件包的依赖关系，可以看到以下内容：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ apt-cache depends build-essential</span><br><span class="line">build-essential</span><br><span class="line"> |Depend<span class="variable">s:libc6</span>-dev</span><br><span class="line">  Depends:<span class="symbol">&lt;libc-dev&gt;</span></span><br><span class="line">    libc6-dev</span><br><span class="line">  Depend<span class="variable">s:gcc</span></span><br><span class="line">  Depend<span class="variable">s:g</span>++</span><br><span class="line">  Depend<span class="variable">s:make</span></span><br><span class="line">    <span class="keyword">make</span>-guile</span><br><span class="line">  Depend<span class="variable">s:dpkg</span>-dev</span><br></pre></td></tr></table></figure><p>安装了该软件包，编译<code>c/c++</code>所需要的软件包也都会被安装。因此如果想在<code>Ubuntu</code>中编译c/c++程序，只需要安装该软件包就可以了</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#安装方法如下：</span></span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash"><span class="built_in">sudo</span> apt-get install build-essential</span></span><br><span class="line"><span class="meta prompt_">$</span><span class="language-bash"><span class="built_in">sudo</span> apt install build-essential</span></span><br></pre></td></tr></table></figure><blockquote><p>推荐：<code>apt</code> 取代<code>apt-get</code> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/528353275">原因</a></p><ul><li>apt 命令是对之前的<code>apt-get | apt-cache</code> 等的封装，提供更加统一，更加适合<strong>终端用户</strong>使用的接口</li><li>apt 具有更<strong>精减</strong>但足够的命令选项，而且参数选项的组织方式更为有效</li><li>apt是为交互使用而设计的。最好在shell脚本中使用<code>apt-get</code>和<code>apt-cache</code>，因为它们在不同版本之间向后兼容，并且有更多选项和功能。</li></ul><p>对于基本命令，两个工具的语法是相同的</p></blockquote><br><h2 id="六、数据操作"><a href="#六、数据操作" class="headerlink" title="六、数据操作"></a>六、数据操作</h2><h3 id="1、课程内容"><a href="#1、课程内容" class="headerlink" title="1、课程内容"></a>1、课程内容</h3><h4 id="1-1-N维数组"><a href="#1-1-N维数组" class="headerlink" title="1.1 N维数组"></a>1.1 N维数组</h4><p>N维数组是机器学习和神经网络的主要数据结构</p><p><img src="./6-1.png"></p><p><img src="./6-2.png"></p><h4 id="1-2-创建数组"><a href="#1-2-创建数组" class="headerlink" title="1.2 创建数组"></a>1.2 创建数组</h4><p>三要素：</p><ul><li>形状：如3×4矩阵</li><li>每个元素的数据类型：如32位浮点数</li><li>每个元素的值：如全零或随机数</li></ul><p><img src="./6-3.png"></p><h4 id="1-3-访问元素"><a href="#1-3-访问元素" class="headerlink" title="1.3 访问元素"></a>1.3 访问元素</h4><p><img src="./6-4.png"></p><blockquote><p>注意：<code>X[0:2]</code>：区间为左闭右开<code>【0,2)</code></p></blockquote><h4 id="1-5-数据操作"><a href="#1-5-数据操作" class="headerlink" title="1.5 数据操作"></a>1.5 数据操作</h4><blockquote><p><code>Torch</code>：【译：火把；手电筒】【<code>PyTorch</code>是一个开源的<code>Python</code>机器学习库，基于<code>Torch</code>，底层由<code>C++</code>实现，应用于人工智能领域，如自然语言处理。 它最初由<code>Facebook</code>的人工智能研究团队开发，并且被用于Uber的概率编程软件Pyro】虽然它被称为<code>PyTorch</code>，但我们应该导入 <code>torch</code> 而不是 <code>pytorch</code>。</p><p><strong>数组与张量：</strong></p><ul><li>张量【英：<code>tensor</code>】：表示一个数值组成的数组，这个数组可能有很多维度【数学维度的定义】【<code>tensorflow</code> 和 <code>pytorch</code>重载了 张量的概念】</li><li>数组【<code>ndarray</code> | <code>N-dimensional array</code>】：属于【计算机语言维度的定义】</li></ul><p><span style="background:#ff0">▲</span><code>Torch</code>中的张量，<strong>一定</strong>是一个<strong>行向量</strong>【1×N】；而<strong>列向量</strong>是一个<code>N×1</code>的矩阵</p></blockquote><p><strong>生成一个tensor数组：0-11；展示属性，并改变形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">In:  x = torch.arange(<span class="number">12</span>)</span><br><span class="line">Out: tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>])</span><br><span class="line">  </span><br><span class="line"><span class="comment">#shape属性：张量的形状  numel函数：返回张量中元素的个数</span></span><br><span class="line">In:  x.shape, x.numel()</span><br><span class="line">Out: torch.Size([<span class="number">12</span>]), <span class="number">12</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#reshape(): 改变一个张量的形状而不改变元素数量和元素值</span></span><br><span class="line">In:  X = x.reshape(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">     X</span><br><span class="line">Out: tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]])</span><br></pre></td></tr></table></figure><p><strong>生成特殊<code>tensor</code>数组并赋值</strong>：其数组元素使用全0、全1、其他常量或者从特定分布中随机采样的数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#两个3行四列的数组#######################</span></span><br><span class="line">In:  torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))  </span><br><span class="line">     torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">     torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">Out: tensor([[[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">         [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]]])</span><br><span class="line">    </span><br><span class="line">     tensor([[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">         [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]])</span><br><span class="line">      </span><br><span class="line">     tensor([[ <span class="number">0.2104</span>,  <span class="number">1.4439</span>, -<span class="number">1.3455</span>, -<span class="number">0.8273</span>],</span><br><span class="line">        [ <span class="number">0.8009</span>,  <span class="number">0.3585</span>, -<span class="number">0.2690</span>,  <span class="number">1.6183</span>],</span><br><span class="line">        [-<span class="number">0.4611</span>,  <span class="number">1.5744</span>, -<span class="number">0.4882</span>, -<span class="number">0.5317</span>]])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#赋值【通过包含数值的Python列表或嵌套列表】</span></span><br><span class="line">In:  torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]]) <span class="comment">#list嵌套list</span></span><br><span class="line">Out: tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><p><strong>元素运算</strong>：常见的标准算术运算符（+、-、<code>*</code>、<code>/</code> 和 <code>**</code>）都可以被升级为按元素运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In:  x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">     y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">     x + y, x - y, x * y, x / y, x**y</span><br><span class="line">Out: (tensor([ <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>, <span class="number">10.</span>]),</span><br><span class="line">      tensor([-<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>]),</span><br><span class="line">      tensor([ <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">16.</span>]),</span><br><span class="line">      tensor([<span class="number">0.5000</span>, <span class="number">1.0000</span>, <span class="number">2.0000</span>, <span class="number">4.0000</span>]),</span><br><span class="line">      tensor([ <span class="number">1.</span>,  <span class="number">4.</span>, <span class="number">16.</span>, <span class="number">64.</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#其他元素运算：按元素方式应用更多的计算</span></span><br><span class="line">In:  torch.exp(x)</span><br><span class="line">Out: tensor([<span class="number">2.7183e+00</span>, <span class="number">7.3891e+00</span>, <span class="number">5.4598e+01</span>, <span class="number">2.9810e+03</span>])</span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>多张量操作</strong>：连结【concatenate】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成一个0-11【长为12】的向量【指定为浮点数】，并reshape到3×4</span></span><br><span class="line">In:  X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>, <span class="number">4</span>)) </span><br><span class="line">     Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]]) <span class="comment">#3×4矩阵</span></span><br><span class="line">     torch.cat((X, Y), dim=<span class="number">0</span>) <span class="comment">#连结XY,在第0维合并【行】！！！注意！！！</span></span><br><span class="line">     torch.cat((X, Y), dim=<span class="number">1</span>) <span class="comment">#连结XY,在第1维合并【列】！！！注意！！！</span></span><br><span class="line"></span><br><span class="line">Out: (tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [ <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]),</span><br><span class="line">      tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>,  <span class="number">4.</span>,  <span class="number">3.</span>,  <span class="number">2.</span>,  <span class="number">1.</span>]]))</span><br></pre></td></tr></table></figure><p><strong>通过逻辑运算符构建二维张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In:  X == Y</span><br><span class="line">Out: tensor([[<span class="literal">False</span>,  <span class="literal">True</span>, <span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure><p><strong>对张量中所有元素求和，产生只有一个元素的张量</strong>：<code>.sum()</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In:  X.<span class="built_in">sum</span>()</span><br><span class="line">Out: tensor(<span class="number">66.</span>)</span><br></pre></td></tr></table></figure><p><strong>【易错】广播机制</strong>：即使形状不同，仍然可以执行元素操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In:  a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">     b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">     a, b</span><br><span class="line">     a + b <span class="comment"># a扩展为3×2，a扩展为3×2【均以最大为准 】</span></span><br><span class="line">Out: (tensor([[<span class="number">0</span>],</span><br><span class="line">         [<span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>]]),</span><br><span class="line">      tensor([[<span class="number">0</span>, <span class="number">1</span>]]))</span><br><span class="line">    </span><br><span class="line">    tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><p><strong>元素访问</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#前者为取最后一行/元素 后者为取第2行至第3行</span></span><br><span class="line">In:  X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line">Out: (tensor([ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]),</span><br><span class="line">       tensor([[ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]]))</span><br></pre></td></tr></table></figure><p><strong>赋值【写元素】</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过指定索引的方式【行列下标，均从0开始】</span></span><br><span class="line">In:  X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">     X</span><br><span class="line">Out: tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">             [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">9.</span>,  <span class="number">7.</span>],</span><br><span class="line">             [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#区域赋值：为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</span></span><br><span class="line">In:  X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span>  <span class="comment">#赋值区间：0行~1行</span></span><br><span class="line">     X</span><br><span class="line">Out: tensor([[<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">             [<span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>, <span class="number">12.</span>],</span><br><span class="line">             [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]])</span><br></pre></td></tr></table></figure><p><strong>内存地址问题</strong>【Python内核会自动释放不需要内存空间】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#某些操作会涉及新的内存分配问题【python中的id(),类似于C++的指针，是唯一标识号】</span></span><br><span class="line">In:  before = <span class="built_in">id</span>(Y) <span class="comment">#取Y的地址</span></span><br><span class="line">     Y = Y + X</span><br><span class="line">     <span class="built_in">id</span>(Y) == before  <span class="comment">#检测Y是否被析构</span></span><br><span class="line">Out: <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#执行原地操作</span></span><br><span class="line">In:  Z = torch.zeros_like(Y)  <span class="comment">#Z的数据的类型和形状，均与Y相似【torch.zeros_like()】</span></span><br><span class="line">     <span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">     Z[:] = X + Y</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Out: <span class="built_in">id</span>(Z): <span class="number">140452400950336</span></span><br><span class="line">     <span class="built_in">id</span>(Z): <span class="number">140452400950336</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#如果在后续计算中没有重复使用X,可以使用X[:] = X+Y 或 X += Y来减少操作的内存开销</span></span><br><span class="line"><span class="comment">#对于较大数据集，需要注意！</span></span><br><span class="line">In:  before = <span class="built_in">id</span>(X)</span><br><span class="line">     X += Y</span><br><span class="line">     <span class="built_in">id</span>(X) == before</span><br><span class="line">Out: <span class="literal">True</span></span><br></pre></td></tr></table></figure><p><strong>torch -&gt; Numpy张量</strong>【Python中最常见最基础】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In:  A = X.numpy()</span><br><span class="line">     B = torch.tensor(A)</span><br><span class="line">     <span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br><span class="line">Out: (numpy.ndarray, torch.Tensor)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#将大小为1的张量转换为 Python 标量</span></span><br><span class="line">In:  a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">     a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br><span class="line">Out: (tensor([<span class="number">3.5000</span>]), <span class="number">3.5</span>, <span class="number">3.5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="2、关于Torch"><a href="#2、关于Torch" class="headerlink" title="2、关于Torch"></a>2、关于Torch</h3><blockquote><p>【详见 Tool-Pytorch】</p><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">参考文档</a> | 以下为已用的函数</p></blockquote><br><h3 id="3、关于Numpy"><a href="#3、关于Numpy" class="headerlink" title="3、关于Numpy"></a>3、关于Numpy</h3><blockquote><p>【详见 <code>Tool-Numpy</code>】</p></blockquote><h3 id="4、关于pandas"><a href="#4、关于pandas" class="headerlink" title="4、关于pandas"></a>4、关于pandas</h3><h3 id="5、关于作图"><a href="#5、关于作图" class="headerlink" title="5、关于作图"></a>5、关于作图</h3><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l<span class="selector-class">.set_figsize</span>()</span><br><span class="line">     d2l<span class="selector-class">.plt</span><span class="selector-class">.scatter</span>(features[:, <span class="number">1</span>].detach()<span class="selector-class">.numpy</span>(),</span><br><span class="line">                labels<span class="selector-class">.detach</span>()<span class="selector-class">.numpy</span>(), <span class="number">1</span>);</span><br></pre></td></tr></table></figure><br><h2 id="七、数据预处理"><a href="#七、数据预处理" class="headerlink" title="七、数据预处理"></a>七、数据预处理</h2><blockquote><p><strong>逗号分隔值</strong>（Comma-Separated Values，<strong>CSV</strong>，有时也称为<strong>字符分隔值</strong>，因为分隔字符也可以不是逗号），其文件以纯文本形式存储表格数据（数字和文本）。纯文本意味着该文件是一个字符序列，不含必须像二进制数字那样被解读的数据。</p><ul><li>CSV文件由任意数目的记录组成，记录间以某种换行符分隔；</li><li>每条记录由字段组成，字段间的分隔符是其它字符或字符串，最常见的是逗号或<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%88%B6%E8%A1%A8%E7%AC%A6/7337607?fromModule=lemma_inlink">制表符</a>；</li><li>通常，所有记录都有完全相同的字段序列。通常都是<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%BA%AF%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6/4865229?fromModule=lemma_inlink">纯文本文件</a>。建议使用WORDPAD或是记事本来开启，再则先另存新档后用EXCEL开启，也是方法之一。</li></ul></blockquote><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>创建一个人工数据集，并存储在csv（逗号分隔值）文件</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>) <span class="comment">#</span></span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>从创建的csv文件中加载原始数据集</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In: <span class="comment"># 如果没有安装pandas，只需要取消以下注释：</span></span><br><span class="line">    <span class="comment"># !pip install pandas</span></span><br><span class="line">    <span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">    data = pd.read_csv(data_file)</span><br><span class="line">    <span class="built_in">print</span>(data)  <span class="comment">#也可以直接 data 【这样会默认Html打印】</span></span><br><span class="line">Out: </span><br><span class="line">    NumRooms Alley   Price</span><br><span class="line"><span class="number">0</span>       NaN  Pave  <span class="number">127500</span></span><br><span class="line"><span class="number">1</span>       <span class="number">2.0</span>   NaN  <span class="number">106000</span></span><br><span class="line"><span class="number">2</span>       <span class="number">4.0</span>   NaN  <span class="number">178100</span></span><br><span class="line"><span class="number">3</span>       NaN   NaN  <span class="number">140000</span></span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>为了处理缺失的数据，典型的方法包括插值和删除， 这里，我们将考虑插值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">In: <span class="comment"># 取data的第0和第1列  取data的第2列</span></span><br><span class="line">    inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 插值：对于数值域，填充已有数据的均值</span></span><br><span class="line">    inputs = inputs.fillna(inputs.mean())</span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line">Out:     </span><br><span class="line">       NumRooms Alley</span><br><span class="line">    <span class="number">0</span>       <span class="number">3.0</span>  Pave</span><br><span class="line">    <span class="number">1</span>       <span class="number">2.0</span>   NaN</span><br><span class="line">    <span class="number">2</span>       <span class="number">4.0</span>   NaN</span><br><span class="line">    <span class="number">3</span>       <span class="number">3.0</span>   NaN</span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>对于inputs中的类别值或离散值，我们将“NaN”视为一个类别</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In: <span class="comment"># 取所有NA列中的类别，将其视为特征</span></span><br><span class="line">    inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(inputs)</span><br><span class="line">Out:</span><br><span class="line">       NumRooms  Alley_Pave  Alley_nan</span><br><span class="line">    <span class="number">0</span>       <span class="number">3.0</span>           <span class="number">1</span>          <span class="number">0</span></span><br><span class="line">    <span class="number">1</span>       <span class="number">2.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line">    <span class="number">2</span>       <span class="number">4.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br><span class="line">    <span class="number">3</span>       <span class="number">3.0</span>           <span class="number">0</span>          <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><p>上面的操作，会导致矩阵稀疏化【稀疏矩阵对机器学习影响不大】</p></blockquote><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>现在inputs和outputs中的所有条目都是数值类型，它们可以转换为张量格式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In: <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">    X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)</span><br><span class="line">    X, y</span><br><span class="line">Out:</span><br><span class="line">    (tensor([[<span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">             [<span class="number">2.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">             [<span class="number">4.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">             [<span class="number">3.</span>, <span class="number">0.</span>, <span class="number">1.</span>]], dtype=torch.float64),</span><br><span class="line">     tensor([<span class="number">127500</span>, <span class="number">106000</span>, <span class="number">178100</span>, <span class="number">140000</span>]))</span><br></pre></td></tr></table></figure><blockquote><p><code>python</code>默认为浮点64位，但会导致计算慢，故对于深度学习来说一般使用<span style="background:#f0a1a8">float32</span></p></blockquote><h2 id="八、预备知识"><a href="#八、预备知识" class="headerlink" title="八、预备知识"></a>八、预备知识</h2><h3 id="1、线性代数"><a href="#1、线性代数" class="headerlink" title="1、线性代数"></a>1、线性代数</h3><h4 id="1-1-相关内容回顾"><a href="#1-1-相关内容回顾" class="headerlink" title="1.1 相关内容回顾"></a>1.1 相关内容回顾</h4><p><img src="./8-1.png"></p><p><img src="./8-2.png"></p><blockquote><p>关于矩阵范数，可参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/507328276">原文</a></p><p>在很多实际问题种，常需要对同一线性空间种的向量引入作为它们”大小“的一种度量，进而比较两个向量之间的”接近程度“。引入的这种体现其”大小“的量，就是<strong>范数</strong>。</p><p>常见有：<code>1-范数</code>、<code>2-范数</code>、<code>∞-范数</code>、<code>p-范数</code>、<code>F范数</code></p></blockquote><p><img src="./8-3.png"></p><h4 id="1-2-线性代数实现"><a href="#1-2-线性代数实现" class="headerlink" title="1.2 线性代数实现"></a>1.2 线性代数实现</h4><h5 id="1-2-1-标量由只有一个元素的张量表示"><a href="#1-2-1-标量由只有一个元素的张量表示" class="headerlink" title="1.2.1 标量由只有一个元素的张量表示"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.1 标量由只有一个元素的张量表示</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In: <span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">    x = torch.tensor([<span class="number">3.0</span>])</span><br><span class="line">    y = torch.tensor([<span class="number">2.0</span>])</span><br><span class="line"></span><br><span class="line">    x + y, x * y, x / y, x**y</span><br><span class="line">Out:     </span><br><span class="line">    (tensor([<span class="number">5.</span>]), tensor([<span class="number">6.</span>]), tensor([<span class="number">1.5000</span>]), tensor([<span class="number">9.</span>]))</span><br></pre></td></tr></table></figure><h5 id="1-2-2-向量"><a href="#1-2-2-向量" class="headerlink" title="1.2.2 向量"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.2 向量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可以将向量视为标量值组成的列表</span></span><br><span class="line">In: x = torch.arange(<span class="number">4</span>)</span><br><span class="line">    x</span><br><span class="line">Out:     </span><br><span class="line">    tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过张量的索引来访问任一元素[从0开始]</span></span><br><span class="line">In: x[<span class="number">3</span>]</span><br><span class="line">Out:     </span><br><span class="line">   tensor(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过张量的索引来访问任一元素[从0开始]</span></span><br><span class="line">In:</span><br><span class="line">    x[<span class="number">3</span>]</span><br><span class="line">Out:     </span><br><span class="line">   tensor(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#访问张量的长度</span></span><br><span class="line">In: <span class="built_in">len</span>(x)</span><br><span class="line">Out:<span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#只有一个轴的张量，形状只有一个元素</span></span><br><span class="line">In: x.shape</span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">4</span>])</span><br></pre></td></tr></table></figure><h5 id="1-2-3-矩阵"><a href="#1-2-3-矩阵" class="headerlink" title="1.2.3 矩阵"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.3 矩阵</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过指定两个分量m和 n来创建一个形状为m×n的矩阵</span></span><br><span class="line">In: A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">    A</span><br><span class="line">Out:     </span><br><span class="line">    tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">        [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#矩阵的转置</span></span><br><span class="line">In: A.T, A.T.shape</span><br><span class="line">Out:</span><br><span class="line">    (tensor([[ <span class="number">0</span>,  <span class="number">4</span>,  <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">5</span>,  <span class="number">9</span>, <span class="number">13</span>, <span class="number">17</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">14</span>, <span class="number">18</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">7</span>, <span class="number">11</span>, <span class="number">15</span>, <span class="number">19</span>]]),</span><br><span class="line">     torch.Size([<span class="number">4</span>, <span class="number">5</span>]))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#对称矩阵（symmetric matrix）A等于其转置：A=A⊤</span></span><br><span class="line">In: B = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">    B</span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">In: B == B.T</span><br><span class="line">Out: </span><br><span class="line">    tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</span></span><br><span class="line">In:  X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">     X</span><br><span class="line">Out: tensor([[[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">         [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">         [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>],</span><br><span class="line">         [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>]]])</span><br></pre></td></tr></table></figure><h5 id="1-2-4-矩阵运算"><a href="#1-2-4-矩阵运算" class="headerlink" title="1.2.4 矩阵运算"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.4 矩阵运算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量</span></span><br><span class="line">In:  A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">     B = A.clone()   <span class="comment">#通过新的内存分配，创建一个副本</span></span><br><span class="line">     A, A + B</span><br><span class="line">Out: (tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>]]),</span><br><span class="line">     tensor([[ <span class="number">0.</span>,  <span class="number">2.</span>,  <span class="number">4.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>, <span class="number">10.</span>, <span class="number">12.</span>, <span class="number">14.</span>],</span><br><span class="line">         [<span class="number">16.</span>, <span class="number">18.</span>, <span class="number">20.</span>, <span class="number">22.</span>],</span><br><span class="line">         [<span class="number">24.</span>, <span class="number">26.</span>, <span class="number">28.</span>, <span class="number">30.</span>],</span><br><span class="line">         [<span class="number">32.</span>, <span class="number">34.</span>, <span class="number">36.</span>, <span class="number">38.</span>]]))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#两个矩阵的按元素乘法称为哈达玛积（Hadamard product）（数学符号⊙）</span></span><br><span class="line">In:  A * B</span><br><span class="line">Out: tensor([[  <span class="number">0.</span>,   <span class="number">1.</span>,   <span class="number">4.</span>,   <span class="number">9.</span>],</span><br><span class="line">        [ <span class="number">16.</span>,  <span class="number">25.</span>,  <span class="number">36.</span>,  <span class="number">49.</span>],</span><br><span class="line">        [ <span class="number">64.</span>,  <span class="number">81.</span>, <span class="number">100.</span>, <span class="number">121.</span>],</span><br><span class="line">        [<span class="number">144.</span>, <span class="number">169.</span>, <span class="number">196.</span>, <span class="number">225.</span>],</span><br><span class="line">        [<span class="number">256.</span>, <span class="number">289.</span>, <span class="number">324.</span>, <span class="number">361.</span>]])</span><br><span class="line">    </span><br><span class="line">In:  a = <span class="number">2</span></span><br><span class="line">     X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">     a + X, (a * X).shape</span><br><span class="line">Out:     </span><br><span class="line">     (tensor([[[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">          [ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>]],</span><br><span class="line"> </span><br><span class="line">         [[<span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>],</span><br><span class="line">          [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>]]]),</span><br><span class="line">      torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]))</span><br></pre></td></tr></table></figure><h5 id="1-2-5-求和"><a href="#1-2-5-求和" class="headerlink" title="1.2.5 求和"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.5 求和</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算其元素的和</span></span><br><span class="line">In:  x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">     x, x.<span class="built_in">sum</span>()</span><br><span class="line">Out: (tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]), tensor(<span class="number">6.</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#表示任意形状张量的元素和【.sum的结果永远是一个标量】</span></span><br><span class="line">In:  A.shape, A.<span class="built_in">sum</span>()</span><br><span class="line">Out: (torch.Size([<span class="number">5</span>, <span class="number">4</span>]), tensor(<span class="number">190.</span>))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#指定张量沿哪一个轴来通过求和降低维度</span></span><br><span class="line"><span class="comment">#axis默认为简单求和</span></span><br><span class="line"><span class="comment">#axis=0: 可以理解为对三维数据中(5, 4)的5这一维度进行求和，剩下4列【结果为向量】</span></span><br><span class="line"><span class="comment">#axis=1: 可以理解为对三维数据中(5, 4)的4这一维度进行求和，剩下5行【结果为向量】</span></span><br><span class="line">In:  A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">Out: tensor(<span class="number">190.</span>)</span><br><span class="line">    </span><br><span class="line">In:  A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">     A_sum_axis0, A_sum_axis0.shape</span><br><span class="line">Out: (tensor([<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]), torch.Size([<span class="number">4</span>]))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">In:     A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">     A_sum_axis1, A_sum_axis1.shape</span><br><span class="line">Out: (tensor([ <span class="number">6.</span>, <span class="number">22.</span>, <span class="number">38.</span>, <span class="number">54.</span>, <span class="number">70.</span>]), torch.Size([<span class="number">5</span>]))</span><br><span class="line">    </span><br><span class="line"><span class="comment">#某个轴计算A元素的累积总和</span></span><br><span class="line">In:  A.cumsum(axis=<span class="number">0</span>)</span><br><span class="line">Out: tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">             [ <span class="number">4.</span>,  <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">10.</span>],</span><br><span class="line">             [<span class="number">12.</span>, <span class="number">15.</span>, <span class="number">18.</span>, <span class="number">21.</span>],</span><br><span class="line">             [<span class="number">24.</span>, <span class="number">28.</span>, <span class="number">32.</span>, <span class="number">36.</span>],</span><br><span class="line">             [<span class="number">40.</span>, <span class="number">45.</span>, <span class="number">50.</span>, <span class="number">55.</span>]])</span><br></pre></td></tr></table></figure><h5 id="1-2-6-求均值"><a href="#1-2-6-求均值" class="headerlink" title="1.2.6 求均值"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.6 求均值</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一个与求和相关的量是平均值（mean或average）</span></span><br><span class="line">In:  A.mean(), A.<span class="built_in">sum</span>() / A.numel()</span><br><span class="line">Out: (tensor(<span class="number">9.5000</span>), tensor(<span class="number">9.5000</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#按某一维度求均值</span></span><br><span class="line">In:  A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]</span><br><span class="line">Out: (tensor([ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]), tensor([ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#keepdim属性！！！如果A对其axis=1求和，维度为1的维度会丢失</span></span><br><span class="line"><span class="comment">#该属性有助于使用广播机制进行操作</span></span><br><span class="line"><span class="comment">#计算总和或均值时保持轴数不变</span></span><br><span class="line">In:  sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">     sum_A</span><br><span class="line">Out: tensor([[ <span class="number">6.</span>],</span><br><span class="line">             [<span class="number">22.</span>],</span><br><span class="line">             [<span class="number">38.</span>],</span><br><span class="line">             [<span class="number">54.</span>],</span><br><span class="line">             [<span class="number">70.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#通过广播将A除以sum_A 【5*4 除以 5*1】【5*1 会被补充为 5*4】</span></span><br><span class="line">In:  A / sum_A</span><br><span class="line">Out: tensor([[<span class="number">0.0000</span>, <span class="number">0.1667</span>, <span class="number">0.3333</span>, <span class="number">0.5000</span>],</span><br><span class="line">        [<span class="number">0.1818</span>, <span class="number">0.2273</span>, <span class="number">0.2727</span>, <span class="number">0.3182</span>],</span><br><span class="line">        [<span class="number">0.2105</span>, <span class="number">0.2368</span>, <span class="number">0.2632</span>, <span class="number">0.2895</span>],</span><br><span class="line">        [<span class="number">0.2222</span>, <span class="number">0.2407</span>, <span class="number">0.2593</span>, <span class="number">0.2778</span>],</span><br><span class="line">        [<span class="number">0.2286</span>, <span class="number">0.2429</span>, <span class="number">0.2571</span>, <span class="number">0.2714</span>]])</span><br></pre></td></tr></table></figure><blockquote><p>当 <code>keepdim = true</code>，被求和的维度不会被丢失，而是置为1</p></blockquote><p><img src="./8-4.png"></p><h5 id="1-2-7-点积"><a href="#1-2-7-点积" class="headerlink" title="1.2.7 点积"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.7 点积</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#两个向量的点积【形状相同】：按元素相乘，再求和【标量】【等价于x*y再sum求和】</span></span><br><span class="line">In:  y = torch.ones(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">     x, y, torch.dot(x, y)</span><br><span class="line">Out: (tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]), tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), tensor(<span class="number">6.</span>))</span><br><span class="line">   </span><br><span class="line"><span class="comment">#等价于: 可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</span></span><br><span class="line">In:  torch.<span class="built_in">sum</span>(x * y)</span><br><span class="line">Out: tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure><h5 id="1-2-8-矩阵向量积【▲】"><a href="#1-2-8-矩阵向量积【▲】" class="headerlink" title="1.2.8 矩阵向量积【▲】"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.8 矩阵向量积【▲】</h5><p>【<code>torch.mv</code> | <code>Matrix-Vector-Multiplication</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵向量积Ax是一个长度为m的列向量【注意：为列向量，长度为m】，其第i个元素是点积ai⊤x</span></span><br><span class="line"><span class="comment">#下例中 ，A为5*4，x为长度为4的向量，结果为一个长度为</span></span><br><span class="line">In:  A.shape, x.shape, torch.mv(A, x)</span><br><span class="line">Out: (torch.Size([<span class="number">5</span>, <span class="number">4</span>]), torch.Size([<span class="number">4</span>]), tensor([ <span class="number">14.</span>,  <span class="number">38.</span>,  <span class="number">62.</span>,  <span class="number">86.</span>, <span class="number">110.</span>]))</span><br></pre></td></tr></table></figure><blockquote><p><code>torch.mv(X, w0)</code>：是矩阵和向量相乘。第一个参数是矩阵，第二个参数只能是一维向量，等价于<code>X乘以w0的转置</code>【w0为横向的，长度为m的向量】</p></blockquote><h5 id="1-2-9-矩阵与矩阵乘法"><a href="#1-2-9-矩阵与矩阵乘法" class="headerlink" title="1.2.9 矩阵与矩阵乘法"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.9 矩阵与矩阵乘法</h5><p>【<code>torch.mm</code> | <code>Matrix-Multiplication</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将矩阵-矩阵乘法AB看作是简单地执行m次矩阵-向量积，并将结果拼接在一起，形成一个n×m矩阵</span></span><br><span class="line">In:  B = torch.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">     torch.mm(A, B)</span><br><span class="line">Out: tensor([[ <span class="number">6.</span>,  <span class="number">6.</span>,  <span class="number">6.</span>],</span><br><span class="line">        [<span class="number">22.</span>, <span class="number">22.</span>, <span class="number">22.</span>],</span><br><span class="line">        [<span class="number">38.</span>, <span class="number">38.</span>, <span class="number">38.</span>],</span><br><span class="line">        [<span class="number">54.</span>, <span class="number">54.</span>, <span class="number">54.</span>],</span><br><span class="line">        [<span class="number">70.</span>, <span class="number">70.</span>, <span class="number">70.</span>]])</span><br></pre></td></tr></table></figure><h5 id="1-2-10-范数"><a href="#1-2-10-范数" class="headerlink" title="1.2.10 范数"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>1.2.10 范数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#L2 范数：向量元素平方和的平方根  【向量范数】</span></span><br><span class="line"><span class="comment"># ||x||2 = 对向量每个元素的平方求和，再开根号</span></span><br><span class="line">In:  u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">     torch.norm(u)</span><br><span class="line">Out: tensor(<span class="number">5.</span>)</span><br><span class="line">   </span><br><span class="line"><span class="comment">#L1 范数：它表示为向量元素的绝对值之和  【向量范数】</span></span><br><span class="line"><span class="comment"># ||x||1 = 对向量每个元素的绝对值求和，再开根号</span></span><br><span class="line">In:  torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br><span class="line">Out: tensor(<span class="number">7.</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment">#矩阵的弗罗贝尼乌斯范数（Frobenius norm）是矩阵元素平方和的平方根  【矩阵范数】</span></span><br><span class="line"><span class="comment"># ||x||F = 对矩阵每个元素的平方求和，再开根号</span></span><br><span class="line">In:  torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br><span class="line">Out: tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure><br><h3 id="2、矩阵计算"><a href="#2、矩阵计算" class="headerlink" title="2、矩阵计算"></a>2、矩阵计算</h3><p><img src="./8-5.png"></p><p><img src="./8-6.png"></p><p><img src="./8-7.png"></p><p><img src="./8-8.png"></p><br><h3 id="3、自动求导"><a href="#3、自动求导" class="headerlink" title="3、自动求导"></a>3、自动求导</h3><h4 id="3-1-基本方法"><a href="#3-1-基本方法" class="headerlink" title="3.1 基本方法"></a>3.1 基本方法</h4><p><img src="./8-9.png"></p><p><img src="./8-10.png"></p><p><img src="./8-11.png"></p><p><img src="./8-12.png"></p><blockquote><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/notes/autograd/#requires_grad">pytorch自动求导机制文档</a></p><p><strong>从后向中排除子图</strong>：每个变量都有两个标志：<code>requires_grad</code>和<code>volatile</code>。它们都允许从梯度计算中精细地排除子图，并可以提高效率。</p><p><strong>1、<code>requires_grad</code></strong></p><p>如果有一个单一的输入操作需要梯度，它的输出也需要梯度。相反，只有所有输入都不需要梯度，输出才不需要。如果其中所有的变量都不需要梯度进行，后向计算不会在子图中执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = Variable(torch.randn(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = Variable(torch.randn(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = Variable(torch.randn(<span class="number">5</span>, <span class="number">5</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = x + y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a + z</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>这个标志特别有用，当您想要冻结部分模型时，或者您事先知道不会使用某些参数的梯度。例如，如果要对预先训练的CNN进行优化，只要切换冻结模型中的<code>requires_grad</code>标志就足够了，直到计算到最后一层才会保存中间缓冲区，其中的仿射变换将使用需要梯度的权重并且网络的输出也将需要它们。</p><p><strong>2、<code>volatile</code></strong></p><p>纯粹的inference模式下推荐使用<code>volatile</code>，当你确定你甚至不会调用<code>.backward()</code>时。它比任何其他自动求导的设置更有效——它将使用绝对最小的内存来评估模型。<code>volatile</code>也决定了<code>require_grad is False</code>。</p><p><code>volatile</code>不同于<code>require_grad</code>的传递。如果一个操作甚至只有有一个<code>volatile</code>的输入，它的输出也将是<code>volatile</code>。<code>Volatility</code>比“不需要梯度”更容易传递——只需要一个<code>volatile</code>的输入即可得到一个<code>volatile</code>的输出，相对的，需要所有的输入“不需要梯度”才能得到不需要梯度的输出。使用volatile标志，您不需要更改模型参数的任何设置来用于inference。创建一个<code>volatile</code>的输入就够了，这将保证不会保存中间状态。</p></blockquote><br><h4 id="3-2-自动求导实现"><a href="#3-2-自动求导实现" class="headerlink" title="3.2 自动求导实现"></a>3.2 自动求导实现</h4><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>例子一</strong></p><p>假设我们想对函数 <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>y</mi><mo>=</mo><mn>2</mn><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><msup><mi>x</mi><mrow><mi>T</mi></mrow></msup><mi>x</mi></math>关于列向量 <code>x</code> 求导【此时的<code>x</code>为张量，必然是一个行向量】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In:  <span class="keyword">import</span> torch</span><br><span class="line">     x = torch.arange(<span class="number">4.0</span>)</span><br><span class="line">     x</span><br><span class="line">    </span><br><span class="line">Out: tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure><p>在我们计算<code>y</code>关于<code>x</code>的梯度之前，我们需要一个地方来存储梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In:  x.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">     x.grad</span><br><span class="line"></span><br><span class="line"><span class="comment">#现在让我们计算y 【此时的y，为隐式构造计算图】</span></span><br><span class="line">In:  y = <span class="number">2</span> * torch.dot(x, x)</span><br><span class="line">     y</span><br><span class="line">Out: tensor(<span class="number">28.</span>, grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p>通过调用<strong>反向传播函数</strong>来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In:  y.backward()</span><br><span class="line">     x.grad</span><br><span class="line">Out: tensor([ <span class="number">0.</span>,  <span class="number">4.</span>,  <span class="number">8.</span>, <span class="number">12.</span>])</span><br><span class="line">    </span><br><span class="line"><span class="comment">#y关于x求导，答案应该是4*x</span></span><br><span class="line">In:  x.grad == <span class="number">4</span> * x</span><br><span class="line">Out: tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><blockquote><p>由矩阵计算的理论，<code>x</code> 与 <code>A</code> 的点积 &lt;x, A&gt; = <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mo>&lt;</mo><mi>x</mi><mo>,</mo><mi>A</mi><mo>&gt;=</mo><msup><mi>x</mi><mrow><mi>T</mi></mrow></msup><mi>A</mi><mo>，对其关于</mo><mi>x</mi><mo>求导后为</mo><msup><mi>A</mi><mrow><mi>T</mi></mrow></msup></math>；同理，<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>y</mi><mo>=&lt;</mo><mi>x</mi><mo>,</mo><mi>x</mi><mo>&gt;=</mo><msup><mi>x</mi><mrow><mi>T</mi></mrow></msup><mi>x</mi><mo>，对其关于</mo><mi>x</mi><mo>求导后为</mo><msup><mi>x</mi><mrow><mi>T</mi></mrow></msup></math></p><p><img src="./8-13.jpg"></p></blockquote><br><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>例子二</strong></p><p>现在让我们计算<code>x</code>的另一个函数【默认情况下，<code>pytorch</code>会<strong>累积梯度</strong>，我们<strong>需要清除之前的值</strong>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In:  x.grad.zero_()</span><br><span class="line">     y = x.<span class="built_in">sum</span>()</span><br><span class="line">     y.backward()</span><br><span class="line">     x.grad</span><br><span class="line">        </span><br><span class="line">Out: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure><br><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>例子三</strong></p><p>深度学习中 ，我们的目的不是计算微分矩阵，而是在批量中每个样本单独计算的偏导数之和【即<span style="background:#f0a1a8">很少计算<strong>向量关于向量的导数</strong>，而是先做和在求导，即一般都是计算<strong>标量关于向量的导数</strong></span>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对非标量调用 backward 需要传入 gredient 参数</span></span><br><span class="line"><span class="comment"># 该参数指定微分函数...</span></span><br><span class="line">In:  x.grad.zero_()</span><br><span class="line">     y = x * x</span><br><span class="line">     y.<span class="built_in">sum</span>().backward()</span><br><span class="line">    <span class="comment">#等价于 y.backward(torch.ones(len(X)))</span></span><br><span class="line">     x.grad</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求导后为2x</span></span><br><span class="line">Out: tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>])</span><br></pre></td></tr></table></figure><br><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>例子四</strong></p><p><span style="background:#f0a1a8">将某些计算移动到记录的计算图之外【Tensor.detach()】</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">In:  x.grad.zero_()</span><br><span class="line">     y = x * x</span><br><span class="line">     u = y.detach()</span><br><span class="line">     z = u * x</span><br><span class="line"></span><br><span class="line">     z.<span class="built_in">sum</span>().backward()</span><br><span class="line">     x.grad == u</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 即z的梯度为u  </span></span><br><span class="line">Out: tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">   </span><br><span class="line">In:  x.grad.zero_()</span><br><span class="line">     y.<span class="built_in">sum</span>().backward()</span><br><span class="line">     x.grad == <span class="number">2</span> * x    </span><br><span class="line">Out: tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><blockquote><p><code>u = y.detach()</code>：将<code>y</code>当作常数，而不是关于<code>x</code>的函数；故u为一个常数，值为<code>x*x</code></p><p>此方法多用于在<strong>固定参数</strong></p></blockquote><br><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span><strong>例子五</strong></p><p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度【在此例中，<code>torch</code>会把计算过程存下来】【Pytorch的优势所在，但会导致计算速度降低】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">In:  <span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">         b = a * <span class="number">2</span></span><br><span class="line">         <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">             b = b * <span class="number">2</span></span><br><span class="line">         <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">             c = b</span><br><span class="line">         <span class="keyword">else</span>:</span><br><span class="line">             c = <span class="number">100</span> * b</span><br><span class="line">         <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">     <span class="comment"># size为空即标量</span></span><br><span class="line">     a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">     d = f(a)</span><br><span class="line">     d.backward()</span><br><span class="line"></span><br><span class="line">     a.grad == d / a</span><br><span class="line"></span><br><span class="line">Out: tensor(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><blockquote><p><code>Pytorch.norm()</code>：【L2范数】<code>||x||2</code> = 对向量每个元素的平方求和，再开根号</p><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/#torchnorm">参考文档</a></p></blockquote><br><h4 id="3-3-理解前向计算与反向计算"><a href="#3-3-理解前向计算与反向计算" class="headerlink" title="3.3 理解前向计算与反向计算"></a>3.3 理解前向计算与反向计算</h4><br><h2 id="九、线性回归"><a href="#九、线性回归" class="headerlink" title="九、线性回归"></a>九、线性回归</h2><blockquote><p>线性回归，是最基础的一个模型，也是理解所有机器学习模型的基础。</p></blockquote><h3 id="1、理解线性回归"><a href="#1、理解线性回归" class="headerlink" title="1、理解线性回归"></a>1、理解线性回归</h3><p><img src="./9-1.png"></p><p><img src="./9-2.png"></p><p><img src="./9-3.png"></p><p><img src="./9-4.png"></p><p><img src="./9-5.png"></p><blockquote><p>最优解的推导过程：</p><p><img src="./9-6.jpg"></p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>线性回归是对n维输入的<strong>加权</strong>，外加<strong>偏差</strong></li><li>使用<strong>平方损失</strong>来衡量<strong>预测值</strong>和<strong>真实值</strong>的差异</li><li>线性回归有<strong>显示解</strong>【仅有凸函数可求最优解】</li><li>线性回归可以看做是<strong>单层神经网络</strong></li></ul><br><h3 id="2、基础优化算法"><a href="#2、基础优化算法" class="headerlink" title="2、基础优化算法"></a>2、基础优化算法</h3><blockquote><p><code>gd</code>【gradient descent】：梯度下降</p><p><code>sgd</code>【Stochastic gradient descent】：随机梯度下降【随机，是<strong>随机采样</strong>的意思】</p></blockquote><p><img src="./9-7.png"></p><p><img src="./9-8.png"></p><br><h3 id="3、线性回归的从零实现"><a href="#3、线性回归的从零实现" class="headerlink" title="3、线性回归的从零实现"></a>3、线性回归的从零实现</h3><blockquote><p>从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><blockquote><p><code>random</code>包用于随机生成</p></blockquote><h4 id="3-1-构造一个人造数据集"><a href="#3-1-构造一个人造数据集" class="headerlink" title="3.1 构造一个人造数据集"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>3.1 构造一个人造数据集</h4><p>根据带有噪声的线性模型构造一个人造数据集。 我们使用线性模型参数<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>w</mi><mo>=</mo><mo stretchy="false">[</mo><mn>2</mn><mo>,</mo><mo>−</mo><mn>3.4</mn><msup><mo stretchy="false">]</mo><mrow><mi>T</mi></mrow></msup><mo>、</mo><mi>b</mi><mo>=</mo><mn>4.2</mn><mo>和噪声项</mo><mi>ϵ</mi></math>生成数据集及其标签：<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>y</mi><mo>=</mo><mi>X</mi><mi>w</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>ϵ</mi></math></p><p>【随机数<code>X</code>，经一系列固定操作，再加上一个随机噪声<code>ϵ</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">synthetic_data</span>(<span class="params">w, b, num_examples</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成 y = Xw + b + 噪声。&quot;&quot;&quot;</span></span><br><span class="line">    X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, <span class="built_in">len</span>(w)))</span><br><span class="line">    y = torch.matmul(X, w) + b</span><br><span class="line">    y += torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, y.shape)</span><br><span class="line">    <span class="keyword">return</span> X, y.reshape((-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features, labels = synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><blockquote><p>上例中，<code>torch.normal(均值，标准差，（样本数量 ，样本的长度为w的长度）)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In:  w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">     b = <span class="number">4.2</span></span><br><span class="line">     X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">4</span>, <span class="number">2</span>))</span><br><span class="line">     y = torch.matmul(X, w) + b</span><br><span class="line">     X,y</span><br><span class="line">Out: (tensor([[-<span class="number">0.2436</span>, -<span class="number">0.2702</span>],</span><br><span class="line">                 [ <span class="number">1.4505</span>,  <span class="number">0.3857</span>],</span><br><span class="line">              [ <span class="number">0.4671</span>,  <span class="number">0.8806</span>],</span><br><span class="line">            	  [ <span class="number">0.4620</span>, -<span class="number">0.2006</span>]]),</span><br><span class="line">      tensor([<span class="number">4.6315</span>, <span class="number">5.7897</span>, <span class="number">2.1403</span>, <span class="number">5.8060</span>]))</span><br></pre></td></tr></table></figure><ul><li><p><strong>X的两个维度</strong>，可以理解为每个样本都均有两个特征，每个特征对应w的两个维度的权重</p></li><li><p><code>y += torch.normal(0, 0.01, y.shape)</code>：在<code>y</code>上加入随机噪音【均值为<code>0</code>，标准差为<code>0.01</code>，形状与<code>y</code>相同】</p></li><li><p>函数的返回值：将<code>y</code>转为列向量返回</p></li></ul></blockquote><p><code>features</code> 中的每一行都包含一个二维数据样本，<code>labels</code> 中的每一行都包含一维标签值（一个标量）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">In:  <span class="built_in">print</span>(<span class="string">&#x27;features:&#x27;</span>, features[<span class="number">0</span>], <span class="string">&#x27;\nlabel:&#x27;</span>, labels[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">Out: features: tensor([-<span class="number">0.6612</span>, -<span class="number">1.8215</span>]) </span><br><span class="line">     label: tensor([<span class="number">9.0842</span>])</span><br><span class="line">        </span><br><span class="line">In:  d2l.set_figsize()</span><br><span class="line">     d2l.plt.scatter(features[:, <span class="number">1</span>].detach().numpy(),</span><br><span class="line">                labels.detach().numpy(), <span class="number">1</span>);</span><br><span class="line">     <span class="comment"># 一些pytorch的版本中，需要将features剥离(detach)出来才能转换为numpy</span></span><br><span class="line">     <span class="comment"># 取features的所有行以及第一列(一个特征)</span></span><br></pre></td></tr></table></figure><p><img src="./9-9.png"></p><h4 id="3-2-处理小批量"><a href="#3-2-处理小批量" class="headerlink" title="3.2 处理小批量"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>3.2 处理小批量</h4><p>定义一个<code>data_iter</code> 函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为<code>batch_size</code>的小批量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)     <span class="comment">#1000个样本</span></span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))     <span class="comment">#生成1000个样本的索引号【0~999】</span></span><br><span class="line">    random.shuffle(indices)     <span class="comment">#打乱【以便于随机访问样本数据】</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):  <span class="comment">#i = 0,10,20...</span></span><br><span class="line">        batch_indices = torch.tensor(indices[i:<span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features[batch_indices], labels[batch_indices]</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数，并打印第一个batch</span></span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(X, <span class="string">&#x27;\n&#x27;</span>, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line"> </span><br><span class="line">Out: tensor([[ <span class="number">0.4113</span>, -<span class="number">0.1393</span>],</span><br><span class="line">            [-<span class="number">1.6666</span>, -<span class="number">0.4404</span>],</span><br><span class="line">            [ <span class="number">0.4753</span>,  <span class="number">0.4441</span>],</span><br><span class="line">            [ <span class="number">0.4856</span>, -<span class="number">0.4297</span>],</span><br><span class="line">            [ <span class="number">0.0251</span>,  <span class="number">3.2046</span>],</span><br><span class="line">            [-<span class="number">0.1691</span>,  <span class="number">0.2191</span>],</span><br><span class="line">            	[ <span class="number">0.1018</span>, -<span class="number">1.0219</span>],</span><br><span class="line">            [ <span class="number">0.6477</span>, -<span class="number">0.3860</span>],</span><br><span class="line">            [ <span class="number">0.6138</span>,  <span class="number">1.7933</span>],</span><br><span class="line">            [-<span class="number">0.2465</span>, -<span class="number">0.4396</span>]]) </span><br><span class="line">     tensor([[ <span class="number">5.5047</span>],</span><br><span class="line">            [ <span class="number">2.3659</span>],</span><br><span class="line">            [ <span class="number">3.6375</span>],</span><br><span class="line">            [ <span class="number">6.6427</span>],</span><br><span class="line">            [-<span class="number">6.6450</span>],</span><br><span class="line">            [ <span class="number">3.0990</span>],</span><br><span class="line">            [ <span class="number">7.8872</span>],</span><br><span class="line">            [ <span class="number">6.8009</span>],</span><br><span class="line">            [-<span class="number">0.6662</span>],</span><br><span class="line">            [ <span class="number">5.2153</span>]])</span><br></pre></td></tr></table></figure><blockquote><ul><li><p><code>batch_indices</code>中，<code>indices</code>有指数、目录的意思，而<code>batch_indices</code>的意思就是<strong>一个批量的索引编号</strong></p></li><li><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/">torch.tensor(list)【用Python–list构造张量】</a></p></li><li><p><code>yield</code>语句可以理解为：先<code>return</code>这个循环内部<code>batch_indices</code>所指的样本特征<code>features</code>和标签<code>labels</code>，然后返回一个自动迭代器，再返回下一个<code>batch_indices</code>所指数据【具体理解见<code>python</code>】</p></li></ul></blockquote><h4 id="3-3-算法核心"><a href="#3-3-算法核心" class="headerlink" title="3.3 算法核心"></a><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>3.3 算法核心</h4><p>数据已就绪，此时<strong>定义 初始化模型参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.normal():返回一个张量（均值，标准差）</span></span><br><span class="line">w = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(<span class="number">2</span>, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p><strong>定义模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linreg</span>(<span class="params">X, w, b</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;线性回归模型。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(X, w) + b   <span class="comment">#返回预测值【y_hat】</span></span><br></pre></td></tr></table></figure><p><strong>定义损失函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">squared_loss</span>(<span class="params">y_hat, y</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;均方损失。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.reshape(y_hat.shape))**<span class="number">2</span> / <span class="number">2</span>   <span class="comment">#注意:此时未求均值</span></span><br></pre></td></tr></table></figure><p><strong>定义优化算法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, lr, batch_size</span>):  <span class="comment">#params是w或b</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;小批量随机梯度下降。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():   <span class="comment">#先不计算梯度，更新的时候才有梯度计算</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">            param -= lr * param.grad / batch_size <span class="comment">#使用负梯度更新w或b，再求均值</span></span><br><span class="line">            param.grad.zero_()                    <span class="comment">#把梯度置零【pytorch不会自动清零】</span></span><br></pre></td></tr></table></figure><p><strong>训练过程</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span>               <span class="comment">#学习率【超参数】</span></span><br><span class="line">num_epochs = <span class="number">3</span>            <span class="comment">#训练次数</span></span><br><span class="line">net = linreg            <span class="comment">#模型类型【为便于更换模型】【linear regression】</span></span><br><span class="line">loss = squared_loss     	<span class="comment">#损失函数【为便于更换损失函数】</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y)</span><br><span class="line">        l.<span class="built_in">sum</span>().backward()                <span class="comment">#▲ 对损失函数的和，做后向计算求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)      <span class="comment">#通过梯度下降做优化,更新w、b</span></span><br><span class="line">                                        <span class="comment">#此处的batch_size,仅对本例有效【样本总数1000】</span></span><br><span class="line">    <span class="comment">#with语句：类似于try-catch-final</span></span><br><span class="line">    <span class="comment">#做完一轮epoch，输出结果</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():                </span><br><span class="line">        train_l = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(train_l.mean()):f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">Out: </span><br><span class="line">    epoch <span class="number">1</span>, loss <span class="number">0.033913</span></span><br><span class="line">    epoch <span class="number">2</span>, loss <span class="number">0.000125</span></span><br><span class="line">    epoch <span class="number">3</span>, loss <span class="number">0.000050</span></span><br></pre></td></tr></table></figure><p>比较真实参数和通过训练学到的参数来评估训练的成功程度【因为是人工数据集，因此我们知道真实的<code>w&amp;b</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In:  <span class="built_in">print</span>(<span class="string">f&#x27;w的估计误差: <span class="subst">&#123;true_w - w.reshape(true_w.shape)&#125;</span>&#x27;</span>)</span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f&#x27;b的估计误差: <span class="subst">&#123;true_b - b&#125;</span>&#x27;</span>)   <span class="comment">#f-string的用法，见python-start</span></span><br><span class="line"></span><br><span class="line">Out: w的估计误差: tensor([-<span class="number">3.0041e-05</span>,  <span class="number">2.2221e-04</span>], grad_fn=&lt;SubBackward0&gt;)</span><br><span class="line">     b的估计误差: tensor([-<span class="number">0.0004</span>], grad_fn=&lt;RsubBackward1&gt;)</span><br></pre></td></tr></table></figure><blockquote><p>学习率较大时，可能会出现求导除零、超出浮点数范围NAN的问题</p></blockquote><p><img src="./9-9.png"></p><h4 id="3-4-从零实现Q-amp-A"><a href="#3-4-从零实现Q-amp-A" class="headerlink" title="3.4 从零实现Q&amp;A"></a>3.4 从零实现Q&amp;A</h4><ul><li><code>detach()</code>：将梯度从计算图中拿出来，不计算梯度</li><li><code>data_iter</code>函数中的写法中，是将所有数据load，如果数据过大，内存会爆的解决办法？【就算有100万条图片数据，我只要每次读入一个batch就行了。如果读入开销大，可以进行预读入】</li><li><code>Indices</code>必须转为<code>Tensor</code>才能去取到样本数据吗？【<span style="background:#b3de4b">待测试</span>】</li><li>每次随机拿一个batch，怎么保证所有的数据都读过了？【本例中的做法是拿到全部样本的索引，然后shuffle，之后顺序取索引号】</li><li>如果样本不是批量的整数倍，如果处理多余的样本？【三种做法。最常见的做法是，照样拿，正如本例中，反之求的是样本损失的平均值（本例中偷懒了）；第二种是直接丢掉；第三种是在原有样本中再随机挑选样本再补齐】</li><li>训练的epoch需要判断收敛吗？怎么做？【即判断epoch已经收敛时，自动化停止训练。最直接的，相邻两次epoch差别不大的时候，就可以认为收敛了，如相差小于1%时认为已收敛；第二种是使用验证数据集，判断模型精度不再增加时，停止训练；实际情况中，也可凭经验设置】</li><li></li></ul><br><h3 id="4、线性回归简洁实现"><a href="#4、线性回归简洁实现" class="headerlink" title="4、线性回归简洁实现"></a>4、线性回归简洁实现</h3><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>通过使用深度学习框架来简洁地实现 线性回归模型 生成数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data  <span class="comment">#使用框架生成数据集</span></span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">true_w = torch.tensor([<span class="number">2</span>, -<span class="number">3.4</span>])</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#该函数使用w/b参数，生成1000个数据样本</span></span><br><span class="line">features, labels = d2l.synthetic_data(true_w, true_b, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>调用框架中现有的API来读取数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_array</span>(<span class="params">data_arrays, batch_size, is_train=<span class="literal">True</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;构造一个PyTorch数据迭代器。&quot;&quot;&quot;</span></span><br><span class="line">    dataset = data.TensorDataset(*data_arrays)</span><br><span class="line">    <span class="keyword">return</span> data.DataLoader(dataset, batch_size, shuffle=is_train)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">data_iter = load_array((features, labels), batch_size) </span><br><span class="line"></span><br><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(data_iter))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">     [tensor([[ <span class="number">0.5650</span>,  <span class="number">1.3529</span>],</span><br><span class="line">         [-<span class="number">0.1374</span>,  <span class="number">0.1456</span>],</span><br><span class="line">         [-<span class="number">0.4337</span>,  <span class="number">0.6755</span>],</span><br><span class="line">         [-<span class="number">1.2355</span>,  <span class="number">0.1951</span>],</span><br><span class="line">         [-<span class="number">0.3829</span>,  <span class="number">1.3490</span>],</span><br><span class="line">         [ <span class="number">0.9553</span>, -<span class="number">0.6591</span>],</span><br><span class="line">         [-<span class="number">1.2348</span>,  <span class="number">3.1388</span>],</span><br><span class="line">         [-<span class="number">1.2602</span>, -<span class="number">1.9106</span>],</span><br><span class="line">         [-<span class="number">0.4024</span>,  <span class="number">0.9888</span>],</span><br><span class="line">         [ <span class="number">1.9672</span>,  <span class="number">0.9410</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.7252</span>],</span><br><span class="line">         [ <span class="number">3.4301</span>],</span><br><span class="line">         [ <span class="number">1.0369</span>],</span><br><span class="line">         [ <span class="number">1.0758</span>],</span><br><span class="line">         [-<span class="number">1.1405</span>],</span><br><span class="line">         [ <span class="number">8.3455</span>],</span><br><span class="line">         [-<span class="number">8.9359</span>],</span><br><span class="line">         [ <span class="number">8.1831</span>],</span><br><span class="line">         [ <span class="number">0.0135</span>],</span><br><span class="line">         [ <span class="number">4.9325</span>]])]</span><br></pre></td></tr></table></figure><blockquote><ol><li>将<code>features</code>和<code>labels</code>转为<code>list</code>传入<code>TensorDateset</code>，会得到一个<code>pytorch</code>的<code>DataSet</code></li><li>将<code>pytorch</code>的<code>DataSet</code>传入<code>DataLoader</code>，返回一个迭代器【该迭代器每次从<code>DataSet</code>中随机取<code>batch_size</code>大小的小批量，<code>shuffle</code>参数标志是否打乱】</li><li>调用<code>load_array</code>返回一个迭代器，交由<code>next</code>函数执行一次【即只展示第一组样本数据】</li></ol></blockquote><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>使用框架的预定义好的层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#nn:神经网络 neural network</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn    </span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment">#线性回归，是一个单层神经网络</span></span><br><span class="line"><span class="comment">#为了方便更换结构，我们将这个线性层，放进Sequential的一个容器里【顺序执行】</span></span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>初始化模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.normal_(<span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">net[<span class="number">0</span>].bias.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#net[0]:可访问到这个线性层</span></span><br><span class="line"><span class="comment">#.weight:可访问到该层的权重w</span></span><br><span class="line"><span class="comment">#.data:可访问到该层的数据</span></span><br><span class="line"><span class="comment">#.normal():以正态分布替换两个权重</span></span><br><span class="line"><span class="comment">#.fill_(0):以0替换b</span></span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>计算均方误差使用的是<code>MSELoss</code>类，也称为平方 <code>L2</code>​ 范数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>实例化 <code>SGD</code> 实例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#框架自带SGD需要两个参数</span></span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><p><span style="background:#ff0"><i class="fas fa-bullseye" style="margin-right:10px"></i></span>训练过程代码与我们从零开始实现时所做的非常相似</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        l = loss(net(X), y)</span><br><span class="line">        trainer.zero_grad()</span><br><span class="line">        l.backward()   <span class="comment">#此处pytorch自动做了sum</span></span><br><span class="line">        trainer.step() <span class="comment">#step函数进行模型的更新 </span></span><br><span class="line">    l = loss(net(features), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l:f&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">Out: </span><br><span class="line">     epoch <span class="number">1</span>, loss <span class="number">0.000269</span></span><br><span class="line">     epoch <span class="number">2</span>, loss <span class="number">0.000093</span></span><br><span class="line">     epoch <span class="number">3</span>, loss <span class="number">0.000093</span></span><br></pre></td></tr></table></figure><br><h3 id="5、线性回归QA"><a href="#5、线性回归QA" class="headerlink" title="5、线性回归QA"></a>5、线性回归QA</h3><blockquote><p>Google colab</p></blockquote><ul><li><p>平方损失与绝对插值【最大区别是：后者是不可导函数】【实际上应用中差别不大】</p></li><li><p>损失为什么求平均？【可以不求平均，二者在数值上是等价的。对于<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msub><mi>w</mi><mrow><mi>t</mi></mrow></msub><mo>=</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mi>η</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>ℓ</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow></mfrac></math>，要么在梯度上求平均，要么学习率求平均】【求平均的好处在于，无论样本有多大，梯度的值有差不多，学习率比较好调，是一种解耦操作】【如果不求平均，梯度会比较大】</p></li><li><p>如何快速寻找合适的学习率？【优化算法章节】</p></li><li><p><code>batchsize</code>是否会影响模型效果？【小是好的，大了不行。越小，对于收敛越好，本质是采样样本越少，样本噪音越多，即就是离真实情况差很多，这种情况下模型的泛化能力比较强】</p></li><li><p>学习率和批次，对收敛结果的影响不是很大，除非超参数很离谱</p></li><li><p>为什么机器学习优化算法采用梯度下降，即一阶导算法，而不是采用牛顿法，即二阶导算法？【二阶导很难求，很贵，只能近似。另外，机器学习的真实案例，很少能求最优解！】</p></li><li><p>使用SGD，是因为大部分Loss太复杂，无显示解，即无法推导出导数为0的解，只能用batch去逼近</p></li><li><p>简洁实现中，for循环中最后一个<code>l = loss(net(features), labels)</code>仅仅是为了print。没有进行清零，是因为只计算了loss，输入forward前向计算，没有backward反向计算，无需清零【pytorch的梯度不自动清零，不清零时，梯度会在原梯度上直接累加】</p></li></ul><br><h2 id="十、Softmax回归"><a href="#十、Softmax回归" class="headerlink" title="十、Softmax回归"></a>十、Softmax回归</h2><h3 id="1、引入Softmax"><a href="#1、引入Softmax" class="headerlink" title="1、引入Softmax"></a>1、引入Softmax</h3><p><img src="./10-1.png"></p><p><img src="./10-2.png"></p><p><img src="./10-3.png"></p><p><img src="./10-4.png"></p><p><img src="./10-5.png"></p><br><h4 id="1-1-Softmax简易求导"><a href="#1-1-Softmax简易求导" class="headerlink" title="1.1 Softmax简易求导"></a>1.1 Softmax简易求导</h4><blockquote><p>Softmax【柔性最大值函数】推导过程</p><p>参考：</p><ul><li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1RT411k7Uz/?spm_id_from=333.337.search-card.all.click&vd_source=ad866fe26d18693e4132a3c33f8fba36">softmax梯度下降整个求解过程：blibili</a></p></li><li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25723112">知乎：详解softmax函数以及相关求导过程</a></p></li></ul></blockquote><p>当我们对分类的Loss进行改进的时候，我们要通过梯度下降，每次优化一个step大小的梯度，<strong>这个时候我们就要求Loss对每个权重矩阵的偏导，然后应用链式法则</strong>。</p><p><strong>下面我们举出一个简单例子，原理一样</strong></p><p><img src="./10-6.png"></p><p>我们能得到下面公式：</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z<span class="number">4</span> = w<span class="number">41</span>*<span class="meta">o1</span>+w<span class="number">42</span>*<span class="meta">o2</span>+w<span class="number">43</span>*<span class="meta">o3</span></span><br><span class="line">z<span class="number">5</span> = w<span class="number">51</span>*<span class="meta">o1</span>+w<span class="number">52</span>*<span class="meta">o2</span>+w<span class="number">53</span>*<span class="meta">o3</span></span><br><span class="line">z<span class="number">6</span> = w<span class="number">61</span>*<span class="meta">o1</span>+w<span class="number">62</span>*<span class="meta">o2</span>+w<span class="number">63</span>*<span class="meta">o3</span></span><br></pre></td></tr></table></figure><blockquote><p>z4,z5,z6分别代表结点4,5,6的输出，01,02,03代表是结点1,2,3往后传的输入</p></blockquote><p>那么我们可以经过<code>softmax</code>函数得到</p><p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>a</mi><mn>4</mn><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>4</mn><mo stretchy="false">)</mo></mrow><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>4</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>5</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>6</mn><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mi>a</mi><mn>5</mn><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>5</mn><mo stretchy="false">)</mo></mrow><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>4</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>5</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>6</mn><mo stretchy="false">)</mo></mrow></mfrac><mo>,</mo><mi>a</mi><mn>6</mn><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>6</mn><mo stretchy="false">)</mo></mrow><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>4</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>5</mn><mo stretchy="false">)</mo><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>z</mi><mn>6</mn><mo stretchy="false">)</mo></mrow></mfrac></math></p><p><strong>OK，我们的重头戏来了，怎么根据求梯度，然后利用梯度下降方法更新梯度！</strong></p><p>要使用梯度下降，肯定需要一个损失函数，<strong>这里我们使用交叉熵作为我们的损失函数。</strong>交叉熵函数形式如下：</p><p><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>i</mi></mrow></munder><msub><mi>y</mi><mi>i</mi></msub><msub><mi>ln</mi><mrow></mrow></msub><mo data-mjx-texclass="NONE">⁡</mo><mrow><msub><mi>a</mi><mi>i</mi></msub></mrow></math></p><blockquote><p>其中y代表我们的真实值，a代表我们softmax求出的值，即预测值。i代表的是输出结点的标号！在上面例子，i就可以取值为4,5,6三个结点。</p></blockquote><p><strong>虽然，看起来感觉复杂了，还有累和，然后还要求导，每一个a都是softmax之后的形式！</strong></p><p><strong>但是实际上不是这样的，我们往往在真实中，如果只预测一个结果，那么在目标中只有一个结点的值为1，比如我认为在该状态下，我想要输出的是第四个动作（第四个结点）,那么训练数据的输出就是a4 = 1，a5=0，a6=0。在这种情况下，除了一个为1，其它都是0，那么所谓的求和符合，就是一个幌子，可以去掉啦！</strong></p><blockquote><p>为了形式化说明，我这里认为训练数据的真实输出为第j个为1，其它均为0！</p></blockquote><p><strong>那么Loss就变成了</strong><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><msub><mi>y</mi><mi>j</mi></msub><msub><mi>ln</mi><mrow></mrow></msub><mo data-mjx-texclass="NONE">⁡</mo><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow></math>，下面开始求导数：</p><p>首先，<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><msub><mi>y</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></math>，那么此时<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>−</mo><mi>ln</mi><mo data-mjx-texclass="NONE">⁡</mo><mrow><msub><mi>a</mi><mi>j</mi></msub></mrow></math>，那么形式越来越简单了，求导分析如下：</p><p><strong>参数的形式在该例子中，总共分为w41，w42，w43，w51，w52，w53，w61，w62，w63。这些，那么比如我要求出w41，w42，w43的偏导，就需要将Loss函数求偏导传到结点4，然后再利用链式法则继续求导即可。举个例子，此时求w41的偏导为:</strong></p><p><img src="./10-17.jpg"></p><p><strong>w51…..w63等参数的偏导同理可以求出，那么我们的关键就在于Loss函数对于结点4,5,6的偏导怎么求，如下：</strong></p><p><strong>这里分为两种情况：一是当 <code>j=i</code> 时：</strong>比如我选定了 <code>j=4</code>，那么就是说现在求导传到4结点</p><p><img src="./10-18.png"></p><p>那么由上面求导结果再乘以交叉熵损失函数求导，结果为 <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mo stretchy="false">(</mo><msub><mi>a</mi><mi>j</mi></msub><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></math>（<strong>形式非常简单，这说明只要正向求一次得出结果，然后反向传梯度的时候，只需要将它结果减1即可，后面还会举例子！</strong>）那么我们可以得到Loss对于4结点的偏导就求出了了（<strong>这里假定4是我们的预计输出</strong>）</p><p><strong>第二种情况是当 <code>j≠i</code> 时：往前传</strong></p><p><img src="./10-19.jpg"></p><p><img src="./10-20.png"></p><p>由上面求导结果再乘以交叉熵损失函数求导，结果为<strong>（形式非常简单，这说明我只要正向求一次得出结果，然后反向传梯度的时候，只需要将它结果保存即可，后续例子会讲到</strong>）<strong>这里就求出了除4之外的其它所有结点的偏导，然后利用链式法则继续传递过去即可！我们的问题也就解决了！</strong></p><blockquote><p><span style="background:#ff0"><i class="fas fa-brain" style="margin-right:10px"></i>  </span><strong>为什么要对 <code>j?=i</code> 分情况求导？</strong></p><p>W参数的更新，不仅仅需要对<strong>目标类别的神经元</strong>进行更新，也要更新<strong>错误类别的输出神经元</strong>的W参数。这样才能尽快让结果往正确类别去靠。如果只是更新正确类别的神经元参数，那可能会使得收敛速度变慢很多。</p><p>也就是说，i = j的时候，更新的只是正确类别的W参数，i !=j时，也更新其他错误类别的参数（两者更新的幅度不一样：学习率是一样的，但是梯度大小不一样）。</p></blockquote><br><h4 id="1-2-举例：为什么计算会比较方便"><a href="#1-2-举例：为什么计算会比较方便" class="headerlink" title="1.2 举例：为什么计算会比较方便"></a>1.2 举例：为什么计算会比较方便</h4><p><strong>举个例子，通过若干层的计算，最后得到的某个训练样本的向量的分数是[ 2, 3, 4 ]，<br>那么经过softmax函数作用后概率分别就是：<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mo stretchy="false">[</mo><mfrac><msup><mi>e</mi><mn>2</mn></msup><mrow><msup><mi>e</mi><mn>2</mn></msup><mo>+</mo><msup><mi>e</mi><mn>3</mn></msup><mo>+</mo><msup><mi>e</mi><mn>4</mn></msup></mrow></mfrac><mo>,</mo><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mfrac><msup><mi>e</mi><mn>2</mn></msup><mrow><msup><mi>e</mi><mn>2</mn></msup><mo>+</mo><msup><mi>e</mi><mn>3</mn></msup><mo>+</mo><msup><mi>e</mi><mn>4</mn></msup></mrow></mfrac><mo>,</mo><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mfrac><msup><mi>e</mi><mn>2</mn></msup><mrow><msup><mi>e</mi><mn>2</mn></msup><mo>+</mo><msup><mi>e</mi><mn>3</mn></msup><mo>+</mo><msup><mi>e</mi><mn>4</mn></msup></mrow></mfrac><mo stretchy="false">]</mo><mo>=</mo><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mo stretchy="false">[</mo><mn>0.0903</mn><mo>,</mo><mn>0.2447</mn><mo>,</mo><mn>0.665</mn><mo stretchy="false">]</mo></math></strong></p><p><strong>如果这个样本正确的分类是第二个的话，那么计算出来的偏导就是<code>[0.0903, 0.2447-1, 0.665] = [0.0903, -0.7553, 0.665]</code>，故计算梯度很简单！然后再根据这个进行back propagation就可以了</strong></p><br><h4 id="1-3-Softmax广义求导"><a href="#1-3-Softmax广义求导" class="headerlink" title="1.3 Softmax广义求导"></a>1.3 Softmax广义求导</h4><blockquote><p>参考<a target="_blank" rel="noopener" href="https://www.cnblogs.com/smallredness/p/11047718.html">此处</a></p></blockquote><ul><li>向量<code>y</code>（为<code>one-hot</code>编码，只有一个值为1，其他的值为0）：真实类别标签(维度为<code>m</code>，表示有<code>m</code>类别)</li><li>向量<code>z</code>为<code>softmax</code>函数的输入，<code>m</code>维列向量</li><li>向量<code>s</code>为<code>softmax</code>函数的输出，<code>m</code>维列向量</li></ul><p><svg xmlns="http://www.w3.org/2000/svg" width="47.476ex" height="11.765ex" viewbox="0 -2850 20984.6 5200" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-416-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-416-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-416-TEX-S4-23A1" d="M319 -645V1154H666V1070H403V-645H319Z"/><path id="MJX-416-TEX-S4-23A3" d="M319 -644V1155H403V-560H666V-644H319Z"/><path id="MJX-416-TEX-S4-23A2" d="M319 0V602H403V0H319Z"/><path id="MJX-416-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-416-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-416-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/><path id="MJX-416-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-416-TEX-S4-23A4" d="M0 1070V1154H347V-645H263V1070H0Z"/><path id="MJX-416-TEX-S4-23A6" d="M263 -560V1155H347V-644H0V-560H263Z"/><path id="MJX-416-TEX-S4-23A5" d="M263 0V602H347V0H263Z"/><path id="MJX-416-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/><path id="MJX-416-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-416-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-416-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-416-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/><path id="MJX-416-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/><path id="MJX-416-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D466"/></g><g data-mml-node="mo" transform="translate(767.8, 0)"><use xlink:href="#MJX-416-TEX-N-3D"/></g><g data-mml-node="mrow" transform="translate(1823.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-S4-23A1" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A3" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A2" transform="scale(1, 4.49)"/></svg></g><g data-mml-node="mtable" transform="translate(667, 0)"><g data-mml-node="mtr" transform="translate(0, 2100)"><g data-mml-node="mtd" transform="translate(136.9, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D466"/></g><g data-mml-node="mn" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-31"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 700)"><g data-mml-node="mtd" transform="translate(136.9, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D466"/></g><g data-mml-node="mn" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-32"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -700)"><g data-mml-node="mtd"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(444.7, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(889.3, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g></g></g><g data-mml-node="mtr" transform="translate(0, -2100)"><g data-mml-node="mtd" transform="translate(3.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D45A"/></g></g></g></g></g><g data-mml-node="mo" transform="translate(1834.3, 0)"><use xlink:href="#MJX-416-TEX-S4-23A4" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A6" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A5" transform="scale(1, 4.49)"/></svg></g></g><g data-mml-node="mo" transform="translate(4324.9, 0)"><use xlink:href="#MJX-416-TEX-N-2C"/></g><g data-mml-node="mstyle" transform="translate(4769.6, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mi" transform="translate(4936.2, 0)"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mo" transform="translate(5679, 0)"><use xlink:href="#MJX-416-TEX-N-3D"/></g><g data-mml-node="mrow" transform="translate(6734.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-S4-23A1" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A3" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A2" transform="scale(1, 4.49)"/></svg></g><g data-mml-node="mtable" transform="translate(667, 0)"><g data-mml-node="mtr" transform="translate(0, 2100)"><g data-mml-node="mtd" transform="translate(149.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mn" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-31"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 700)"><g data-mml-node="mtd" transform="translate(149.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mn" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-32"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -700)"><g data-mml-node="mtd"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(444.7, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(889.3, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g></g></g><g data-mml-node="mtr" transform="translate(0, -2100)"><g data-mml-node="mtd" transform="translate(15.7, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D45A"/></g></g></g></g></g><g data-mml-node="mo" transform="translate(1834.3, 0)"><use xlink:href="#MJX-416-TEX-S4-23A4" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A6" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A5" transform="scale(1, 4.49)"/></svg></g></g><g data-mml-node="mo" transform="translate(9236.1, 0)"><use xlink:href="#MJX-416-TEX-N-2C"/></g><g data-mml-node="mstyle" transform="translate(9680.8, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mi" transform="translate(9847.4, 0)"><use xlink:href="#MJX-416-TEX-I-1D460"/></g><g data-mml-node="mo" transform="translate(10594.2, 0)"><use xlink:href="#MJX-416-TEX-N-3D"/></g><g data-mml-node="mrow" transform="translate(11650, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-S4-23A1" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A3" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A2" transform="scale(1, 4.49)"/></svg></g><g data-mml-node="mtable" transform="translate(667, 0)"><g data-mml-node="mtr" transform="translate(0, 2100)"><g data-mml-node="mtd" transform="translate(147.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D460"/></g><g data-mml-node="mn" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-31"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, 700)"><g data-mml-node="mtd" transform="translate(147.4, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D460"/></g><g data-mml-node="mn" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-N-32"/></g></g></g></g><g data-mml-node="mtr" transform="translate(0, -700)"><g data-mml-node="mtd"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(444.7, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g><g data-mml-node="mo" transform="translate(889.3, 0)"><use xlink:href="#MJX-416-TEX-N-2E"/></g></g></g><g data-mml-node="mtr" transform="translate(0, -2100)"><g data-mml-node="mtd" transform="translate(13.7, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D45A"/></g></g></g></g></g><g data-mml-node="mo" transform="translate(1834.3, 0)"><use xlink:href="#MJX-416-TEX-S4-23A4" transform="translate(0, 1696)"/><use xlink:href="#MJX-416-TEX-S4-23A6" transform="translate(0, -1706)"/><svg width="667" height="1802" y="-651" x="0" viewbox="0 450.5 667 1802"><use xlink:href="#MJX-416-TEX-S4-23A5" transform="scale(1, 4.49)"/></svg></g></g><g data-mml-node="mo" transform="translate(14151.3, 0)"><use xlink:href="#MJX-416-TEX-N-2C"/></g><g data-mml-node="mstyle" transform="translate(14596, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="msub" transform="translate(14762.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(15803.4, 0)"><use xlink:href="#MJX-416-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(16859.2, 0)"><g data-mml-node="msup" transform="translate(1536.4, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D456"/></g></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(220, -749.6)"><g data-mml-node="mstyle"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-416-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-416-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-416-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-416-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-416-TEX-I-1D458"/></g></g></g></g></g></g><rect width="3885.4" height="60" x="120" y="220"/></g></g></g></svg></p><p><img src="./10-21.png"></p><blockquote><p>小问题：SVG的居中需要注意的是，SVG默认是内联样式。</p></blockquote><p>交叉熵损失函数：<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>c</mi><mo>=</mo><mo>−</mo><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi></mrow></munderover><msub><mi>y</mi><mi>j</mi></msub><mi>ln</mi><mo data-mjx-texclass="NONE">⁡</mo><msub><mi>s</mi><mi>j</mi></msub></math></p><p>损失函数对向量<code>z</code>中的每个<code>zi</code>求偏导：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="44.585ex" height="6.549ex" viewbox="0 -1562.5 19706.6 2894.7" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-606-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/><path id="MJX-606-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/><path id="MJX-606-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-606-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-606-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-606-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-606-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/><path id="MJX-606-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-606-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-606-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-606-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-606-TEX-N-6C" d="M42 46H56Q95 46 103 60V68Q103 77 103 91T103 124T104 167T104 217T104 272T104 329Q104 366 104 407T104 482T104 542T103 586T103 603Q100 622 89 628T44 637H26V660Q26 683 28 683L38 684Q48 685 67 686T104 688Q121 689 141 690T171 693T182 694H185V379Q185 62 186 60Q190 52 198 49Q219 46 247 46H263V0H255L232 1Q209 2 183 2T145 3T107 3T57 1L34 0H26V46H42Z"/><path id="MJX-606-TEX-N-6E" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q450 438 463 329Q464 322 464 190V104Q464 66 466 59T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"/><path id="MJX-606-TEX-N-2061" d=""/><path id="MJX-606-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-606-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(383, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="mi" transform="translate(566, 0)"><use xlink:href="#MJX-606-TEX-I-1D450"/></g></g><g data-mml-node="mrow" transform="translate(220, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D456"/></g></g></g><rect width="1525" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2042.7, 0)"><use xlink:href="#MJX-606-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(3098.5, 0)"><use xlink:href="#MJX-606-TEX-N-2212"/></g><g data-mml-node="munderover" transform="translate(4043.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-606-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(124.5, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-606-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1190, 0)"><use xlink:href="#MJX-606-TEX-N-31"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D45A"/></g></g></g><g data-mml-node="mfrac" transform="translate(5653.8, 0)"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g><g data-mml-node="mi" transform="translate(1564, 0)"><use xlink:href="#MJX-606-TEX-N-6C"/><use xlink:href="#MJX-606-TEX-N-6E" transform="translate(278, 0)"/></g><g data-mml-node="mo" transform="translate(2398, 0)"><use xlink:href="#MJX-606-TEX-N-2061"/></g><g data-mml-node="msub" transform="translate(2564.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(1219.3, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g></g><rect width="3575" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(9691.1, 0)"><use xlink:href="#MJX-606-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(10191.3, 0)"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(12285.4, 0)"><use xlink:href="#MJX-606-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(13341.2, 0)"><use xlink:href="#MJX-606-TEX-N-2212"/></g><g data-mml-node="munderover" transform="translate(14285.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-606-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(124.5, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-606-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1190, 0)"><use xlink:href="#MJX-606-TEX-N-31"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D45A"/></g></g></g><g data-mml-node="mfrac" transform="translate(15896.5, 0)"><g data-mml-node="msub" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g><rect width="1031.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(17390, 0)"><use xlink:href="#MJX-606-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(17890.3, 0)"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-606-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-606-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g></g></g></svg></p><p>当 <code>j = i</code> 时：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="88.863ex" height="9.591ex" viewbox="0 -2882.4 39277.4 4239.4" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true"><defs><path id="MJX-956-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/><path id="MJX-956-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-956-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-956-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-956-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-956-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-956-TEX-S3-28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"/><path id="MJX-956-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/><path id="MJX-956-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/><path id="MJX-956-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-956-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/><path id="MJX-956-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-956-TEX-S3-29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"/><path id="MJX-956-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/><path id="MJX-956-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-956-TEX-SO-28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"/><path id="MJX-956-TEX-SO-29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"/><path id="MJX-956-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-956-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-956-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2094.1, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(3149.9, 0)"><g data-mml-node="mrow" transform="translate(220, 1432.9)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-N-2202"/></g><g data-mml-node="mrow" transform="translate(566, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-S3-28"/></g><g data-mml-node="mfrac" transform="translate(736, 0)"><g data-mml-node="msup" transform="translate(1453, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -307.4)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mrow" transform="translate(220, -629.6) scale(0.707)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mstyle" transform="scale(1.414)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g></g></g><g data-mml-node="msup" transform="translate(3363.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 359) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -340.4)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g></g><rect width="3461" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(4437, 0)"><use xlink:href="#MJX-956-TEX-S3-29"/></g></g></g><g data-mml-node="mrow" transform="translate(2427, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g><rect width="5939" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(9606.6, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(10662.4, 0)"><g data-mml-node="mrow" transform="translate(220, 803.3)"><g data-mml-node="msup"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mo" transform="translate(1274.9, 0)"><use xlink:href="#MJX-956-TEX-N-22C5"/></g><g data-mml-node="mstyle" transform="translate(1775.1, 0)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g><g data-mml-node="mo" transform="translate(3907.6, 0)"><use xlink:href="#MJX-956-TEX-N-2212"/></g><g data-mml-node="msup" transform="translate(4907.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mo" transform="translate(6182.7, 0)"><use xlink:href="#MJX-956-TEX-N-22C5"/></g><g data-mml-node="msup" transform="translate(6683, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g></g></g><g data-mml-node="msup" transform="translate(2472.9, -1007.5)"><g data-mml-node="mrow"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-28"/></g><g data-mml-node="mstyle" transform="translate(458, 0)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g></g><g data-mml-node="mo" transform="translate(4143.4, 0)"><use xlink:href="#MJX-956-TEX-SO-29"/></g></g><g data-mml-node="mn" transform="translate(4601.4, 576.6) scale(0.707)"><use xlink:href="#MJX-956-TEX-N-32"/></g></g><rect width="9710.7" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(20890.9, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(21946.7, 0)"><g data-mml-node="msup" transform="translate(1536.4, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mstyle" transform="translate(220, -749.6)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g></g><rect width="3885.4" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(26294.3, 0)"><use xlink:href="#MJX-956-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(26794.5, 0)"><g data-mml-node="mstyle" transform="translate(220, 803.3)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g><g data-mml-node="mo" transform="translate(3907.6, 0)"><use xlink:href="#MJX-956-TEX-N-2212"/></g><g data-mml-node="msup" transform="translate(4907.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g></g></g></g><g data-mml-node="mstyle" transform="translate(1357.6, -749.6)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D458"/></g></g></g></g></g><rect width="6160.5" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(33472.8, 0)"><use xlink:href="#MJX-956-TEX-N-3D"/></g><g data-mml-node="msub" transform="translate(34528.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(35513.8, 0)"><use xlink:href="#MJX-956-TEX-N-22C5"/></g><g data-mml-node="mrow" transform="translate(36014, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-956-TEX-N-28"/></g><g data-mml-node="mn" transform="translate(389, 0)"><use xlink:href="#MJX-956-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(1111.2, 0)"><use xlink:href="#MJX-956-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(2111.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-956-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-956-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(2874.4, 0)"><use xlink:href="#MJX-956-TEX-N-29"/></g></g></g></g></svg></p><p>当 <code>j ≠ i</code> 时：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="80.511ex" height="9.591ex" viewbox="0 -2882.4 35586.1 4239.4" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-1162-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/><path id="MJX-1162-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-1162-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-1162-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-1162-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-1162-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-1162-TEX-S3-28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"/><path id="MJX-1162-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/><path id="MJX-1162-TEX-SO-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"/><path id="MJX-1162-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-1162-TEX-I-1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"/><path id="MJX-1162-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-1162-TEX-S3-29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"/><path id="MJX-1162-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/><path id="MJX-1162-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/><path id="MJX-1162-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-1162-TEX-SO-28" d="M152 251Q152 646 388 850H416Q422 844 422 841Q422 837 403 816T357 753T302 649T255 482T236 250Q236 124 255 19T301 -147T356 -251T403 -315T422 -340Q422 -343 416 -349H388Q359 -325 332 -296T271 -213T212 -97T170 56T152 251Z"/><path id="MJX-1162-TEX-SO-29" d="M305 251Q305 -145 69 -349H56Q43 -349 39 -347T35 -338Q37 -333 60 -307T108 -239T160 -136T204 27T221 250T204 473T160 636T108 740T60 807T35 839Q35 850 50 850H56H69Q197 743 256 566Q305 425 305 251Z"/><path id="MJX-1162-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2094.1, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(3149.9, 0)"><g data-mml-node="mrow" transform="translate(220, 1432.9)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-N-2202"/></g><g data-mml-node="mrow" transform="translate(566, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-S3-28"/></g><g data-mml-node="mfrac" transform="translate(736, 0)"><g data-mml-node="msup" transform="translate(1436.3, 394) scale(0.707)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 472.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -307.4)"><use xlink:href="#MJX-1162-TEX-I-1D457"/></g></g></g></g><g data-mml-node="mrow" transform="translate(220, -629.6) scale(0.707)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mstyle" transform="scale(1.414)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1162-TEX-N-31"/></g></g></g></g></g><g data-mml-node="msup" transform="translate(3363.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 359) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -340.4)"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g></g></g></g></g><rect width="3461" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(4437, 0)"><use xlink:href="#MJX-1162-TEX-S3-29"/></g></g></g><g data-mml-node="mrow" transform="translate(2427, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D456"/></g></g></g><rect width="5939" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(9606.6, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mfrac" transform="translate(10662.4, 0)"><g data-mml-node="mrow" transform="translate(220, 803.3)"><g data-mml-node="mn"><use xlink:href="#MJX-1162-TEX-N-30"/></g><g data-mml-node="mo" transform="translate(722.2, 0)"><use xlink:href="#MJX-1162-TEX-N-22C5"/></g><g data-mml-node="mstyle" transform="translate(1222.4, 0)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1162-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g></g></g></g><g data-mml-node="mo" transform="translate(3907.6, 0)"><use xlink:href="#MJX-1162-TEX-N-2212"/></g><g data-mml-node="msup" transform="translate(4907.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D457"/></g></g></g></g><g data-mml-node="mo" transform="translate(6216.2, 0)"><use xlink:href="#MJX-1162-TEX-N-22C5"/></g><g data-mml-node="msup" transform="translate(6716.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D456"/></g></g></g></g></g></g><g data-mml-node="msup" transform="translate(2213.3, -1007.5)"><g data-mml-node="mrow"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-28"/></g><g data-mml-node="mstyle" transform="translate(458, 0)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1162-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g></g></g></g></g><g data-mml-node="mo" transform="translate(4143.4, 0)"><use xlink:href="#MJX-1162-TEX-SO-29"/></g></g><g data-mml-node="mn" transform="translate(4601.4, 576.6) scale(0.707)"><use xlink:href="#MJX-1162-TEX-N-32"/></g></g><rect width="9191.6" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(20371.7, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(21427.5, 0)"><use xlink:href="#MJX-1162-TEX-N-2212"/></g><g data-mml-node="mfrac" transform="translate(22205.5, 0)"><g data-mml-node="msup" transform="translate(1519.6, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D457"/></g></g></g></g><g data-mml-node="mstyle" transform="translate(220, -749.6)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1162-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g></g></g></g></g><rect width="3885.4" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(26553.1, 0)"><use xlink:href="#MJX-1162-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(27053.4, 0)"><g data-mml-node="msup" transform="translate(1536.4, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mstyle" transform="translate(220, -749.6)"><g data-mml-node="munderover"><g data-mml-node="mo"><use xlink:href="#MJX-1162-TEX-SO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(1056, 477.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D45A"/></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g><g data-mml-node="mo" transform="translate(521, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1299, 0)"><use xlink:href="#MJX-1162-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(2544.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 289) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D458"/></g></g></g></g></g><rect width="3885.4" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(31456.6, 0)"><use xlink:href="#MJX-1162-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(32512.3, 0)"><use xlink:href="#MJX-1162-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(33290.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(34322.9, 0)"><use xlink:href="#MJX-1162-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(34823.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-1162-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-1162-TEX-I-1D456"/></g></g></g></g></svg></p><p>故有：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="27.125ex" height="5.472ex" viewbox="0 -1469.2 11989.2 2418.7" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-134-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/><path id="MJX-134-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-134-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-134-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-134-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-134-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-134-TEX-S3-7B" d="M618 -943L612 -949H582L568 -943Q472 -903 411 -841T332 -703Q327 -682 327 -653T325 -350Q324 -28 323 -18Q317 24 301 61T264 124T221 171T179 205T147 225T132 234Q130 238 130 250Q130 255 130 258T131 264T132 267T134 269T139 272T144 275Q207 308 256 367Q310 436 323 519Q324 529 325 851Q326 1124 326 1154T332 1205Q369 1358 566 1443L582 1450H612L618 1444V1429Q618 1413 616 1411L608 1406Q599 1402 585 1393T552 1372T515 1343T479 1305T449 1257T429 1200Q425 1180 425 1152T423 851Q422 579 422 549T416 498Q407 459 388 424T346 364T297 318T250 284T214 264T197 254L188 251L205 242Q290 200 345 138T416 3Q421 -18 421 -48T423 -349Q423 -397 423 -472Q424 -677 428 -694Q429 -697 429 -699Q434 -722 443 -743T465 -782T491 -816T519 -845T548 -868T574 -886T595 -899T610 -908L616 -910Q618 -912 618 -928V-943Z"/><path id="MJX-134-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/><path id="MJX-134-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-134-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-134-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-134-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-134-TEX-N-A0" d=""/><path id="MJX-134-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2094.1, 0)"><use xlink:href="#MJX-134-TEX-N-3D"/></g><g data-mml-node="mrow" transform="translate(3149.9, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-134-TEX-S3-7B"/></g><g data-mml-node="mtable" transform="translate(750, 0)"><g data-mml-node="mtr" transform="translate(0, 622.1)"><g data-mml-node="mtd"><g data-mml-node="msub"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(985.2, 0)"><use xlink:href="#MJX-134-TEX-N-22C5"/></g><g data-mml-node="mo" transform="translate(1485.4, 0)"><use xlink:href="#MJX-134-TEX-N-28"/></g><g data-mml-node="mn" transform="translate(1874.4, 0)"><use xlink:href="#MJX-134-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(2596.6, 0)"><use xlink:href="#MJX-134-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(3596.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(4359.8, 0)"><use xlink:href="#MJX-134-TEX-N-29"/></g></g><g data-mml-node="mtd" transform="translate(5748.8, 0)"><g data-mml-node="mtext"><use xlink:href="#MJX-134-TEX-N-A0"/></g><g data-mml-node="mi" transform="translate(250, 0)"><use xlink:href="#MJX-134-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(939.8, 0)"><use xlink:href="#MJX-134-TEX-N-3D"/></g><g data-mml-node="mi" transform="translate(1995.6, 0)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g></g><g data-mml-node="mtr" transform="translate(0, -577.9)"><g data-mml-node="mtd"><g data-mml-node="mo"><use xlink:href="#MJX-134-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(778, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(1810.6, 0)"><use xlink:href="#MJX-134-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(2310.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-134-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g></g><g data-mml-node="mtd" transform="translate(5748.8, 0)"><g data-mml-node="mtext"><use xlink:href="#MJX-134-TEX-N-A0"/></g><g data-mml-node="mi" transform="translate(250, 0)"><use xlink:href="#MJX-134-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(939.8, 0)"><use xlink:href="#MJX-134-TEX-N-2260"/></g><g data-mml-node="mi" transform="translate(1995.6, 0)"><use xlink:href="#MJX-134-TEX-I-1D456"/></g></g></g></g><g data-mml-node="mo" transform="translate(8839.3, 0)"/></g></g></g></svg></p><p>损失函数对向量<code>z</code>中的每个<code>zi</code>求偏导：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="94.717ex" height="7.07ex" viewbox="0 -1749.5 41865 3124.8" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true"><defs><path id="MJX-844-TEX-N-2202" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/><path id="MJX-844-TEX-I-1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/><path id="MJX-844-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-844-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-844-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-844-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-844-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/><path id="MJX-844-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-844-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-844-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-844-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-844-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-844-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/><path id="MJX-844-TEX-S4-28" d="M758 -1237T758 -1240T752 -1249H736Q718 -1249 717 -1248Q711 -1245 672 -1199Q237 -706 237 251T672 1700Q697 1730 716 1749Q718 1750 735 1750H752Q758 1744 758 1741Q758 1737 740 1713T689 1644T619 1537T540 1380T463 1176Q348 802 348 251Q348 -242 441 -599T744 -1218Q758 -1237 758 -1240Z"/><path id="MJX-844-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/><path id="MJX-844-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"/><path id="MJX-844-TEX-S4-29" d="M33 1741Q33 1750 51 1750H60H65Q73 1750 81 1743T119 1700Q554 1207 554 251Q554 -707 119 -1199Q76 -1250 66 -1250Q65 -1250 62 -1250T56 -1249Q55 -1249 53 -1249T49 -1250Q33 -1250 33 -1239Q33 -1236 50 -1214T98 -1150T163 -1052T238 -910T311 -727Q443 -335 443 251Q443 402 436 532T405 831T339 1142T224 1438T50 1716Q33 1737 33 1741Z"/><path id="MJX-844-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-844-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(383, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="mi" transform="translate(566, 0)"><use xlink:href="#MJX-844-TEX-I-1D450"/></g></g><g data-mml-node="mrow" transform="translate(220, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g></g><rect width="1525" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2042.7, 0)"><use xlink:href="#MJX-844-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(3098.5, 0)"><use xlink:href="#MJX-844-TEX-N-2212"/></g><g data-mml-node="munderover" transform="translate(4043.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-844-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(124.5, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-844-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1190, 0)"><use xlink:href="#MJX-844-TEX-N-31"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D45A"/></g></g></g><g data-mml-node="mfrac" transform="translate(5653.8, 0)"><g data-mml-node="msub" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><rect width="1031.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(7147.4, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(7647.6, 0)"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(9741.7, 0)"><use xlink:href="#MJX-844-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(10797.5, 0)"><use xlink:href="#MJX-844-TEX-N-2212"/></g><g data-mml-node="mrow" transform="translate(11575.5, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-844-TEX-S4-28"/></g><g data-mml-node="mfrac" transform="translate(792, 0)"><g data-mml-node="msub" transform="translate(220, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><rect width="984" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2238.2, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(2738.4, 0)"><g data-mml-node="mrow" transform="translate(220, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g></g><g data-mml-node="mrow" transform="translate(222, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g></g><rect width="1529" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(4729.6, 0)"><use xlink:href="#MJX-844-TEX-N-2B"/></g><g data-mml-node="munderover" transform="translate(5729.8, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-844-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(179.3, -1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-844-TEX-N-2260"/></g><g data-mml-node="mi" transform="translate(1190, 0)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D45A"/></g></g></g><g data-mml-node="mfrac" transform="translate(7340.5, 0)"><g data-mml-node="msub" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><rect width="1031.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(8834, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="mfrac" transform="translate(9334.2, 0)"><g data-mml-node="mrow" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g></g><g data-mml-node="mrow" transform="translate(245.7, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-N-2202"/></g><g data-mml-node="msub" transform="translate(566, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D467"/></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g></g><rect width="1576.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(11150.6, 0)"><use xlink:href="#MJX-844-TEX-S4-29"/></g></g><g data-mml-node="mo" transform="translate(23795.8, 0)"><use xlink:href="#MJX-844-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(24851.6, 0)"><use xlink:href="#MJX-844-TEX-N-2212"/></g><g data-mml-node="mrow" transform="translate(25629.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-844-TEX-S4-28"/></g><g data-mml-node="mfrac" transform="translate(792, 0)"><g data-mml-node="msub" transform="translate(220, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><rect width="984" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(2238.2, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(2738.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(3501.3, 0)"><use xlink:href="#MJX-844-TEX-N-28"/></g><g data-mml-node="mn" transform="translate(3890.3, 0)"><use xlink:href="#MJX-844-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(4612.6, 0)"><use xlink:href="#MJX-844-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(5612.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(6375.7, 0)"><use xlink:href="#MJX-844-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(6987, 0)"><use xlink:href="#MJX-844-TEX-N-2B"/></g><g data-mml-node="munderover" transform="translate(7987.2, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-844-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(179.3, -1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-844-TEX-N-2260"/></g><g data-mml-node="mi" transform="translate(1190, 0)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D45A"/></g></g></g><g data-mml-node="mfrac" transform="translate(9597.9, 0)"><g data-mml-node="msub" transform="translate(220, 754.2)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><g data-mml-node="msub" transform="translate(230.5, -686)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><rect width="1031.3" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(11091.4, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="mo" transform="translate(11591.6, 0)"><use xlink:href="#MJX-844-TEX-N-28"/></g><g data-mml-node="mo" transform="translate(11980.6, 0)"><use xlink:href="#MJX-844-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(12758.6, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(13791.2, 0)"><use xlink:href="#MJX-844-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(14291.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-844-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-844-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(15054.4, 0)"><use xlink:href="#MJX-844-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(15443.4, 0)"><use xlink:href="#MJX-844-TEX-S4-29"/></g></g></g></g></svg></p><p><svg xmlns="http://www.w3.org/2000/svg" width="78.544ex" height="6.647ex" viewbox="0 -1562.5 34716.3 2937.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true"><defs><path id="MJX-4-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-4-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-4-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-4-TEX-I-1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/><path id="MJX-4-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-4-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-4-TEX-I-1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/><path id="MJX-4-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-4-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/><path id="MJX-4-TEX-LO-2211" d="M60 948Q63 950 665 950H1267L1325 815Q1384 677 1388 669H1348L1341 683Q1320 724 1285 761Q1235 809 1174 838T1033 881T882 898T699 902H574H543H251L259 891Q722 258 724 252Q725 250 724 246Q721 243 460 -56L196 -356Q196 -357 407 -357Q459 -357 548 -357T676 -358Q812 -358 896 -353T1063 -332T1204 -283T1307 -196Q1328 -170 1348 -124H1388Q1388 -125 1381 -145T1356 -210T1325 -294L1267 -449L666 -450Q64 -450 61 -448Q55 -446 55 -439Q55 -437 57 -433L590 177Q590 178 557 222T452 366T322 544L56 909L55 924Q55 945 60 948Z"/><path id="MJX-4-TEX-I-1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/><path id="MJX-4-TEX-N-2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"/><path id="MJX-4-TEX-I-1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-4-TEX-N-22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(1055.8, 0)"><use xlink:href="#MJX-4-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(1833.8, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(2617.7, 0)"><use xlink:href="#MJX-4-TEX-N-28"/></g><g data-mml-node="mn" transform="translate(3006.7, 0)"><use xlink:href="#MJX-4-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(3729, 0)"><use xlink:href="#MJX-4-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(4729.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(5492.1, 0)"><use xlink:href="#MJX-4-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(6103.3, 0)"><use xlink:href="#MJX-4-TEX-N-2B"/></g><g data-mml-node="munderover" transform="translate(7103.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(179.3, -1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-4-TEX-N-2260"/></g><g data-mml-node="mi" transform="translate(1190, 0)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45A"/></g></g></g><g data-mml-node="msub" transform="translate(8714.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(9767.8, 0)"><use xlink:href="#MJX-4-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(10268, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(11308.7, 0)"><use xlink:href="#MJX-4-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(12364.5, 0)"><use xlink:href="#MJX-4-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(13142.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(14148.7, 0)"><use xlink:href="#MJX-4-TEX-N-2B"/></g><g data-mml-node="msub" transform="translate(15148.9, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(16134.1, 0)"><use xlink:href="#MJX-4-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(16634.3, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(17640.5, 0)"><use xlink:href="#MJX-4-TEX-N-2B"/></g><g data-mml-node="munderover" transform="translate(18640.7, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(179.3, -1123.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-4-TEX-N-2260"/></g><g data-mml-node="mi" transform="translate(1190, 0)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45A"/></g></g></g><g data-mml-node="msub" transform="translate(20251.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(21304.9, 0)"><use xlink:href="#MJX-4-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(21805.1, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(22845.9, 0)"><use xlink:href="#MJX-4-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(23901.7, 0)"><use xlink:href="#MJX-4-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(24679.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(25685.8, 0)"><use xlink:href="#MJX-4-TEX-N-2B"/></g><g data-mml-node="munderover" transform="translate(26686, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-4-TEX-LO-2211"/></g><g data-mml-node="TeXAtom" transform="translate(124.5, -1087.9) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D457"/></g><g data-mml-node="mo" transform="translate(412, 0)"><use xlink:href="#MJX-4-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1190, 0)"><use xlink:href="#MJX-4-TEX-N-31"/></g></g><g data-mml-node="TeXAtom" transform="translate(411.6, 1150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D45A"/></g></g></g><g data-mml-node="msub" transform="translate(28296.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D457"/></g></g><g data-mml-node="mo" transform="translate(29350.3, 0)"><use xlink:href="#MJX-4-TEX-N-22C5"/></g><g data-mml-node="msub" transform="translate(29850.5, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(30891.2, 0)"><use xlink:href="#MJX-4-TEX-N-3D"/></g><g data-mml-node="msub" transform="translate(31947, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D460"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g><g data-mml-node="mo" transform="translate(32932.2, 0)"><use xlink:href="#MJX-4-TEX-N-2212"/></g><g data-mml-node="msub" transform="translate(33932.4, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-4-TEX-I-1D466"/></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><use xlink:href="#MJX-4-TEX-I-1D456"/></g></g></g></g></svg></p><p>最后，附一张图对wij求导的过程【<span style="background:#b3de4b">存在问题未解决，待补充</span>】</p><p><img src="./10-22.jpg"></p><br><h4 id="1-4-交叉熵损失优缺点"><a href="#1-4-交叉熵损失优缺点" class="headerlink" title="1.4 交叉熵损失优缺点"></a>1.4 交叉熵损失优缺点</h4><p>采用了类间竞争机制，比较擅长于学习类间的信息，但是只关心对于正确标签预测概率的准确性，而忽略了其他非正确标签的差异，从而导致学习到的特征比较散。</p><p>使用逻辑函数得到概率，并结合交叉熵当损失函数时，当模型效果差的时，学习速度较快，模型效果好时，学习速度会变慢。</p><br><h3 id="2、损失函数"><a href="#2、损失函数" class="headerlink" title="2、损失函数"></a>2、损失函数</h3><p><img src="./10-7.png"></p><p><img src="./10-8.png"></p><p><img src="./10-9.png"></p><p><img src="./10-10.png"></p><br><h3 id="3、图片分类数据集"><a href="#3、图片分类数据集" class="headerlink" title="3、图片分类数据集"></a>3、图片分类数据集</h3><blockquote><p><code>MNIST</code>数据集是约1986年提出，数据集过于简单，故此处使用<code>Fashion-MNIST</code></p><p>MNIST是一个手写体数字的图片数据集，该数据集来由美国国家标准与技术研究所（<code>National Institute of Standards and Technology (NIST)</code>）发起整理，一共统计了来自250个不同的人手写数字图片，其中50%是高中生，50%来自人口普查局的工作人员。该数据集的收集目的是希望通过算法，实现对手写数字的识别。</p><p>1998年，Yan LeCun 等人发表了论文<code>《Gradient-Based Learning Applied to Document Recognition》</code>，首次提出了LeNet-5 网络，利用上述数据集实现了手写字体的识别。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision                    <span class="comment">### pytorch对于计算机视觉实现的一个库</span></span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data     	<span class="comment">### 方便读取小批量数据</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment">### 对数据进行操作的model</span></span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">d2l.use_svg_display()                <span class="comment">### 要求使用svg来显示图片【清晰度高】</span></span><br></pre></td></tr></table></figure><p>通过<code>Torchvision</code>框架中的内置函数将 <code>Fashion-MNIST</code> 数据集下载并读取到内存中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过ToTensor实例，将图像数据从PIL类型转换为32位浮点数格式</span></span><br><span class="line"><span class="comment"># 并除以255使得所有像素均在0到1之间</span></span><br><span class="line"><span class="comment"># 可将数据提前下载放到目标文件夹，download可为false,以防止下载失败</span></span><br><span class="line"></span><br><span class="line">trans = transforms.ToTensor()   <span class="comment"># 预处理：将transform转为Tensor</span></span><br><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">True</span>,</span><br><span class="line">                                                transform=trans,</span><br><span class="line">                                                download=<span class="literal">True</span>)</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                               transform=trans, download=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">len</span>(mnist_train), <span class="built_in">len</span>(mnist_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Out: (<span class="number">60000</span>, <span class="number">10000</span>)</span><br><span class="line">    </span><br><span class="line">In:  mnist_train[<span class="number">0</span>][<span class="number">0</span>].shape <span class="comment"># 第一张图片的形状 (channel, height, width) </span></span><br><span class="line">Out: torch.Size([<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]) <span class="comment">#黑白图片，故通道channel数为1，长宽各为28</span></span><br><span class="line">    </span><br><span class="line">In:  <span class="built_in">len</span>(mnist_train[<span class="number">0</span>]) <span class="comment"># 是一个长度为2的元组 (1通道28×28的图片, label)</span></span><br><span class="line">Out: <span class="number">2</span></span><br></pre></td></tr></table></figure><blockquote><p><code>torchvision.datasets.FashionMNIST(...)</code>参数解读：【<a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/data/">参考文档</a>】</p><ul><li><code>root</code>：下载/读取路径</li><li><code>train</code>：下载的是否为训练数据</li><li><code>transform</code>：读取后的数据的类型</li><li><code>download</code>：是否下载</li></ul></blockquote><p>两个可视化数据集的函数【<code>get_fashion_mnist_labels</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回Fashion-MNIST数据集的文本标签。&quot;&quot;&quot;</span></span><br><span class="line">    text_labels = [</span><br><span class="line">        <span class="string">&#x27;t-shirt&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;coat&#x27;</span>, <span class="string">&#x27;sandal&#x27;</span>, <span class="string">&#x27;shirt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;sneaker&#x27;</span>, <span class="string">&#x27;bag&#x27;</span>, <span class="string">&#x27;ankle boot&#x27;</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">imgs, num_rows, num_cols, titles=<span class="literal">None</span>, scale=<span class="number">1.5</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;Plot a list of images.&quot;&quot;&quot;</span></span><br><span class="line">    figsize = (num_cols * scale, num_rows * scale)    <span class="comment">#(13.5, 3.0)</span></span><br><span class="line">    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)</span><br><span class="line">    axes = axes.flatten()  <span class="comment">#.flatten():维度展开函数</span></span><br><span class="line">    <span class="keyword">for</span> i, (ax, img) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(axes, imgs)):</span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(img):</span><br><span class="line">            <span class="comment">#图片张量</span></span><br><span class="line">            ax.imshow(img.numpy())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#PIL图片</span></span><br><span class="line">            ax.imshow(img)</span><br><span class="line">        ax.axes.get_xaxis().set_visible(<span class="literal">False</span>)   <span class="comment">#是否显示子图的x轴刻度</span></span><br><span class="line">        ax.axes.get_yaxis().set_visible(<span class="literal">False</span>)     <span class="comment">#是否显示子图的y轴刻度</span></span><br><span class="line">        <span class="keyword">if</span> titles:</span><br><span class="line">            ax.set_title(titles[i])</span><br><span class="line">    <span class="keyword">return</span> axes</span><br></pre></td></tr></table></figure><p>几个样本的图像及其相应的标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, y = <span class="built_in">next</span>(<span class="built_in">iter</span>(data.DataLoader(mnist_train, batch_size=<span class="number">18</span>)))</span><br><span class="line">show_images(X.reshape(<span class="number">18</span>, <span class="number">28</span>, <span class="number">28</span>), <span class="number">2</span>, <span class="number">9</span>, titles=get_fashion_mnist_labels(y))</span><br></pre></td></tr></table></figure><p><img src="./10-11.png"></p><blockquote><ul><li><code>data.DataLoader(mnist_train, batch_size=18)</code>：从<code>mnist_train</code>训练集中拿出<code>batch_size</code>大小的小批量</li><li><code>iter()</code>：使用该函数构造出一个<code>iterator</code></li><li><code>next()</code>：返回该迭代器的第一个元素</li></ul><p>对于<code>show_images()</code>函数：使用<code>matplotlib</code>画图，传入的<code>X</code>参数进行<code>reshape</code>，丢弃1维通道，分两行，每行9张图片打印，标题来自<code>y</code>标签</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X, y  _shape</span></span><br><span class="line">In:  <span class="built_in">print</span>(X.shape, y.shape)</span><br><span class="line">Out: torch.Size([<span class="number">18</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]) torch.Size([<span class="number">18</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在show_images()内部打印</span></span><br><span class="line"><span class="built_in">print</span>(figsize)</span><br><span class="line"><span class="built_in">print</span>(d2l.plt.subplots(num_rows, num_cols, figsize=figsize))</span><br><span class="line"></span><br><span class="line">Out: </span><br></pre></td></tr></table></figure><p><img src="./10-12.png"></p><p>对于<code>get_fashion_mnist_labels(labels)</code>函数：返回一个<code>List</code>，该<code>List</code>是遍历<code>labels</code>索引序列而转换得来的字符串序列，如此处：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In:  get_fashion_mnist_labels([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line">Out: [<span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;dress&#x27;</span>, <span class="string">&#x27;trouser&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>, <span class="string">&#x27;pullover&#x27;</span>]</span><br></pre></td></tr></table></figure><p>……</p></blockquote><p>读取一小批量数据，大小为<code>batch_size</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader_workers</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用4个进程来读取数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_iter = data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                             num_workers=get_dataloader_workers())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ▲▲▲通常需要去测试，模型读取数据的耗时【常见的性能瓶颈】</span></span><br><span class="line"><span class="comment"># 用于测试速度的函数</span></span><br><span class="line">timer = d2l.Timer()              </span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="keyword">continue</span></span><br><span class="line"><span class="string">f&#x27;<span class="subst">&#123;timer.stop():<span class="number">.2</span>f&#125;</span> sec&#x27;</span> </span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    <span class="string">&#x27;5.45 sec&#x27;</span></span><br></pre></td></tr></table></figure><p>定义 <code>load_data_fashion_mnist</code> 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># resize参数：是否需要将原28×28的图片变换大小</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_fashion_mnist</span>(<span class="params">batch_size, resize=<span class="literal">None</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;下载Fashion-MNIST数据集，然后将其加载到内存中。&quot;&quot;&quot;</span></span><br><span class="line">    trans = [transforms.ToTensor()]</span><br><span class="line">    <span class="keyword">if</span> resize: <span class="comment">#----------------------------------</span></span><br><span class="line">        trans.insert(<span class="number">0</span>, transforms.Resize(resize))</span><br><span class="line">    trans = transforms.Compose(trans)</span><br><span class="line">    mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>,</span><br><span class="line">                                                    train=<span class="literal">True</span>,</span><br><span class="line">                                                    transform=trans,</span><br><span class="line">                                                    download=<span class="literal">True</span>)</span><br><span class="line">    mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&quot;../data&quot;</span>,</span><br><span class="line">                                                   train=<span class="literal">False</span>,</span><br><span class="line">                                                   transform=trans,</span><br><span class="line">                                                   download=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> (data.DataLoader(mnist_train, batch_size, shuffle=<span class="literal">True</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()),</span><br><span class="line">            data.DataLoader(mnist_test, batch_size, shuffle=<span class="literal">False</span>,</span><br><span class="line">                            num_workers=get_dataloader_workers()))</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = load_data_fashion_mnist(<span class="number">32</span>, resize=<span class="number">64</span>)</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">    <span class="built_in">print</span>(X.shape, X.dtype, y.shape, y.dtype)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">32</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span>]) torch.float32 torch.Size([<span class="number">32</span>]) torch.int64</span><br></pre></td></tr></table></figure><br><h3 id="4、Softmax从零开始实现"><a href="#4、Softmax从零开始实现" class="headerlink" title="4、Softmax从零开始实现"></a>4、Softmax从零开始实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line"><span class="comment"># 训练集和测试集的迭代器</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>【<code>Softmax</code>回归的输入是一个向量】将展平每个图像，把它们看作长度为784的向量。 因为我们的数据集有10个类别，所以网络输出维度为 10【<strong>此操作会损失空间信息 | 卷积神经网络</strong>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># W初始化为一个高斯随机分布的值</span></span><br><span class="line">W = torch.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=(num_inputs, num_outputs), requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><blockquote><p><code>X</code>输入向量【Tensor】：<code>(1×784)</code>，<code>W</code>：<code>(784×10)</code>，<code>b</code>：<code>(1×10)</code>，</p><p>另外：回顾矩阵求和【给定矩阵X，对所有元素求和】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In:  XX = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>]])</span><br><span class="line">     <span class="built_in">print</span>(XX.shape)</span><br><span class="line">     XX.<span class="built_in">sum</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>), XX.<span class="built_in">sum</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>).shape, XX.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">Out: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">      (tensor([[<span class="number">5.</span>, <span class="number">7.</span>, <span class="number">9.</span>]]),</span><br><span class="line">     torch.Size([<span class="number">1</span>, <span class="number">3</span>]),</span><br><span class="line">        tensor([[ <span class="number">6.</span>],</span><br><span class="line">            [<span class="number">15.</span>]]))</span><br></pre></td></tr></table></figure></blockquote><p>实现<code>softmax</code>：<math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>X</mi><msub><mo stretchy="false">)</mo><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>k</mi></mrow></munder><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac></math>【原X为行向量，但此时X为矩阵，故此时是按每一行做<code>Softmax</code>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment">#第一维求和</span></span><br><span class="line">    <span class="keyword">return</span> X_exp / partition   <span class="comment">#▲▲使用的广播机制</span></span><br></pre></td></tr></table></figure><blockquote><p>测试该函数：【将每个元素变成一个非负数。此外，依据概率原理，每行总和为1】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">In:  X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">     X_prob = softmax(X)</span><br><span class="line">     X_prob, X_prob.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">Out：(tensor([[<span class="number">0.0599</span>, <span class="number">0.1886</span>, <span class="number">0.5760</span>, <span class="number">0.1060</span>, <span class="number">0.0695</span>],</span><br><span class="line">         [<span class="number">0.3192</span>, <span class="number">0.2758</span>, <span class="number">0.0286</span>, <span class="number">0.0575</span>, <span class="number">0.3189</span>]]),</span><br><span class="line">       tensor([<span class="number">1.0000</span>, <span class="number">1.0000</span>]))</span><br></pre></td></tr></table></figure></blockquote><p>实现softmax回归模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(torch.matmul(X.reshape(-<span class="number">1</span>, W.shape[<span class="number">0</span>]), W) + b)</span><br></pre></td></tr></table></figure><blockquote><p><code>X</code>的<code>shape</code>：<code>torch.Size(256, 1, 28, 28])</code>；<code>y.shape = torch.Size([256])</code></p><p><code>X.reshape(-1, W.shape[0])</code>：<code>-1</code>即未知，通过第二个参数计算得到。其中，<code>W.shape</code>为<code>784×10</code>，<code>W.shape[0]=784</code>。故<code>X.reshape(-1, W.shape[0])</code>后，<code>X</code>形状为<code>256×784</code></p><p><code>matmul()</code>为矩阵连乘：<code>[256，784] × [784，10] = [256，10]</code></p></blockquote><p>以下为举例，如何进行【拿取真实类别，在预测值中对应预测的概率】</p><blockquote><p>创建一个数据<code>y_hat</code>，其中包含2个样本在3个类别的预测概率， 使用<code>y</code>作为<code>y_hat</code>中概率的索引</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个张量，表示两个真实的标号；# y_hat是预测值，此处为两个样本，三个类别</span></span><br><span class="line">y = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">y_hat = torch.tensor([[<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.5</span>]])</span><br><span class="line"></span><br><span class="line">y_hat[[<span class="number">0</span>, <span class="number">1</span>], y]     <span class="comment"># 取出真实值对应在预测值中的概率</span></span><br><span class="line">    </span><br><span class="line">Out: tensor([<span class="number">0.1000</span>, <span class="number">0.5000</span>])</span><br></pre></td></tr></table></figure></blockquote><p>实现交叉熵损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cross_entropy</span></span><br><span class="line"><span class="comment"># range(n): 生成0~n-1的向量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat, y</span>):</span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_hat[<span class="built_in">range</span>(<span class="built_in">len</span>(y_hat)), y])</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">In:  cross_entropy(y_hat, y)</span><br><span class="line">Out: tensor([<span class="number">2.3026</span>, <span class="number">0.6931</span>])</span><br></pre></td></tr></table></figure><p>将预测类别与真实 <code>y</code> 元素进行比较</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat, y</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算预测正确的数量。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(y_hat.shape) &gt; <span class="number">1</span> <span class="keyword">and</span> y_hat.shape[<span class="number">1</span>] &gt; <span class="number">1</span>: <span class="comment">#若y_hat为二维矩阵，且列数&gt;1</span></span><br><span class="line">        y_hat = y_hat.argmax(axis=<span class="number">1</span>)     <span class="comment">#对每一行求argmax,将最大值的下标存储在y_hat中</span></span><br><span class="line">    cmp = y_hat.<span class="built_in">type</span>(y.dtype) == y        <span class="comment">#数据类型可能不同，比较后cmp为bool类型</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(cmp.<span class="built_in">type</span>(y.dtype).<span class="built_in">sum</span>())    <span class="comment">#cmp转为y的类型，再求和，此时为正确总数</span></span><br><span class="line"></span><br><span class="line">accuracy(y_hat, y) / <span class="built_in">len</span>(y)</span><br><span class="line"></span><br><span class="line">Out: <span class="number">0.5</span></span><br></pre></td></tr></table></figure><p>我们可以评估在任意模型 <code>net</code> 的准确率【正确预测数 / 预测总数】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算在指定数据集上模型的精度。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()    <span class="comment">#将模型设置为评估模式【好习惯 | 在该模式下，不计算梯度】</span></span><br><span class="line">    metric = Accumulator(<span class="number">2</span>)        <span class="comment">#【累加器】正确预测数、预测总数</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        metric.add(accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><code>Accumulator</code> 实例中创建了 2 个变量，用于分别存储正确预测的数量和预测的总数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Accumulator 的实现</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Accumulator</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;在`n`个变量上累加。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = [<span class="number">0.0</span>] * n    <span class="comment">#[0.0, 0.0]</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = [a + <span class="built_in">float</span>(b) <span class="keyword">for</span> a, b <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.data, args)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reset</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.data = [<span class="number">0.0</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment">#随机参数的正确率</span></span><br><span class="line">evaluate_accuracy(net, test_iter)</span><br><span class="line"></span><br><span class="line">Out: <span class="number">0.0307</span></span><br></pre></td></tr></table></figure><p>Softmax回归的训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch_ch3</span>(<span class="params">net, train_iter, loss, updater</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型一个迭代周期（定义见第3章）。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):    <span class="comment">#是否使用nn模型</span></span><br><span class="line">        net.train()                <span class="comment">#要计算梯度</span></span><br><span class="line">    metric = Accumulator(<span class="number">3</span>)        <span class="comment">#长度为3的累加器</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">        y_hat = net(X)</span><br><span class="line">        l = loss(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(updater, torch.optim.Optimizer):    <span class="comment">#如果更新器是来自pytorch</span></span><br><span class="line">            updater.zero_grad()        <span class="comment">#清空梯度</span></span><br><span class="line">            l.backward()            <span class="comment">#计算梯度</span></span><br><span class="line">            updater.step()            <span class="comment">#更新参数</span></span><br><span class="line">            metric.add(</span><br><span class="line">                <span class="built_in">float</span>(l) * <span class="built_in">len</span>(y), accuracy(y_hat, y),</span><br><span class="line">                y.size().numel())    <span class="comment">#累加（总损失，预测正确数，样本总数）</span></span><br><span class="line">        <span class="keyword">else</span>:    <span class="comment">#从零实现时，loss是一个向量</span></span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            updater(X.shape[<span class="number">0</span>])        <span class="comment">#根据批量大小更新</span></span><br><span class="line">            metric.add(<span class="built_in">float</span>(l.<span class="built_in">sum</span>()), accuracy(y_hat, y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">2</span>], metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">    <span class="comment">#loss总合/总样本数   正确样本数/总样本数</span></span><br></pre></td></tr></table></figure><p>定义一个在动画中绘制数据的实用程序类【辅助函数：可视化训练过程】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Animator</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;在动画中绘制数据。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">                 fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), nrows=<span class="number">1</span>, ncols=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span><br><span class="line">        <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            legend = []</span><br><span class="line">        d2l.use_svg_display()</span><br><span class="line">        <span class="variable language_">self</span>.fig, <span class="variable language_">self</span>.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)</span><br><span class="line">        <span class="keyword">if</span> nrows * ncols == <span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.axes = [<span class="variable language_">self</span>.axes,]</span><br><span class="line">        <span class="variable language_">self</span>.config_axes = <span class="keyword">lambda</span>: d2l.set_axes(<span class="variable language_">self</span>.axes[</span><br><span class="line">            <span class="number">0</span>], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br><span class="line">        <span class="variable language_">self</span>.X, <span class="variable language_">self</span>.Y, <span class="variable language_">self</span>.fmts = <span class="literal">None</span>, <span class="literal">None</span>, fmts</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(y, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            y = [y]</span><br><span class="line">        n = <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(x, <span class="string">&quot;__len__&quot;</span>):</span><br><span class="line">            x = [x] * n</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.X:</span><br><span class="line">            <span class="variable language_">self</span>.X = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.Y:</span><br><span class="line">            <span class="variable language_">self</span>.Y = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">        <span class="keyword">for</span> i, (a, b) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(x, y)):</span><br><span class="line">            <span class="keyword">if</span> a <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> b <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                <span class="variable language_">self</span>.X[i].append(a)</span><br><span class="line">                <span class="variable language_">self</span>.Y[i].append(b)</span><br><span class="line">        <span class="variable language_">self</span>.axes[<span class="number">0</span>].cla()</span><br><span class="line">        <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.X, <span class="variable language_">self</span>.Y, <span class="variable language_">self</span>.fmts):</span><br><span class="line">            <span class="variable language_">self</span>.axes[<span class="number">0</span>].plot(x, y, fmt)</span><br><span class="line">        <span class="variable language_">self</span>.config_axes()</span><br><span class="line">        display.display(<span class="variable language_">self</span>.fig)</span><br><span class="line">        display.clear_output(wait=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>训练函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch3</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, updater</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;训练模型（定义见第3章）。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#可视化</span></span><br><span class="line">    animator = Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">0.3</span>, <span class="number">0.9</span>],</span><br><span class="line">                        legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    <span class="comment">#扫n遍数据</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, train_metrics + (test_acc,))</span><br><span class="line">    train_loss, train_acc = train_metrics</span><br><span class="line">    <span class="keyword">assert</span> train_loss &lt; <span class="number">0.5</span>, train_loss</span><br><span class="line">    <span class="keyword">assert</span> train_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> train_acc &gt; <span class="number">0.7</span>, train_acc</span><br><span class="line">    <span class="keyword">assert</span> test_acc &lt;= <span class="number">1</span> <span class="keyword">and</span> test_acc &gt; <span class="number">0.7</span>, test_acc</span><br></pre></td></tr></table></figure><p>小批量随机梯度下降来优化模型的损失函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updater</span>(<span class="params">batch_size</span>):</span><br><span class="line">    <span class="keyword">return</span> d2l.sgd([W, b], lr, batch_size)</span><br></pre></td></tr></table></figure><p>训练模型10个迭代周期</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)</span><br></pre></td></tr></table></figure><p><img src="./10-13.png"></p><blockquote><p><strong>异常</strong>：有可能 <code>train_loss</code>不显示，根据<code>log</code>可知是<code>loss</code>太小了，无法显示</p></blockquote><p>对图像进行分类预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ch3</span>(<span class="params">net, test_iter, n=<span class="number">6</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;预测标签（定义见第3章）。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> test_iter:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    trues = d2l.get_fashion_mnist_labels(y)</span><br><span class="line">    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=<span class="number">1</span>))</span><br><span class="line">    titles = [true + <span class="string">&#x27;\n&#x27;</span> + pred <span class="keyword">for</span> true, pred <span class="keyword">in</span> <span class="built_in">zip</span>(trues, preds)]</span><br><span class="line">    d2l.show_images(X[<span class="number">0</span>:n].reshape((n, <span class="number">28</span>, <span class="number">28</span>)), <span class="number">1</span>, n, titles=titles[<span class="number">0</span>:n])</span><br><span class="line"></span><br><span class="line">predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><p><img src="./10-14.png"></p><br><h3 id="5、Softmax简洁实现"><a href="#5、Softmax简洁实现" class="headerlink" title="5、Softmax简洁实现"></a>5、Softmax简洁实现</h3><p>通过深度学习框架的高级API能够使实现 softmax 回归变得更加容易</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>Softmax 回归的输出层是一个全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pytorch不会隐式地调整输入的形状</span></span><br><span class="line"><span class="comment">#因此我们定义展平层（flatten），在线性层前调整网络输入的形状</span></span><br><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:    <span class="comment">#如果是线性层</span></span><br><span class="line">        <span class="comment">#将权重init成一个均值为0，默认为0，方差0.01的随机值</span></span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>在交叉熵损失函数中传递未归一化的预测，并同时计算softmax及其对数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><p>使用学习率为0.1的小批量随机梯度下降作为优化算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>调用 之前 定义的训练函数来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">10</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="./10-15.png"></p><br><h3 id="6、Softmax回归-Q-amp-A"><a href="#6、Softmax回归-Q-amp-A" class="headerlink" title="6、Softmax回归 Q&amp;A"></a>6、Softmax回归 Q&amp;A</h3><h4 id="6-1-Softlabel的训练策略？为什么有效？"><a href="#6-1-Softlabel的训练策略？为什么有效？" class="headerlink" title="6.1 Softlabel的训练策略？为什么有效？"></a>6.1 <code>Softlabel</code>的训练策略？为什么有效？</h4><p><code>Soft label</code>与<code>Hard Label</code>对应，先参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_29788741/article/details/128072190">此文章</a>，该文章来从<strong>标签平滑</strong>和<strong>知识蒸馏</strong>理解，探讨一下<code>hard label</code>和<code>soft label</code>之间的关系。</p><p>深度学习领域中，通常将数据标注为<code>hard label</code>，但事实上同一个数据包含不同类别的信息，直接标注为<code>hard label</code>会导致大量信息的损失，进而影响最终模型取得的效果。</p><p>【<span style="background:#b3de4b">挖一个坑，此处不太理解，沐神说</span>：<code>Hard-Label</code>的问题在于，使用指数来不断逼近真实值的0或1，需要输出为无穷大，其他的输出很小，这显然难度很大；因此，提出对正确类赋为<code>0.9</code>，其他的为<code>0.1/n</code>，这样的情况下，是可以用<code>Softmax</code>来逼近0.9或0.1的】</p><p><img src="./10-16.png"></p><h4 id="6-2-Softmax回归与Logistic回归？"><a href="#6-2-Softmax回归与Logistic回归？" class="headerlink" title="6.2 Softmax回归与Logistic回归？"></a>6.2 <code>Softmax</code>回归与<code>Logistic</code>回归？</h4><p><code>Logistic Regression</code>是针对二分类问题，是Softmax回归的特例，但实际情况下都是多分类问题，故跳过。</p><p>另外，对于二分类的预测，只需要对1或0进行预测，另一个必然是<code>（1-预测值）</code></p><h4 id="6-3-为什么用交叉熵作为损失函数？相对熵、互信息？"><a href="#6-3-为什么用交叉熵作为损失函数？相对熵、互信息？" class="headerlink" title="6.3 为什么用交叉熵作为损失函数？相对熵、互信息？"></a>6.3 为什么用交叉熵作为损失函数？相对熵、互信息？</h4><p>交叉熵简单；相互熵好处在于<code>H(p, q) = H(q, p)</code>；互信息不好算。总之，我们只关心两个概率之间的距离。</p><h4 id="6-4-这样的N分类问题，只认为有一个正类、n-1个负类，会不会不平衡？"><a href="#6-4-这样的N分类问题，只认为有一个正类、n-1个负类，会不会不平衡？" class="headerlink" title="6.4 这样的N分类问题，只认为有一个正类、n-1个负类，会不会不平衡？"></a>6.4 这样的N分类问题，只认为有一个正类、n-1个负类，会不会不平衡？</h4><p>会有这样的情况。如果用one-hot编码，其实并不关心负类会怎么样。但是，<strong>需要注意的是</strong>，如果某一个类别的样本过少，必然会导致不平衡，即模型不准确</p><h4 id="6-5-MSE的最大似然函数？参考意义？"><a href="#6-5-MSE的最大似然函数？参考意义？" class="headerlink" title="6.5 MSE的最大似然函数？参考意义？"></a>6.5 MSE的最大似然函数？参考意义？</h4><p>这是统计学的概念，意义在于可以解释模型为什么好。</p><p>其实，最小化损失，就等价于最大化似然函数【似然函数，就说是对于一个模型，即参数权重，使得模型的预测值等于真实值的概率最大】【<span style="background:#b3de4b">此处有个最大似然的坑需要填</span>】</p><h4 id="6-6-梯度下降的速度和学习的率的关系"><a href="#6-6-梯度下降的速度和学习的率的关系" class="headerlink" title="6.6 梯度下降的速度和学习的率的关系"></a>6.6 梯度下降的速度和学习的率的关系</h4><p>对于权重<code>w/b</code>的更新有两项，一项是负梯度【不同时的损失函数，其梯度不同】，另一项是学习率【人为控制】。</p><h4 id="6-7-Little-Problems"><a href="#6-7-Little-Problems" class="headerlink" title="6.7 Little Problems"></a>6.7 Little Problems</h4><ul><li><p>如果数据下载出现<code>HTTP Error</code>的问题【从pytorch官网上自动下载】：官网上有如何进行手动下载的教程【<a target="_blank" rel="noopener" href="https://paperswithcode.com/sota">Browse State-of-the-Art ：数据集下载</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/GODXML/article/details/125065304">手动数据集下载教程</a>】</p></li><li><p><code>data.DataLoader</code>中的<code>num_workers</code>?</p><p>该参数标志使用多少进程并行执行</p></li><li><p>关于自己构建数据集，如何用pytorch读入？</p><p>【沐神说，假如十个类，猫的图片在一个文件夹中，狗的图片在一个文件夹中，分好类，这样是可以被<code>pytorch</code>读入的】【参考<a target="_blank" rel="noopener" href="https://pytorch.apachecn.org/2.0/tutorials/Introduction_to_PyTorch/data_tutorial/#_4">社区中文文档</a>】</p></li></ul><br><h2 id="十一、多层感知机"><a href="#十一、多层感知机" class="headerlink" title="十一、多层感知机"></a>十一、多层感知机</h2><h3 id="1、感知机"><a href="#1、感知机" class="headerlink" title="1、感知机"></a>1、感知机</h3><blockquote><p>感知机 、训练感知机</p><p>收敛定理、XOR问题</p></blockquote><p><img src="./11-1.png"></p><p><img src="./11-2.png"></p><p><img src="./11-3.png"></p><br><h3 id="2、多层感知机"><a href="#2、多层感知机" class="headerlink" title="2、多层感知机"></a>2、多层感知机</h3><blockquote><p>【<code>Multilayer Perceptron</code>】学习XOR问题、单隐藏层-单分类、多类分类、多隐藏层</p></blockquote><p><img src="./11-4.png"></p><p><strong>总结</strong></p><ul><li>多层感知机使用隐藏层和激活函数来得到非线性模型</li><li>常用的激活函数是<code>Sigmoid</code> | <code>Tanh</code> | <code>ReLU</code></li><li>使用Softmax来处理多类分类</li><li>超参数：隐藏层数、各隐藏层大小</li><li>知道如何结合非线性函数来构建具有更强表达能力的多层神经网络结构，但是具体在哪里使用哪些函数要积累经验</li></ul><br><h3 id="3、常见的激活函数"><a href="#3、常见的激活函数" class="headerlink" title="3、常见的激活函数"></a>3、常见的激活函数</h3><p><img src="./11-5.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>ReLU提供了一种非常简单的非线性变换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(-<span class="number">8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.relu(x)</span><br><span class="line">d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;relu(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 PyTorch 中，detach() 方法用于返回一个新的 Tensor</span></span><br><span class="line"><span class="comment"># 这个 Tensor 和原来的 Tensor 共享相同的内存空间，但是不会被计算图所追踪</span></span><br><span class="line"><span class="comment"># 也就是说它不会参与反向传播，不会影响到原有的计算图，这使得它成为处理中间结果的一种有效方式</span></span><br></pre></td></tr></table></figure><blockquote><p>🔺 画图的时候<code>tesnsor</code>类不能做参数，要用<code>.detach()</code></p><p>🔺<code>plt.figure</code>设置幕布：<code>fig = plt.figure(figsize=(w, h), dpi=dpi)</code>。【缺省时，<code>w=6.4</code>, <code>h=4.8</code>；如果<code>dpi=100</code>, 则图片属性中的分辨率为<code>640*480</code>】【<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/545234640">参考文章</a>】</p></blockquote><p><img src="./11-6.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line">d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of relu&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure><p><img src="./11-7.png"></p><p>对于一个定义域在R中的输入，<em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.sigmoid(x)</span><br><span class="line">d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;sigmoid(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure><p><img src="./11-8.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.data.zero_()</span><br><span class="line">y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line">d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of sigmoid&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure><p><img src="./11-9.png"></p><p>Tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.tanh(x)</span><br><span class="line">d2l.plot(x.detach(), y.detach(), <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;tanh(x)&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure><p><img src="./11-10.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.data.zero_()</span><br><span class="line">y.backward(torch.ones_like(x), retain_graph=<span class="literal">True</span>)</span><br><span class="line">d2l.plot(x.detach(), x.grad, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;grad of tanh&#x27;</span>, figsize=(<span class="number">5</span>, <span class="number">2.5</span>))</span><br></pre></td></tr></table></figure><p><img src="./11-11.png"></p><br><h3 id="4、多层感知机代码实现"><a href="#4、多层感知机代码实现" class="headerlink" title="4、多层感知机代码实现"></a>4、多层感知机代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>实现一个具有单隐藏层的多层感知机，它包含256个隐藏单元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 784×256【即XW】隐藏层【X数据均为Tensor行向量，即X:batch_size ×784】</span></span><br><span class="line">W1 = nn.Parameter(</span><br><span class="line">    torch.randn(num_inputs, num_hiddens, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># 256×1【256行】【用到了广播机制】</span></span><br><span class="line">b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 256×10【即XW】输出层【隐藏层输出为256×256】【输出层输出为256×10】</span></span><br><span class="line">W2 = nn.Parameter(</span><br><span class="line">    torch.randn(num_hiddens, num_outputs, requires_grad=<span class="literal">True</span>) * <span class="number">0.01</span>)</span><br><span class="line">b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br></pre></td></tr></table></figure><blockquote><p>🔺当参数W1随机为0时，似乎无影响；当W1W2均为0时，<code>d2l.train_ch3</code>函数中的<code>assert</code>断言错误</p></blockquote><p>实现ReLU激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">relu</span>(<span class="params">X</span>):</span><br><span class="line">    a = torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(X, a)</span><br></pre></td></tr></table></figure><p>实现我们的模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(X @ W1 + b1)</span><br><span class="line">    <span class="keyword">return</span> (H @ W2 + b2)</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><blockquote><p><code>@</code>：矩阵乘法，即<code>torch.matmul(x，y)</code></p></blockquote><p>多层感知机的训练过程与softmax回归的训练过程完全相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">10</span>, <span class="number">0.1</span></span><br><span class="line">updater = torch.optim.SGD(params, lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)</span><br></pre></td></tr></table></figure><p><img src="./11-12.png"></p><p>在一些测试数据上应用这个模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d2l.predict_ch3(net, test_iter)</span><br></pre></td></tr></table></figure><p><img src="./11-13.png"></p><br><h3 id="5、多层感知机简单实现"><a href="#5、多层感知机简单实现" class="headerlink" title="5、多层感知机简单实现"></a>5、多层感知机简单实现</h3><p>通过高级API更简洁地实现多层感知机</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>隐藏层 包含256个隐藏单元，并使用了ReLU激活函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU(),</span><br><span class="line">                    nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure><p>训练过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size, lr, num_epochs = <span class="number">256</span>, <span class="number">0.1</span>, <span class="number">10</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line"></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="./11-14.png"></p><br><h3 id="6、Q-amp-A"><a href="#6、Q-amp-A" class="headerlink" title="6、Q&amp;A"></a>6、Q&amp;A</h3><h4 id="6-1-Little-Problems"><a href="#6-1-Little-Problems" class="headerlink" title="6.1 Little Problems"></a>6.1 Little Problems</h4><ul><li>通常神经网络中的一层是指带权重的一层，即包含激活函数【每一层都有一批可学习的参数】</li><li>收敛定理中的<code>ρ</code>和<code>r</code>统计学概念，机器学习关注的计算，想算出<code>ρ</code>和<code>r</code>是很难的</li><li>SVM替代了多层感知机【SVM对超参数不敏感；SVM优化较容易；二者效果差不多，但SVM使用较简单、数学基础好。此处不介绍SVM，是因为SVM要换一套优化函数不使用sgd】</li><li>理论上，一层【隐藏层+激活函数】可以拟合任何函数，但实际上优化算法无法求解。</li><li>最好不要是动态的神经网络，最好保证模型的稳定</li></ul><h4 id="6-2-为什么要增加隐藏层层数，而不是增加神经元个数？"><a href="#6-2-为什么要增加隐藏层层数，而不是增加神经元个数？" class="headerlink" title="6.2 为什么要增加隐藏层层数，而不是增加神经元个数？"></a>6.2 为什么要增加隐藏层层数，而不是增加神经元个数？</h4><p>两种做法：一种是瘦子【层数多，节点少】，一种是胖子【层数少，节点多】，这两种的模型复杂度差不多。</p><p>但是，胖子不好训练！胖子，俗称 <strong>浅度学习</strong>；瘦子，俗称 <strong>深度学习</strong>【训练更容易】。</p><p>因为，胖子学习<strong>极其容易过拟合</strong>【协调大量神经元，一次吃个大胖子】</p><h4 id="6-3-ReLU为什么有用？激活函数的本质是什么？"><a href="#6-3-ReLU为什么有用？激活函数的本质是什么？" class="headerlink" title="6.3 ReLU为什么有用？激活函数的本质是什么？"></a>6.3 ReLU为什么有用？激活函数的本质是什么？</h4><p>ReLU函数是一个分段线性函数，即非线性函数【线性函数是一根线】。激活函数的本质是引入非线性性，而已！</p><p>激活函数的选择，远远没有选择超参数大小来的重要！</p><br><h2 id="十二、模型选择与过-欠拟合"><a href="#十二、模型选择与过-欠拟合" class="headerlink" title="十二、模型选择与过/欠拟合"></a>十二、模型选择与过/欠拟合</h2><h3 id="1、模型选择"><a href="#1、模型选择" class="headerlink" title="1、模型选择"></a>1、模型选择</h3><blockquote><p>训练误差、泛化误差</p><p>验证数据集、测试数据集</p><p>K-则交叉验证</p></blockquote><p><img src="./12-1.png"></p><br><h3 id="2、过拟合和欠拟合"><a href="#2、过拟合和欠拟合" class="headerlink" title="2、过拟合和欠拟合"></a>2、过拟合和欠拟合</h3><h4 id="模型容量及影响"><a href="#模型容量及影响" class="headerlink" title="模型容量及影响"></a>模型容量及影响</h4><p><img src="./12-2-1.png"></p><h4 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h4><ul><li>统计学习理论的一个核心思想【可以理解为，能完美记住该数据集的模型的模型复杂度】</li><li>对于一个分类模型，VC等于一个最大的数据集大小，不管如何给定标号，都存在一个模型来对他们进行完美分类</li></ul><h4 id="VC维的用处"><a href="#VC维的用处" class="headerlink" title="VC维的用处"></a>VC维的用处</h4><ul><li>提供为什么一个模型好的理论依据<ul><li>它可以衡量<strong>训练误差</strong>和<strong>泛化误差</strong>之间的间隔</li></ul></li><li>但深度学习中很少使用<ul><li>衡量不是很准确</li><li>计算深度学习模型的VC很困难</li></ul></li></ul><h4 id="线性分类器的VC维"><a href="#线性分类器的VC维" class="headerlink" title="线性分类器的VC维"></a>线性分类器的VC维</h4><ul><li>2维输入的感知机，VC维 = 3【能够分类任何三个点，但不能是4个(XOR问题)】</li></ul><p><img src="./12-2-2.png"></p><ul><li>支持N维输入的感知机的VC维是N+1</li><li>一些多层感知机的VC维是O(Nlog2N)</li></ul><h4 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h4><p>多个重要因素</p><ul><li>样本个数</li><li>每个样本的元素个数</li><li>时间、空间结构【时间：如股票预测；时空：如视频】</li><li>多样性</li></ul><h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><ul><li>模型容量需要<strong>匹配数据复杂度</strong>，否则可能会导致欠拟合或过拟合</li><li>统计机器学习提供数学工具来衡量模型复杂度</li><li>实际中一般靠观察<strong>训练误差</strong>和<strong>验证误差</strong></li></ul><br><h3 id="3、代码理解"><a href="#3、代码理解" class="headerlink" title="3、代码理解"></a>3、代码理解</h3><p>通过多项式拟合来交互地探索这些概念</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>使用以下三阶多项式来生成训练和测试数据的标签【以下先生成人工数据集】：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="56.704ex" height="5.018ex" viewbox="0 -1509.9 25063.2 2217.9" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-182-TEX-I-1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-182-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-182-TEX-N-35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z"/><path id="MJX-182-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/><path id="MJX-182-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-182-TEX-N-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/><path id="MJX-182-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/><path id="MJX-182-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/><path id="MJX-182-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-182-TEX-N-33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/><path id="MJX-182-TEX-N-34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"/><path id="MJX-182-TEX-N-21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z"/><path id="MJX-182-TEX-N-36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"/><path id="MJX-182-TEX-I-1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/><path id="MJX-182-TEX-I-1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/><path id="MJX-182-TEX-I-210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"/><path id="MJX-182-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/><path id="MJX-182-TEX-I-1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/><path id="MJX-182-TEX-N-223C" d="M55 166Q55 241 101 304T222 367Q260 367 296 349T362 304T421 252T484 208T554 189Q616 189 655 236T694 338Q694 350 698 358T708 367Q722 367 722 334Q722 260 677 197T562 134H554Q517 134 481 152T414 196T355 248T292 293T223 311Q179 311 145 286Q109 257 96 218T80 156T69 133Q55 133 55 166Z"/><path id="MJX-182-TEX-I-1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"/><path id="MJX-182-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-182-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/><path id="MJX-182-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/><path id="MJX-182-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-182-TEX-I-1D466"/></g><g data-mml-node="mo" transform="translate(767.8, 0)"><use xlink:href="#MJX-182-TEX-N-3D"/></g><g data-mml-node="mn" transform="translate(1823.6, 0)"><use xlink:href="#MJX-182-TEX-N-35"/></g><g data-mml-node="mo" transform="translate(2545.8, 0)"><use xlink:href="#MJX-182-TEX-N-2B"/></g><g data-mml-node="mn" transform="translate(3546, 0)"><use xlink:href="#MJX-182-TEX-N-31"/><use xlink:href="#MJX-182-TEX-N-2E" transform="translate(500, 0)"/><use xlink:href="#MJX-182-TEX-N-32" transform="translate(778, 0)"/></g><g data-mml-node="mi" transform="translate(4824, 0)"><use xlink:href="#MJX-182-TEX-I-1D465"/></g><g data-mml-node="mo" transform="translate(5618.2, 0)"><use xlink:href="#MJX-182-TEX-N-2212"/></g><g data-mml-node="mn" transform="translate(6618.4, 0)"><use xlink:href="#MJX-182-TEX-N-33"/><use xlink:href="#MJX-182-TEX-N-2E" transform="translate(500, 0)"/><use xlink:href="#MJX-182-TEX-N-34" transform="translate(778, 0)"/></g><g data-mml-node="mfrac" transform="translate(7896.4, 0)"><g data-mml-node="msup" transform="translate(220, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-182-TEX-I-1D465"/></g><g data-mml-node="mn" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-182-TEX-N-32"/></g></g><g data-mml-node="mrow" transform="translate(318.8, -686)"><g data-mml-node="mn"><use xlink:href="#MJX-182-TEX-N-32"/></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-182-TEX-N-21"/></g></g><rect width="1175.6" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(9534.2, 0)"><use xlink:href="#MJX-182-TEX-N-2B"/></g><g data-mml-node="mn" transform="translate(10534.4, 0)"><use xlink:href="#MJX-182-TEX-N-35"/><use xlink:href="#MJX-182-TEX-N-2E" transform="translate(500, 0)"/><use xlink:href="#MJX-182-TEX-N-36" transform="translate(778, 0)"/></g><g data-mml-node="mfrac" transform="translate(11812.4, 0)"><g data-mml-node="msup" transform="translate(220, 676)"><g data-mml-node="mi"><use xlink:href="#MJX-182-TEX-I-1D465"/></g><g data-mml-node="mn" transform="translate(572, 363) scale(0.707)"><use xlink:href="#MJX-182-TEX-N-33"/></g></g><g data-mml-node="mrow" transform="translate(318.8, -686)"><g data-mml-node="mn"><use xlink:href="#MJX-182-TEX-N-33"/></g><g data-mml-node="mo" transform="translate(500, 0)"><use xlink:href="#MJX-182-TEX-N-21"/></g></g><rect width="1175.6" height="60" x="120" y="220"/></g><g data-mml-node="mo" transform="translate(13450.2, 0)"><use xlink:href="#MJX-182-TEX-N-2B"/></g><g data-mml-node="mi" transform="translate(14450.4, 0)"><use xlink:href="#MJX-182-TEX-I-1D716"/></g><g data-mml-node="mstyle" transform="translate(14856.4, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(15023.1, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(15189.8, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(15356.4, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(15523.1, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(15689.8, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mi" transform="translate(15856.4, 0)"><use xlink:href="#MJX-182-TEX-I-1D464"/></g><g data-mml-node="mi" transform="translate(16572.4, 0)"><use xlink:href="#MJX-182-TEX-I-210E"/></g><g data-mml-node="mi" transform="translate(17148.4, 0)"><use xlink:href="#MJX-182-TEX-I-1D452"/></g><g data-mml-node="mi" transform="translate(17614.4, 0)"><use xlink:href="#MJX-182-TEX-I-1D45F"/></g><g data-mml-node="mi" transform="translate(18065.4, 0)"><use xlink:href="#MJX-182-TEX-I-1D452"/></g><g data-mml-node="mstyle" transform="translate(18531.4, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mstyle" transform="translate(18698.1, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mi" transform="translate(18864.8, 0)"><use xlink:href="#MJX-182-TEX-I-1D716"/></g><g data-mml-node="mstyle" transform="translate(19270.8, 0)"><g data-mml-node="mspace"/></g><g data-mml-node="mo" transform="translate(19715.2, 0)"><use xlink:href="#MJX-182-TEX-N-223C"/></g><g data-mml-node="mi" transform="translate(20771, 0)"><use xlink:href="#MJX-182-TEX-I-1D441"/></g><g data-mml-node="mo" transform="translate(21659, 0)"><use xlink:href="#MJX-182-TEX-N-28"/></g><g data-mml-node="mn" transform="translate(22048, 0)"><use xlink:href="#MJX-182-TEX-N-30"/></g><g data-mml-node="mo" transform="translate(22548, 0)"><use xlink:href="#MJX-182-TEX-N-2C"/></g><g data-mml-node="msup" transform="translate(22992.7, 0)"><g data-mml-node="mn"><use xlink:href="#MJX-182-TEX-N-30"/><use xlink:href="#MJX-182-TEX-N-2E" transform="translate(500, 0)"/><use xlink:href="#MJX-182-TEX-N-31" transform="translate(778, 0)"/></g><g data-mml-node="mn" transform="translate(1278, 413) scale(0.707)"><use xlink:href="#MJX-182-TEX-N-32"/></g></g><g data-mml-node="mo" transform="translate(24674.2, 0)"><use xlink:href="#MJX-182-TEX-N-29"/></g></g></g></svg></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">max_degree = <span class="number">20</span>     <span class="comment"># 20个特征</span></span><br><span class="line">n_train, n_test = <span class="number">100</span>, <span class="number">100</span>    <span class="comment"># 暂时把测试集与验证集混为一谈</span></span><br><span class="line">true_w = np.zeros(max_degree)</span><br><span class="line">true_w[<span class="number">0</span>:<span class="number">4</span>] = np.array([<span class="number">5</span>, <span class="number">1.2</span>, -<span class="number">3.4</span>, <span class="number">5.6</span>])    <span class="comment"># w系数</span></span><br><span class="line"></span><br><span class="line">features = np.random.normal(size=(n_train + n_test, <span class="number">1</span>)) <span class="comment"># 高斯分布生成矩阵100×1</span></span><br><span class="line">np.random.shuffle(features)</span><br><span class="line">poly_features = np.power(features, np.arange(max_degree).reshape(<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line"><span class="comment"># 等价于reshape(1,20)</span></span><br><span class="line"><span class="comment"># print(np.power([2,3], [3,4]))  输出[ 8 81]</span></span><br><span class="line"><span class="comment"># 此处使用了广播机制，故poly_features大小100×20</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_degree):</span><br><span class="line">    poly_features[:, i] /= math.gamma(i + <span class="number">1</span>)</span><br><span class="line">labels = np.dot(poly_features, true_w)</span><br><span class="line">labels += np.random.normal(scale=<span class="number">0.1</span>, size=labels.shape)</span><br></pre></td></tr></table></figure><blockquote><p>此处的 <code>math.gamm()</code> 函数，即伽马函数。<code>Gamma</code>函数是对正实数进行的一种扩展，定义如下：</p><p><svg xmlns="http://www.w3.org/2000/svg" width="21.619ex" height="5.232ex" viewbox="0 -1400.6 9555.8 2312.5" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-66-TEX-N-393" d="M128 619Q121 626 117 628T101 631T58 634H25V680H554V676Q556 670 568 560T582 444V440H542V444Q542 445 538 478T523 545T492 598Q454 634 349 634H334Q264 634 249 633T233 621Q232 618 232 339L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"/><path id="MJX-66-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-66-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-66-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-66-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-66-TEX-LO-222B" d="M114 -798Q132 -824 165 -824H167Q195 -824 223 -764T275 -600T320 -391T362 -164Q365 -143 367 -133Q439 292 523 655T645 1127Q651 1145 655 1157T672 1201T699 1257T733 1306T777 1346T828 1360Q884 1360 912 1325T944 1245Q944 1220 932 1205T909 1186T887 1183Q866 1183 849 1198T832 1239Q832 1287 885 1296L882 1300Q879 1303 874 1307T866 1313Q851 1323 833 1323Q819 1323 807 1311T775 1255T736 1139T689 936T633 628Q574 293 510 -5T410 -437T355 -629Q278 -862 165 -862Q125 -862 92 -831T55 -746Q55 -711 74 -698T112 -685Q133 -685 150 -700T167 -741Q167 -789 114 -798Z"/><path id="MJX-66-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"/><path id="MJX-66-TEX-N-30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"/><path id="MJX-66-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/><path id="MJX-66-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-66-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-66-TEX-I-1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/><path id="MJX-66-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-66-TEX-N-393"/></g><g data-mml-node="mo" transform="translate(625, 0)"><use xlink:href="#MJX-66-TEX-N-28"/></g><g data-mml-node="mi" transform="translate(1014, 0)"><use xlink:href="#MJX-66-TEX-I-1D467"/></g><g data-mml-node="mo" transform="translate(1479, 0)"><use xlink:href="#MJX-66-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(2145.8, 0)"><use xlink:href="#MJX-66-TEX-N-3D"/></g><g data-mml-node="msubsup" transform="translate(3201.6, 0)"><g data-mml-node="mo"><use xlink:href="#MJX-66-TEX-LO-222B"/></g><g data-mml-node="TeXAtom" transform="translate(1013.4, 1088.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-66-TEX-N-221E"/></g></g><g data-mml-node="TeXAtom" transform="translate(556, -896.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><use xlink:href="#MJX-66-TEX-N-30"/></g></g></g><g data-mml-node="msup" transform="translate(5138.7, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-66-TEX-I-1D465"/></g><g data-mml-node="TeXAtom" transform="translate(572, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><use xlink:href="#MJX-66-TEX-I-1D467"/></g><g data-mml-node="mo" transform="translate(465, 0)"><use xlink:href="#MJX-66-TEX-N-2212"/></g><g data-mml-node="mn" transform="translate(1243, 0)"><use xlink:href="#MJX-66-TEX-N-31"/></g></g></g><g data-mml-node="msup" transform="translate(6993.2, 0)"><g data-mml-node="mi"><use xlink:href="#MJX-66-TEX-I-1D452"/></g><g data-mml-node="TeXAtom" transform="translate(466, 413) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-66-TEX-N-2212"/></g><g data-mml-node="mi" transform="translate(778, 0)"><use xlink:href="#MJX-66-TEX-I-1D465"/></g></g></g><g data-mml-node="mi" transform="translate(8463.8, 0)"><use xlink:href="#MJX-66-TEX-I-1D451"/></g><g data-mml-node="mi" transform="translate(8983.8, 0)"><use xlink:href="#MJX-66-TEX-I-1D465"/></g></g></g></svg></p><p>当z为自然数时，<code>Gamma</code>函数等于阶乘，即</p><p><svg xmlns="http://www.w3.org/2000/svg" width="14.581ex" height="2.262ex" viewbox="0 -750 6445 1000" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-88-TEX-N-393" d="M128 619Q121 626 117 628T101 631T58 634H25V680H554V676Q556 670 568 560T582 444V440H542V444Q542 445 538 478T523 545T492 598Q454 634 349 634H334Q264 634 249 633T233 621Q232 618 232 339L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"/><path id="MJX-88-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-88-TEX-I-1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/><path id="MJX-88-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-88-TEX-N-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/><path id="MJX-88-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-88-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path id="MJX-88-TEX-N-21" d="M78 661Q78 682 96 699T138 716T180 700T199 661Q199 654 179 432T158 206Q156 198 139 198Q121 198 119 206Q118 209 98 431T78 661ZM79 61Q79 89 97 105T141 121Q164 119 181 104T198 61Q198 31 181 16T139 1Q114 1 97 16T79 61Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><use xlink:href="#MJX-88-TEX-N-393"/></g><g data-mml-node="mo" transform="translate(625, 0)"><use xlink:href="#MJX-88-TEX-N-28"/></g><g data-mml-node="mi" transform="translate(1014, 0)"><use xlink:href="#MJX-88-TEX-I-1D467"/></g><g data-mml-node="mo" transform="translate(1479, 0)"><use xlink:href="#MJX-88-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(2145.8, 0)"><use xlink:href="#MJX-88-TEX-N-3D"/></g><g data-mml-node="mo" transform="translate(3201.6, 0)"><use xlink:href="#MJX-88-TEX-N-28"/></g><g data-mml-node="mi" transform="translate(3590.6, 0)"><use xlink:href="#MJX-88-TEX-I-1D467"/></g><g data-mml-node="mo" transform="translate(4277.8, 0)"><use xlink:href="#MJX-88-TEX-N-2212"/></g><g data-mml-node="mn" transform="translate(5278, 0)"><use xlink:href="#MJX-88-TEX-N-31"/></g><g data-mml-node="mo" transform="translate(5778, 0)"><use xlink:href="#MJX-88-TEX-N-29"/></g><g data-mml-node="mo" transform="translate(6167, 0)"><use xlink:href="#MJX-88-TEX-N-21"/></g></g></g></svg></p><p>对于非自然数，<code>Gamma</code>函数的值是无法用基本的算术运算来计算的，必须使用数值逼近等计算方法。因此，在Go语言中，<code>Gamma</code>函数的计算也是不可避免的。</p></blockquote><p>看一下前2个样本</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">true_w, features, poly_features, labels = [</span><br><span class="line">    torch.tensor(x, dtype=torch.float32)</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> [true_w, features, poly_features, labels]]</span><br><span class="line"></span><br><span class="line">features[:<span class="number">2</span>], poly_features[:<span class="number">2</span>, :], labels[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    (tensor([[<span class="number">1.8354</span>],[<span class="number">0.2099</span>]]),</span><br><span class="line"> tensor([[<span class="number">1.0000e+00</span>, <span class="number">1.8354e+00</span>, <span class="number">1.6843e+00</span>, <span class="number">1.0304e+00</span>, <span class="number">4.7280e-01</span>, <span class="number">1.7355e-01</span>,</span><br><span class="line">          <span class="number">5.3088e-02</span>, <span class="number">1.3919e-02</span>, <span class="number">3.1934e-03</span>, <span class="number">6.5122e-04</span>, <span class="number">1.1952e-04</span>, <span class="number">1.9943e-05</span>,</span><br><span class="line">          <span class="number">3.0501e-06</span>, <span class="number">4.3062e-07</span>, <span class="number">5.6454e-08</span>, <span class="number">6.9075e-09</span>, <span class="number">7.9236e-10</span>, <span class="number">8.5545e-11</span>,</span><br><span class="line">          <span class="number">8.7226e-12</span>, <span class="number">8.4258e-13</span>],</span><br><span class="line">         [<span class="number">1.0000e+00</span>, <span class="number">2.0988e-01</span>, <span class="number">2.2025e-02</span>, <span class="number">1.5409e-03</span>, <span class="number">8.0851e-05</span>, <span class="number">3.3938e-06</span>,</span><br><span class="line">          <span class="number">1.1872e-07</span>, <span class="number">3.5595e-09</span>, <span class="number">9.3385e-11</span>, <span class="number">2.1778e-12</span>, <span class="number">4.5707e-14</span>, <span class="number">8.7210e-16</span>,</span><br><span class="line">          <span class="number">1.5253e-17</span>, <span class="number">2.4626e-19</span>, <span class="number">3.6918e-21</span>, <span class="number">5.1656e-23</span>, <span class="number">6.7760e-25</span>, <span class="number">8.3657e-27</span>,</span><br><span class="line">          <span class="number">9.7544e-29</span>, <span class="number">1.0775e-30</span>]]),</span><br><span class="line"> tensor([<span class="number">7.2075</span>, <span class="number">5.0886</span>]))</span><br></pre></td></tr></table></figure><p>实现一个函数来评估模型在给定数据集上的损失</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_loss</span>(<span class="params">net, data_iter, loss</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;评估给定数据集上模型的损失。&quot;&quot;&quot;</span></span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        out = net(X)</span><br><span class="line">        y = y.reshape(out.shape)</span><br><span class="line">        l = loss(out, y)</span><br><span class="line">        metric.add(l.<span class="built_in">sum</span>(), l.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>定义训练函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">train_features, test_features, train_labels, test_labels,</span></span><br><span class="line"><span class="params">          num_epochs=<span class="number">400</span></span>):</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    input_shape = train_features.shape[-<span class="number">1</span>]</span><br><span class="line">    net = nn.Sequential(nn.Linear(input_shape, <span class="number">1</span>, bias=<span class="literal">False</span>))</span><br><span class="line">    batch_size = <span class="built_in">min</span>(<span class="number">10</span>, train_labels.shape[<span class="number">0</span>])</span><br><span class="line">    train_iter = d2l.load_array((train_features, train_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                                batch_size)</span><br><span class="line">    test_iter = d2l.load_array((test_features, test_labels.reshape(-<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                               batch_size, is_train=<span class="literal">False</span>)</span><br><span class="line">    trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">1</span>, num_epochs], ylim=[<span class="number">1e-3</span>, <span class="number">1e2</span>],</span><br><span class="line">                            legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        d2l.train_epoch_ch3(net, train_iter, loss, trainer)</span><br><span class="line">        <span class="keyword">if</span> epoch == <span class="number">0</span> <span class="keyword">or</span> (epoch + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (evaluate_loss(</span><br><span class="line">                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;weight:&#x27;</span>, net[<span class="number">0</span>].weight.data.numpy())</span><br></pre></td></tr></table></figure><p>三阶多项式函数拟合(正态)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train(poly_features[:n_train, :<span class="number">4</span>], poly_features[n_train:, :<span class="number">4</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br><span class="line">Out:</span><br><span class="line">    weight: [[ <span class="number">4.994806</span>   <span class="number">1.2026892</span> -<span class="number">3.390093</span>   <span class="number">5.61095</span>  ]]</span><br></pre></td></tr></table></figure><p><img src="./12-3.png"></p><p>线性函数拟合(欠拟合)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只使用一少部分数据训练</span></span><br><span class="line">train(poly_features[:n_train, :<span class="number">2</span>], poly_features[n_train:, :<span class="number">2</span>],</span><br><span class="line">      labels[:n_train], labels[n_train:])</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    weight: [[<span class="number">3.4597428</span> <span class="number">3.9528937</span>]]</span><br></pre></td></tr></table></figure><p><img src="./12-4.png"></p><p>高阶多项式函数拟合(过拟合)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多次训练</span></span><br><span class="line">train(poly_features[:n_train, :], poly_features[n_train:, :],</span><br><span class="line">      labels[:n_train], labels[n_train:], num_epochs=<span class="number">1500</span>)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    weight: [[ <span class="number">4.9564543e+00</span>  <span class="number">1.2192174e+00</span> -<span class="number">3.2015967e+00</span>  <span class="number">5.4091620e+00</span></span><br><span class="line">  -<span class="number">5.5862081e-01</span>  <span class="number">9.4079292e-01</span>  <span class="number">1.6903839e-01</span> -<span class="number">1.2290818e-01</span></span><br><span class="line">   <span class="number">1.0401705e-02</span> -<span class="number">4.0972054e-02</span>  <span class="number">8.2412558e-03</span> -<span class="number">5.3764634e-02</span></span><br><span class="line">  -<span class="number">6.9650121e-02</span> -<span class="number">1.5937085e-01</span> -<span class="number">4.1621658e-03</span>  <span class="number">9.0129197e-02</span></span><br><span class="line">  -<span class="number">8.8063665e-02</span> -<span class="number">1.5993164e-01</span>  <span class="number">1.6256860e-02</span>  <span class="number">2.6524325e-03</span>]]</span><br></pre></td></tr></table></figure><p><img src="./12-5.png"></p><br><h3 id="4、Q-amp-A"><a href="#4、Q-amp-A" class="headerlink" title="4、Q&amp;A"></a>4、Q&amp;A</h3><h4 id="4-1-Little-Problems"><a href="#4-1-Little-Problems" class="headerlink" title="4.1 Little Problems"></a>4.1 Little Problems</h4><ul><li>SVM使用kernel来匹配模型复杂度的，计算复杂，适用于小数据量；SVM参数可调性较差</li><li>神经网络是一种“语言”，模型可编程性很强，可通过卷积做特征的提取，集特征提取和分类与一体；而SVM本身是一个分类器</li><li>模型剪枝，是为了让模型精简，让小模型运行更快</li><li>超参数的设计，一般靠研究者的经验，不要太大，也不要大小。调整方法，最简单的就是所谓老中医，自己训练自己调；其次就是<strong>随机超参数【推荐】</strong>，比如随机100次训练，选取效果最好的超参数； 贝叶斯的方法需要训练1000或10000次之后效果才会好起来【关于HPO，超参数选择】</li><li>假设有一个二分类问题，实际样本数据的情况为1：9。此时，如果数据集比较小，建议验证集的正反比例为1：1。否则，验证集很容易做到精度90%，比如把所有数据都判正或负。</li><li>SVM流行，多层感知机没落，是因为SVM有数学基础推理，但效果没有好多少；后来深度学习CNN流行，是因为效果好</li><li>MLP【Multilayer Perceptron】理论上可以拟合任意函数，但实际上并不能</li></ul><h4 id="4-2-验证数据集和训练数据体的数据清理"><a href="#4-2-验证数据集和训练数据体的数据清理" class="headerlink" title="4.2 验证数据集和训练数据体的数据清理"></a>4.2 验证数据集和训练数据体的数据清理</h4><p><strong>Q：</strong>验证数据集和训练数据体的数据清理（如异常值的处理）和特征构建（如标准化）是否需要放在一起处理？</p><p><strong>A：</strong>做标准化【即数据减去均值，再除以方差】，而均值方差怎么算？两种做法。</p><ul><li>一种是训练集和验证集放在一起求均值方差，因为并没有涉及标号，故该方法在实际生产中是可行的；</li><li>另一种是只在训练集上求均值方差，该方法比较保险【或者说，你只能拿到训练数据】</li></ul><h4 id="4-3-关于K折交叉验证"><a href="#4-3-关于K折交叉验证" class="headerlink" title="4.3 关于K折交叉验证"></a>4.3 关于K折交叉验证</h4><ul><li><p>K则交叉验证需要做K次，学习成本高，故在深度学习中，较大数据集上一般不用；但话说回来，K则交叉验证是当数据量不足的时候使用</p></li><li><p>K则交叉验证的<strong>目的</strong>是<strong>确定超参数</strong>。一般做法是，确定超参数后，在所有训练集上再训练一次得到模型参数；第二是不再训练，选择效果最好的那一折的模型；第三种是当预测的时候，使用K个模型进行预测，然后取预测值的均值</p></li><li><p>K则交叉验证一般是第一次分完组，之后就不再分组；而每次在打乱再分组的做法也可以，但这种做法的目的应该是为了得到K个模型</p></li></ul><br><h2 id="十三、权重衰退"><a href="#十三、权重衰退" class="headerlink" title="十三、权重衰退"></a>十三、权重衰退</h2><blockquote><p>权重衰退【Weight Decay】，是最常见的<strong>处理过拟合</strong>的方法【硬性 | 柔性】</p></blockquote><h3 id="1、权重衰退"><a href="#1、权重衰退" class="headerlink" title="1、权重衰退"></a>1、权重衰退</h3><p><img src="./13-1.png"></p><br><h3 id="2、代码理解"><a href="#2、代码理解" class="headerlink" title="2、代码理解"></a>2、代码理解</h3><h4 id="2-1-从零实现"><a href="#2-1-从零实现" class="headerlink" title="2.1 从零实现"></a>2.1 从零实现</h4><p>权重衰减是最广泛使用的正则化的技术之一</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>像以前一样生成一些数据 <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mi>y</mi><mo>=</mo><mn>0.05</mn><mo>+</mo><munderover><mo data-mjx-texclass="OP">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>d</mi></mrow></munderover><mn>0.01</mn><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>ϵ</mi><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mtext>&nbsp;</mtext><mtext>&nbsp;</mtext><mi>w</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>e</mi><mtext>&nbsp;</mtext><mstyle scriptlevel="0"><mspace width="thinmathspace"></mspace></mstyle><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msup><mn>0.01</mn><mn>2</mn></msup><mo stretchy="false">)</mo></math></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 因为数据较为简单，因此如果模型复杂会导致过拟合。故训练样本选择小</span></span><br><span class="line">n_train, n_test, num_inputs, batch_size = <span class="number">20</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">5</span></span><br><span class="line"><span class="comment"># num_inputs：特征维度</span></span><br><span class="line">true_w, true_b = torch.ones((num_inputs, <span class="number">1</span>)) * <span class="number">0.01</span>, <span class="number">0.05</span> <span class="comment"># 200×1</span></span><br><span class="line">train_data = d2l.synthetic_data(true_w, true_b, n_train) <span class="comment"># 生成人工数据集</span></span><br><span class="line">train_iter = d2l.load_array(train_data, batch_size) <span class="comment"># 数组变iterator</span></span><br><span class="line">test_data = d2l.synthetic_data(true_w, true_b, n_test)</span><br><span class="line">test_iter = d2l.load_array(test_data, batch_size, is_train=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>初始化模型参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>():</span><br><span class="line">    w = torch.normal(<span class="number">0</span>, <span class="number">1</span>, size=(num_inputs, <span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> [w, b]</span><br></pre></td></tr></table></figure><p>定义<code>L2</code>范数惩罚【L2范数定义】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">l2_penalty</span>(<span class="params">w</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">sum</span>(w.<span class="built_in">pow</span>(<span class="number">2</span>)) / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>定义训练代码实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">lambd</span>):</span><br><span class="line">    w, b = init_params()</span><br><span class="line">    net, loss = <span class="keyword">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># with torch.enable_grad():</span></span><br><span class="line">             <span class="comment"># 🔺不同之处：lambd λ</span></span><br><span class="line">            l = loss(net(X), y) + lambd * l2_penalty(w)</span><br><span class="line">            l.<span class="built_in">sum</span>().backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数是：&#x27;</span>, torch.norm(w).item())</span><br></pre></td></tr></table></figure><p>忽略正则化直接训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    w的L2范数是： <span class="number">13.317912101745605</span></span><br></pre></td></tr></table></figure><p><img src="./13-2.png"></p><blockquote><p>此时，loss不断下降，但验证集正确率不变，这是典型的过拟合</p></blockquote><p>使用权重衰减</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train(lambd=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    w的L2范数是： <span class="number">0.36755216121673584</span></span><br></pre></td></tr></table></figure><p><img src="./13-3.png"></p><blockquote><p>“阀”使得权重不会太大。🔺此处可尝试<strong>调整 λ 更大</strong>，但可能会欠拟合</p></blockquote><br><h4 id="2-2-简洁实现"><a href="#2-2-简洁实现" class="headerlink" title="2.2 简洁实现"></a>2.2 简洁实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_concise</span>(<span class="params">wd</span>):</span><br><span class="line">    net = nn.Sequential(nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">        param.data.normal_()</span><br><span class="line">    loss = nn.MSELoss()</span><br><span class="line">    num_epochs, lr = <span class="number">100</span>, <span class="number">0.003</span></span><br><span class="line">    <span class="comment"># weight_decay:λ</span></span><br><span class="line">    trainer = torch.optim.SGD([&#123;</span><br><span class="line">        <span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].weight,</span><br><span class="line">        <span class="string">&#x27;weight_decay&#x27;</span>: wd&#125;, &#123;</span><br><span class="line">            <span class="string">&quot;params&quot;</span>: net[<span class="number">0</span>].bias&#125;], lr=lr)</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epochs&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">                            xlim=[<span class="number">5</span>, num_epochs], legend=[<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">                trainer.zero_grad()</span><br><span class="line">                l = loss(net(X), y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.step()</span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            animator.add(epoch + <span class="number">1</span>, (d2l.evaluate_loss(net, train_iter, loss),</span><br><span class="line">                                     d2l.evaluate_loss(net, test_iter, loss)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;w的L2范数：&#x27;</span>, net[<span class="number">0</span>].weight.norm().item())</span><br></pre></td></tr></table></figure><p>这些图看起来和我们从零开始实现权重衰减时的图相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_concise(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    w的L2范数： <span class="number">13.924778938293457</span></span><br><span class="line">    </span><br><span class="line">train_concise(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    w的L2范数： <span class="number">0.3728162348270416</span></span><br></pre></td></tr></table></figure><p><img src="./13-4.png"></p><br><h3 id="3、Q-amp-A"><a href="#3、Q-amp-A" class="headerlink" title="3、Q&amp;A"></a>3、Q&amp;A</h3><h4 id="3-1-Little-Problems"><a href="#3-1-Little-Problems" class="headerlink" title="3.1 Little Problems"></a>3.1 Little Problems</h4><ul><li>实践中权重衰退的值一般取0.1、0.01、0.001、0.0001等，效果会有，但优化力度不够，之后会有更多的模型复杂度控制。</li><li>【λ阀值的意义】由于噪音的存在，模型会尝试记住噪音，将过大权值w拉回真实权重而去忽略噪音，避免过拟合。阀值λ，控制拉回的幅度，即λ小，纠正幅度小；λ大，纠正幅度大。当然，如果数据没有噪音，λ的存在就没有了意义。</li></ul><h4 id="3-2-为什么参数不大，复杂度就低？"><a href="#3-2-为什么参数不大，复杂度就低？" class="headerlink" title="3.2 为什么参数不大，复杂度就低？"></a>3.2 为什么参数不大，复杂度就低？</h4><p>如果在比较小的范围取参数权重w，那么模型的空间就会变小。</p><p>以《…硬性限制》为例子，如果参数选择比较大，我们可以使用很复杂的曲线去拟合；而此时，我们限制w的范围，使其处于一个范围，这样模型复杂度会变低，也学不出复杂模型。</p><br><h2 id="十四、丢弃法"><a href="#十四、丢弃法" class="headerlink" title="十四、丢弃法"></a>十四、丢弃法</h2><h3 id="1、Dropout"><a href="#1、Dropout" class="headerlink" title="1、Dropout"></a>1、Dropout</h3><blockquote><p>目前主流的多层感知机模型大小的一个控制办法</p></blockquote><p><img src="./14-1.png"></p><br><h3 id="2、代码理解-1"><a href="#2、代码理解-1" class="headerlink" title="2、代码理解"></a>2、代码理解</h3><p>我们实现 <code>dropout_layer</code> 函数，该函数以<code>dropout</code>的概率丢弃张量输入<code>X</code>中的元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dropout_layer</span>(<span class="params">X, dropout</span>):</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= dropout &lt;= <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.zeros_like(X)</span><br><span class="line">    <span class="keyword">if</span> dropout == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    <span class="comment"># 掩码的思路:生成一个只有0和1的矩阵</span></span><br><span class="line">       <span class="comment"># 做乘法，远比选函数快</span></span><br><span class="line">    mask = (torch.randn(X.shape) &gt; dropout).<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">return</span> mask * X / (<span class="number">1.0</span> - dropout)</span><br></pre></td></tr></table></figure><p>测试<code>dropout_layer</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">2</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">0.5</span>))</span><br><span class="line"><span class="built_in">print</span>(dropout_layer(X, <span class="number">1.</span>))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]])</span><br><span class="line">    tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]])</span><br><span class="line">    tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">14.</span>],</span><br><span class="line">        [ <span class="number">0.</span>, <span class="number">18.</span>, <span class="number">20.</span>,  <span class="number">0.</span>, <span class="number">24.</span>,  <span class="number">0.</span>, <span class="number">28.</span>,  <span class="number">0.</span>]])</span><br><span class="line">    tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p>定义具有两个隐藏层的多层感知机，每个隐藏层包含256个单元</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens1, num_hiddens2 = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">dropout1, dropout2 = <span class="number">0.2</span>, <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,</span></span><br><span class="line"><span class="params">                 is_training=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.num_inputs = num_inputs</span><br><span class="line">        <span class="variable language_">self</span>.training = is_training</span><br><span class="line">        <span class="variable language_">self</span>.lin1 = nn.Linear(num_inputs, num_hiddens1)</span><br><span class="line">        <span class="variable language_">self</span>.lin2 = nn.Linear(num_hiddens1, num_hiddens2)</span><br><span class="line">        <span class="variable language_">self</span>.lin3 = nn.Linear(num_hiddens2, num_outputs)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># 将X进行reshape之后，进入第一个隐藏层，再relu一下</span></span><br><span class="line">        H1 = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.lin1(X.reshape((-<span class="number">1</span>, <span class="variable language_">self</span>.num_inputs))))</span><br><span class="line">        <span class="comment"># 只有在训练的时候dropout </span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training == <span class="literal">True</span>:</span><br><span class="line">            H1 = dropout_layer(H1, dropout1)</span><br><span class="line">        H2 = <span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.lin2(H1))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training == <span class="literal">True</span>:</span><br><span class="line">            H2 = dropout_layer(H2, dropout2)</span><br><span class="line">        <span class="comment"># 输出不做dropout</span></span><br><span class="line">        out = <span class="variable language_">self</span>.lin3(H2)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)</span><br></pre></td></tr></table></figure><p>训练和测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr, batch_size = <span class="number">10</span>, <span class="number">0.5</span>, <span class="number">256</span></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="./14-2.png"></p><blockquote><p>如果<code>dropout=0</code>，训练loss会很小，容易过拟合</p><p>因此，在有dropout的情况下，可以适当将隐藏层变的大一点</p></blockquote><p><strong>简洁实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout1), nn.Linear(<span class="number">256</span>, <span class="number">256</span>), nn.ReLU(),</span><br><span class="line">                    nn.Dropout(dropout2), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line">net.apply(init_weights);</span><br></pre></td></tr></table></figure><p>对模型进行训练和测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)</span><br></pre></td></tr></table></figure><p><img src="./14-3.png"></p><h3 id="3、Q-amp-A-1"><a href="#3、Q-amp-A-1" class="headerlink" title="3、Q&amp;A"></a>3、Q&amp;A</h3><h4 id="3-1-Little-Problems-1"><a href="#3-1-Little-Problems-1" class="headerlink" title="3.1 Little Problems"></a>3.1 Little Problems</h4><ul><li>Dropout随机置零之后，相应的梯度也会置零，未置零的项也会相应乘一个值【相当于每次从网络中随机采样一个小网络】</li><li>Dropout，包括神经网络，其可重复性一直就是一个比较难的问题【但Dropout相对较好，如果<strong>固定随机种子</strong>，重复Run 10次结果基本一样】但话说回来，不需要可重复性，随机性越高，模型稳定性也越好</li><li>BN是给<strong>卷积层</strong>用的，Dropout是给<strong>全连接层</strong>用的</li><li>再次强调：【推理中的Dropout是直接返回输入】Dropout是一个正则项，作用的为了当你更新权重的时候，让模型复杂度降低，减弱过拟合；而做推理或预测，是直接使用参数，不更新参数。当然，如果在推理时Dropout，会导致预测值不稳定，可能需要多次预测再平均。</li><li>Dropout每次随机选几个子网络，最后做平均的做法，类似于随机森林多决策树做投票的思想</li><li>Dropout和Weight Decay都属于正则，前者主要应用于全连接层，后者可应用于卷积、Transform都可以用。Dropout的优势在于好调参，很直观，即丢弃多少</li><li>MLP也用的越来越少，其性能好但不好调参。当然，目前更常用CNN，其实是MLP的一个特例。</li><li>Dropout有可能会导致参数收敛减慢，经验上与<code>lr</code>无关【LearningRate对期望和方差敏感】</li></ul><h4 id="3-2-Dropout函数返回值相关"><a href="#3-2-Dropout函数返回值相关" class="headerlink" title="3.2 Dropout函数返回值相关"></a>3.2 Dropout函数返回值相关</h4><p><strong>Q：</strong>Dropout函数返回值的表达式【<code>return X*mask/(1-p)</code>】，未被丢弃的部分会除以<code>（1-p）</code>，但训练数据的标签还是原来的值，如何保持对应？</p><p><strong>A：</strong>Dropout唯一改变的隐藏层的输出，标签并不改变【改标签也常用，属于正则化手法，可能在CV设涉及】。除以<code>(1-p)</code>之后，期望不变决定了输出值与标签的对应。</p><p>例如，dropout=0.5，即训练时，有一半的神经元设为0，剩余神经元会乘以2，故期望不会发生变化</p><br><h2 id="十五、数值稳定性、模型初始化、激活函数"><a href="#十五、数值稳定性、模型初始化、激活函数" class="headerlink" title="十五、数值稳定性、模型初始化、激活函数"></a>十五、数值稳定性、模型初始化、激活函数</h2><h3 id="1、数值稳定性"><a href="#1、数值稳定性" class="headerlink" title="1、数值稳定性"></a>1、数值稳定性</h3><blockquote><p>特别是当神经网络变的很深时，数值很容易不稳定</p></blockquote><p><img src="./15-1.png"></p><h3 id="2、模型初始化和激活函数"><a href="#2、模型初始化和激活函数" class="headerlink" title="2、模型初始化和激活函数"></a>2、模型初始化和激活函数</h3><p><img src="./15-2.png"></p><h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h4><p>合理的权重初始值和激活函数的选取，可以提升数值稳定性</p><br><h3 id="3、Q-amp-A-2"><a href="#3、Q-amp-A-2" class="headerlink" title="3、Q&amp;A"></a>3、Q&amp;A</h3><h4 id="3-1-Little-Problems-2"><a href="#3-1-Little-Problems-2" class="headerlink" title="3.1 Little Problems"></a>3.1 Little Problems</h4><ul><li>Infinite值是太大了，一般是<strong>lr过大</strong>或者<strong>初始权重过大</strong>【调整缩小方差】；而NAN一般是除0错误，可能是已经梯度过小</li><li>python默认float16位，但芯片硬件倾向于做更低位的运算，8位甚至1位</li><li>梯度消失的原因可能是sigmoid导致的，但可能还有其他原因；梯度爆炸是由于每一层输出的值过大导致</li><li>输出或参数值在一个合理的区间里，均值为0，方差为一个固定值，这样计算不叫好算。而用正态分布纯粹是因为算起来很容易。</li><li>可视化梯度山地图等高线是一个待研究的领域</li><li>强制使得每一层的输出特征均值为0，方差为1，不会损失模型的表达能力，因为只是讲模型拉到一个合理的范围，便于计算</li><li>所有的方法都是缓解数值稳定性问题，而不是解决，整个深度学习的进展基本都是为了提高数值稳定性</li></ul><h4 id="3-2-对于复杂数学公式的应对办法"><a href="#3-2-对于复杂数学公式的应对办法" class="headerlink" title="3.2 对于复杂数学公式的应对办法"></a>3.2 对于复杂数学公式的应对办法</h4><p>深度学习的一个好处就是不需要你懂太多数学的东西，可导就行了，不像SVM等需要很多数学方法来优化。但反过来讲，数学还是得学，这是理解能力，决定了你能处理多复杂的任务，类似电脑内存；而会调参能写代码是代码能力，类似电脑cpu</p><h4 id="3-3-实现Xavier初始化"><a href="#3-3-实现Xavier初始化" class="headerlink" title="3.3 实现Xavier初始化"></a>3.3 实现<code>Xavier</code>初始化</h4><blockquote><p>待补充</p></blockquote><br><h2 id="十六、Kaggle房价预测"><a href="#十六、Kaggle房价预测" class="headerlink" title="十六、Kaggle房价预测"></a>十六、Kaggle房价预测</h2><blockquote><p>待补充</p></blockquote><br><br><br><h2 id="十七、未归类问题汇总"><a href="#十七、未归类问题汇总" class="headerlink" title="十七、未归类问题汇总"></a>十七、未归类问题汇总</h2><h3 id="1-沐神代码的可视化代码，不显示Train-Loss"><a href="#1-沐神代码的可视化代码，不显示Train-Loss" class="headerlink" title="1 沐神代码的可视化代码，不显示Train_Loss?"></a>1 沐神代码的可视化代码，不显示<code>Train_Loss</code>?</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改loss模型,添加 reduction=&#x27;none&#x27;</span></span><br><span class="line">loss = nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure><br><h3 id="2-全连接层与隐藏层的区别"><a href="#2-全连接层与隐藏层的区别" class="headerlink" title="2 全连接层与隐藏层的区别"></a>2 全连接层与隐藏层的区别</h3><p>如果当前层与前一层的所有神经元都有连接，那么我们称该层为<strong>全连接层</strong>。就像下图这个样子，我们称右侧的层为全连接层。</p><p><img src="./17-1.png"></p><blockquote><p>如果左侧层有<code>m</code>个神经元，右侧层<code>n</code>个神经元，且n为全连接层，那么左侧与右侧之间一共有<code>m×n</code>个w参数</p></blockquote><p><strong>隐藏层</strong>其实是一个简单的概念，在神经网络中，除了输入层和输出层之外的层级，都是隐藏层。这是因为在神经网络中，除了输入层和输出层之外的其他层级就像一个黑盒一般，无法看见，里面的层级是隐藏在这其中。</p><p>对于多层感知机MLP来说，<strong>输出层</strong>是将全连接层的输出经过激活函数激活后输出。</p><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><h2 id="往次组会汇报"><a href="#往次组会汇报" class="headerlink" title="*往次组会汇报"></a>*往次组会汇报</h2><h4 id="7-6组会下周计划："><a href="#7-6组会下周计划：" class="headerlink" title="7.6组会下周计划："></a>7.6组会下周计划：</h4><p>1、跟视频课程动手练习+理解</p><p>2、适应conda环境和Linux环境，之后的课程应该会加快速度</p><h4 id="7-13组会汇报"><a href="#7-13组会汇报" class="headerlink" title="7.13组会汇报"></a>7.13组会汇报</h4><ul><li><p>完成了虚拟机中Linux环境的配置，熟悉了常用Linux命令</p></li><li><p>课程的数据操作和数据预处理，额外去搜集并查阅了Numpy、Pandas、Pytorch库，以便未来随时查阅</p></li><li><p>课程的线性代数、线性代数的实现、矩阵计算以及实现</p></li></ul><p><strong>下周计划</strong></p><ul><li><p>结合其他已有材料，动手理解以下课程【线性回归】</p></li><li><p>整理上周的笔记</p></li><li><p>之前慢的原因也有分析【知识笔记的框架搭建 | 工具软件不熟练 | 效率问题】</p></li></ul><h4 id="7-20组会汇报"><a href="#7-20组会汇报" class="headerlink" title="7.20组会汇报"></a>7.20组会汇报</h4><p><strong>本周工作</strong></p><ul><li>自动求导及实现【课程 | 自动求导的原理】</li><li>线性回归及从零实现【矩阵求导、最小化损失来学习wb时最优显示解的推导、跟视频课做了一遍从零实现（看的比较慢，结合其他的资料【pytorch的常用的方法总结、Python的常用语法函数的积累】、）正在习惯这种数据处理的思路】</li><li>线性回归的简洁实现【现有函数调用】</li><li>softmax回归开头</li></ul><p><strong>下周计划</strong></p><ul><li>softmax回归全流程</li><li>多层感知机那一块</li></ul><h4 id="7-27组会汇报"><a href="#7-27组会汇报" class="headerlink" title="7.27组会汇报"></a>7.27组会汇报</h4><p><strong>本周工作</strong></p><ul><li><p>softmax章节【Softmax引入、推导 | 从零实现和简洁实现】理解为主</p></li><li><p>matplotlib绘图编程、python已用到的语法、函数和表达式的总结、numpy、pytorch</p></li><li><p>感知机、多层感知机、代码实现</p></li><li><p>模型选择的问题，过拟合/欠拟合</p></li></ul><p><strong>下周计划</strong></p><ul><li>权重衰退的理解、数值稳定性、激活函数</li><li>提高效率</li></ul><br><h4 id="8-10组会汇报"><a href="#8-10组会汇报" class="headerlink" title="8.10组会汇报"></a>8.10组会汇报</h4><p><strong>本周工作</strong></p><ul><li><p>因为自我感觉对Softmax章节理解不太深刻，重新听了一遍该章节的课程，并手推了softmax 损失函数梯度计算公式</p></li><li><p>多层感知机、模型选择欠拟合过拟合</p></li><li><p>权重衰退的理解、数值稳定性【未总结】</p></li><li><p>pytorch、Numpy的一些已经用到的API的查找和记录</p></li><li><p>卷积神经网络开头，概念部分的需要进一步更深的理解</p></li></ul><p><strong>下周计划</strong></p><ul><li>深度学习：kaggle实战/卷积</li><li>了解老师的研究项目的论文</li></ul><p><strong>小总结</strong></p><ul><li>主要是深度学习的一个入门，新内容的学习、思维和记录框架【环境搭建、预备知识、线性&amp;softmax回归、多层感知机、软能力的接触】</li><li>最关键的是进入一个研一的学习和工作状态</li><li>计划方面：本来预期是这个课程，目前来看，学习的内容量还是很大的；之后后续也会继续跟着课程推进</li></ul><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Moustache 微信支付"><p>微信支付</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Moustache</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/" title="入门深度学习">https://hammerzer.github.io/2023/07/09/Introduction-to-deep-learning/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2023/05/28/traveller/" rel="prev" title="旅行者笔记"><i class="fa fa-chevron-left"></i> 旅行者笔记</a></div><div class="post-nav-item"><a href="/2023/07/19/Python-Start/" rel="next" title="Python-Start">Python-Start <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CONTENT-OUTLINE"><span class="nav-number">1.</span> <span class="nav-text">CONTENT OUTLINE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%87%E3%80%81%E7%9B%AE%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">〇、目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="nav-number">3.</span> <span class="nav-text">一、安装虚拟机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0%E3%80%81%E5%85%B3%E4%BA%8EVMware"><span class="nav-number">3.1.</span> <span class="nav-text">0、关于VMware</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E8%A7%A3%E5%86%B3%E8%99%9A%E6%8B%9F%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98%E3%80%90%E5%85%B3%E4%BA%8EBIOS%E8%AE%BE%E7%BD%AE%E3%80%91"><span class="nav-number">3.2.</span> <span class="nav-text">1、解决虚拟启动问题【关于BIOS设置】</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Ubuntu%E9%97%AE%E9%A2%98"><span class="nav-number">4.</span> <span class="nav-text">二、Ubuntu问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0%E3%80%81%E5%B0%8F%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB"><span class="nav-number">4.1.</span> <span class="nav-text">0、小问题汇总</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-1-%E5%B1%8F%E5%B9%95%E6%98%BE%E7%A4%BA%E5%A4%A7%E5%B0%8F%E8%B0%83%E6%95%B4"><span class="nav-number">4.1.1.</span> <span class="nav-text">0.1 屏幕显示大小调整</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-2-%E5%88%9D%E5%A7%8Bsu%E5%AF%86%E7%A0%81%E9%94%99%E8%AF%AF"><span class="nav-number">4.1.2.</span> <span class="nav-text">0.2 初始su密码错误</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-3-Ubuntu%E4%B8%8A%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D"><span class="nav-number">4.1.3.</span> <span class="nav-text">0.3 Ubuntu上修改主机名</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-4-Ubuntu%E5%AE%89%E8%A3%85%E4%B8%80%E7%9B%B4%E5%8D%A1%E9%BB%91%E5%B1%8F"><span class="nav-number">4.1.4.</span> <span class="nav-text">0.4 Ubuntu安装一直卡黑屏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-5-%E4%BF%AE%E6%94%B9%E6%9C%80%E4%BD%B3apt%E4%B8%8B%E8%BD%BD%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">4.1.5.</span> <span class="nav-text">0.5 修改最佳apt下载服务器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-6-%E9%94%99%E8%AF%AF%EF%BC%9A%E6%97%A0%E6%B3%95%E5%BE%97%E5%88%B0%E9%94%81"><span class="nav-number">4.1.6.</span> <span class="nav-text">0.6 错误：无法得到锁</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-7-Ubuntu%E6%89%93%E5%BC%80%E7%BB%88%E7%AB%AF%E6%97%B6%E8%87%AA%E5%8A%A8%E9%80%80%E5%87%BAbase%E7%8E%AF%E5%A2%83"><span class="nav-number">4.1.7.</span> <span class="nav-text">0.7 Ubuntu打开终端时自动退出base环境</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Ubuntu-%E5%88%86%E5%8C%BA%E5%8F%B7Sda%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">4.2.</span> <span class="nav-text">1、Ubuntu 分区号Sda的解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81Ubuntu%E5%BF%AB%E6%8D%B7%E9%94%AE"><span class="nav-number">4.3.</span> <span class="nav-text">2、Ubuntu快捷键</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E7%BB%88%E7%AB%AF%E5%91%BD%E4%BB%A4"><span class="nav-number">4.4.</span> <span class="nav-text">3、终端命令</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Ananconda%E7%9B%B8%E5%85%B3"><span class="nav-number">5.</span> <span class="nav-text">三、Ananconda相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Anaconda%E4%BF%AE%E6%94%B9%E5%9B%BD%E5%86%85%E9%95%9C%E5%83%8F%E6%BA%90"><span class="nav-number">5.1.</span> <span class="nav-text">1 Anaconda修改国内镜像源</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E9%80%9A%E8%BF%87-conda-config-%E5%91%BD%E4%BB%A4%E7%94%9F%E6%88%90%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">5.1.1.</span> <span class="nav-text">1.1 通过 conda config 命令生成配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">5.1.2.</span> <span class="nav-text">1.2 修改配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E6%9F%A5%E7%9C%8B%E6%98%AF%E5%90%A6%E7%94%9F%E6%95%88"><span class="nav-number">5.1.3.</span> <span class="nav-text">1.3 查看是否生效</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Anaconda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%B0%8F%E7%BB%93"><span class="nav-number">5.2.</span> <span class="nav-text">2 Anaconda常用命令小结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E7%8E%AF%E5%A2%83%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3"><span class="nav-number">5.2.1.</span> <span class="nav-text">2.1 环境管理相关</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-Python%E7%AE%A1%E7%90%86%E7%9B%B8%E5%85%B3"><span class="nav-number">5.2.2.</span> <span class="nav-text">2.2 Python管理相关</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%88%86%E4%BA%AB%E7%8E%AF%E5%A2%83"><span class="nav-number">5.3.</span> <span class="nav-text">3 分享环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Anaconda-%E5%8D%B8%E8%BD%BD-%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85"><span class="nav-number">5.4.</span> <span class="nav-text">4 Anaconda 卸载+重新安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-Jupter-Notebook"><span class="nav-number">5.5.</span> <span class="nav-text">5 Jupter Notebook</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85"><span class="nav-number">5.5.1.</span> <span class="nav-text">5.1 下载安装</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E4%BD%BF%E7%94%A8Tips"><span class="nav-number">5.5.2.</span> <span class="nav-text">5.2 使用Tips</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-0-A-Few"><span class="nav-number">5.5.2.1.</span> <span class="nav-text">5.2.0 A Few</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-1-%E5%9C%A8%E7%89%B9%E5%AE%9A%E6%96%87%E4%BB%B6%E5%A4%B9%E8%BF%90%E8%A1%8C"><span class="nav-number">5.5.2.2.</span> <span class="nav-text">5.2.1 在特定文件夹运行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-2-%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0"><span class="nav-number">5.5.2.3.</span> <span class="nav-text">5.2.2 魔法函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-2-3-%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E4%B9%8BLaTex"><span class="nav-number">5.5.2.4.</span> <span class="nav-text">5.2.3 数学公式之LaTex</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-%E5%9C%A8-Jupyter-%E4%B8%AD%E5%88%87%E6%8D%A2%E4%BD%BF%E7%94%A8-conda-%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="nav-number">5.5.3.</span> <span class="nav-text">5.3 在 Jupyter 中切换使用 conda 虚拟环境</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#5-3-1-%E5%9C%A8-conda-%E7%8E%AF%E5%A2%83%E4%B8%AD%E8%BF%90%E8%A1%8C-Jupyter-%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8C%E5%86%85%E6%A0%B8"><span class="nav-number">5.5.3.1.</span> <span class="nav-text">5.3.1 在 conda 环境中运行 Jupyter 服务器和内核</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-3-2-%E4%B8%BA-conda-%E7%8E%AF%E5%A2%83%E5%88%9B%E5%BB%BA%E7%89%B9%E6%AE%8A%E5%86%85%E6%A0%B8"><span class="nav-number">5.5.3.2.</span> <span class="nav-text">5.3.2 为 conda 环境创建特殊内核</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-3-3-%E4%BD%BF%E7%94%A8-nb-conda-kernels-%E6%B7%BB%E5%8A%A0%E6%89%80%E6%9C%89%E7%8E%AF%E5%A2%83"><span class="nav-number">5.5.3.3.</span> <span class="nav-text">5.3.3 使用 nb_conda_kernels 添加所有环境</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-conda-install-%E4%B8%8E-pip-install-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">5.6.</span> <span class="nav-text">6 conda install 与 pip install 的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-%E4%BD%BF%E7%94%A8%E5%8C%BA%E5%88%AB"><span class="nav-number">5.6.1.</span> <span class="nav-text">6.1 使用区别</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-%E5%BA%93%E7%9A%84%E5%82%A8%E5%AD%98%E4%BD%8D%E7%BD%AE"><span class="nav-number">5.6.2.</span> <span class="nav-text">6.2 库的储存位置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93"><span class="nav-number">5.6.3.</span> <span class="nav-text">6.3 使用总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-%E9%94%99%E8%AF%AF%E8%A7%A3%E5%86%B3"><span class="nav-number">5.7.</span> <span class="nav-text">7 错误解决</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-conda%E5%91%BD%E4%BB%A4%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8"><span class="nav-number">5.7.1.</span> <span class="nav-text">7.1 conda命令不能使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-Conda-%E5%90%AF%E5%8A%A8%E6%8A%A5%E9%94%99-Invoke-Expression"><span class="nav-number">5.7.2.</span> <span class="nav-text">7.2 Conda 启动报错 Invoke-Expression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%85%B3%E4%BA%8EGithub"><span class="nav-number">6.</span> <span class="nav-text">四、关于Github</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E8%B7%9F%E6%B2%90%E7%A5%9E%E5%AD%A6%E6%90%AD%E5%BB%BA%E7%8E%AF%E5%A2%83"><span class="nav-number">7.</span> <span class="nav-text">五、跟沐神学搭建环境</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#0%E3%80%81Little-Problems"><span class="nav-number">7.1.</span> <span class="nav-text">0、Little Problems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#0-1-pip%E5%AE%89%E8%A3%85%E6%A8%A1%E5%9D%97%E6%8A%A5%E9%94%99"><span class="nav-number">7.1.1.</span> <span class="nav-text">0.1 pip安装模块报错</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-2-%E5%AE%89%E8%A3%85torch%E6%97%B6%E8%BF%9B%E7%A8%8B%E8%A2%AB%E6%9D%80"><span class="nav-number">7.1.2.</span> <span class="nav-text">0.2 安装torch时进程被杀</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-3-Ubuntu%E6%97%A0%E6%B3%95%E5%AE%89%E8%A3%85-git"><span class="nav-number">7.1.3.</span> <span class="nav-text">0.3 Ubuntu无法安装 git</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#0-4-%E8%BF%9B%E5%85%A5conda%E7%8E%AF%E5%A2%83%E5%87%BA%E9%94%99"><span class="nav-number">7.1.4.</span> <span class="nav-text">0.4 进入conda环境出错</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E6%93%8D%E4%BD%9C"><span class="nav-number">7.2.</span> <span class="nav-text">2、操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E5%85%B3%E4%BA%8Ebuild-essential"><span class="nav-number">7.2.1.</span> <span class="nav-text">1.1 关于build-essential</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="nav-number">8.</span> <span class="nav-text">六、数据操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E8%AF%BE%E7%A8%8B%E5%86%85%E5%AE%B9"><span class="nav-number">8.1.</span> <span class="nav-text">1、课程内容</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-N%E7%BB%B4%E6%95%B0%E7%BB%84"><span class="nav-number">8.1.1.</span> <span class="nav-text">1.1 N维数组</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E5%88%9B%E5%BB%BA%E6%95%B0%E7%BB%84"><span class="nav-number">8.1.2.</span> <span class="nav-text">1.2 创建数组</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-%E8%AE%BF%E9%97%AE%E5%85%83%E7%B4%A0"><span class="nav-number">8.1.3.</span> <span class="nav-text">1.3 访问元素</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-5-%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="nav-number">8.1.4.</span> <span class="nav-text">1.5 数据操作</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%85%B3%E4%BA%8ETorch"><span class="nav-number">8.2.</span> <span class="nav-text">2、关于Torch</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E5%85%B3%E4%BA%8ENumpy"><span class="nav-number">8.3.</span> <span class="nav-text">3、关于Numpy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E5%85%B3%E4%BA%8Epandas"><span class="nav-number">8.4.</span> <span class="nav-text">4、关于pandas</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81%E5%85%B3%E4%BA%8E%E4%BD%9C%E5%9B%BE"><span class="nav-number">8.5.</span> <span class="nav-text">5、关于作图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">9.</span> <span class="nav-text">七、数据预处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86"><span class="nav-number">10.</span> <span class="nav-text">八、预备知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-number">10.1.</span> <span class="nav-text">1、线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9%E5%9B%9E%E9%A1%BE"><span class="nav-number">10.1.1.</span> <span class="nav-text">1.1 相关内容回顾</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AE%9E%E7%8E%B0"><span class="nav-number">10.1.2.</span> <span class="nav-text">1.2 线性代数实现</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-1-%E6%A0%87%E9%87%8F%E7%94%B1%E5%8F%AA%E6%9C%89%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84%E5%BC%A0%E9%87%8F%E8%A1%A8%E7%A4%BA"><span class="nav-number">10.1.2.1.</span> <span class="nav-text">1.2.1 标量由只有一个元素的张量表示</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-2-%E5%90%91%E9%87%8F"><span class="nav-number">10.1.2.2.</span> <span class="nav-text">1.2.2 向量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-3-%E7%9F%A9%E9%98%B5"><span class="nav-number">10.1.2.3.</span> <span class="nav-text">1.2.3 矩阵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-4-%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97"><span class="nav-number">10.1.2.4.</span> <span class="nav-text">1.2.4 矩阵运算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-5-%E6%B1%82%E5%92%8C"><span class="nav-number">10.1.2.5.</span> <span class="nav-text">1.2.5 求和</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-6-%E6%B1%82%E5%9D%87%E5%80%BC"><span class="nav-number">10.1.2.6.</span> <span class="nav-text">1.2.6 求均值</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-7-%E7%82%B9%E7%A7%AF"><span class="nav-number">10.1.2.7.</span> <span class="nav-text">1.2.7 点积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-8-%E7%9F%A9%E9%98%B5%E5%90%91%E9%87%8F%E7%A7%AF%E3%80%90%E2%96%B2%E3%80%91"><span class="nav-number">10.1.2.8.</span> <span class="nav-text">1.2.8 矩阵向量积【▲】</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-9-%E7%9F%A9%E9%98%B5%E4%B8%8E%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="nav-number">10.1.2.9.</span> <span class="nav-text">1.2.9 矩阵与矩阵乘法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#1-2-10-%E8%8C%83%E6%95%B0"><span class="nav-number">10.1.2.10.</span> <span class="nav-text">1.2.10 范数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="nav-number">10.2.</span> <span class="nav-text">2、矩阵计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="nav-number">10.3.</span> <span class="nav-text">3、自动求导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E6%96%B9%E6%B3%95"><span class="nav-number">10.3.1.</span> <span class="nav-text">3.1 基本方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E5%AE%9E%E7%8E%B0"><span class="nav-number">10.3.2.</span> <span class="nav-text">3.2 自动求导实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E7%90%86%E8%A7%A3%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E5%8F%8D%E5%90%91%E8%AE%A1%E7%AE%97"><span class="nav-number">10.3.3.</span> <span class="nav-text">3.3 理解前向计算与反向计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">11.</span> <span class="nav-text">九、线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">11.1.</span> <span class="nav-text">1、理解线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">11.1.1.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">11.2.</span> <span class="nav-text">2、基础优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="nav-number">11.3.</span> <span class="nav-text">3、线性回归的从零实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-%E6%9E%84%E9%80%A0%E4%B8%80%E4%B8%AA%E4%BA%BA%E9%80%A0%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">11.3.1.</span> <span class="nav-text">3.1 构造一个人造数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E5%A4%84%E7%90%86%E5%B0%8F%E6%89%B9%E9%87%8F"><span class="nav-number">11.3.2.</span> <span class="nav-text">3.2 处理小批量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E7%AE%97%E6%B3%95%E6%A0%B8%E5%BF%83"><span class="nav-number">11.3.3.</span> <span class="nav-text">3.3 算法核心</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0Q-amp-A"><span class="nav-number">11.3.4.</span> <span class="nav-text">3.4 从零实现Q&amp;A</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">11.4.</span> <span class="nav-text">4、线性回归简洁实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92QA"><span class="nav-number">11.5.</span> <span class="nav-text">5、线性回归QA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E3%80%81Softmax%E5%9B%9E%E5%BD%92"><span class="nav-number">12.</span> <span class="nav-text">十、Softmax回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%BC%95%E5%85%A5Softmax"><span class="nav-number">12.1.</span> <span class="nav-text">1、引入Softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-Softmax%E7%AE%80%E6%98%93%E6%B1%82%E5%AF%BC"><span class="nav-number">12.1.1.</span> <span class="nav-text">1.1 Softmax简易求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-%E4%B8%BE%E4%BE%8B%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AE%A1%E7%AE%97%E4%BC%9A%E6%AF%94%E8%BE%83%E6%96%B9%E4%BE%BF"><span class="nav-number">12.1.2.</span> <span class="nav-text">1.2 举例：为什么计算会比较方便</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-Softmax%E5%B9%BF%E4%B9%89%E6%B1%82%E5%AF%BC"><span class="nav-number">12.1.3.</span> <span class="nav-text">1.3 Softmax广义求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">12.1.4.</span> <span class="nav-text">1.4 交叉熵损失优缺点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">12.2.</span> <span class="nav-text">2、损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">12.3.</span> <span class="nav-text">3、图片分类数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81Softmax%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">12.4.</span> <span class="nav-text">4、Softmax从零开始实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81Softmax%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">12.5.</span> <span class="nav-text">5、Softmax简洁实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81Softmax%E5%9B%9E%E5%BD%92-Q-amp-A"><span class="nav-number">12.6.</span> <span class="nav-text">6、Softmax回归 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-Softlabel%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F"><span class="nav-number">12.6.1.</span> <span class="nav-text">6.1 Softlabel的训练策略？为什么有效？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-Softmax%E5%9B%9E%E5%BD%92%E4%B8%8ELogistic%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="nav-number">12.6.2.</span> <span class="nav-text">6.2 Softmax回归与Logistic回归？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E4%BA%A4%E5%8F%89%E7%86%B5%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F%E7%9B%B8%E5%AF%B9%E7%86%B5%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF%EF%BC%9F"><span class="nav-number">12.6.3.</span> <span class="nav-text">6.3 为什么用交叉熵作为损失函数？相对熵、互信息？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-%E8%BF%99%E6%A0%B7%E7%9A%84N%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%EF%BC%8C%E5%8F%AA%E8%AE%A4%E4%B8%BA%E6%9C%89%E4%B8%80%E4%B8%AA%E6%AD%A3%E7%B1%BB%E3%80%81n-1%E4%B8%AA%E8%B4%9F%E7%B1%BB%EF%BC%8C%E4%BC%9A%E4%B8%8D%E4%BC%9A%E4%B8%8D%E5%B9%B3%E8%A1%A1%EF%BC%9F"><span class="nav-number">12.6.4.</span> <span class="nav-text">6.4 这样的N分类问题，只认为有一个正类、n-1个负类，会不会不平衡？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-MSE%E7%9A%84%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0%EF%BC%9F%E5%8F%82%E8%80%83%E6%84%8F%E4%B9%89%EF%BC%9F"><span class="nav-number">12.6.5.</span> <span class="nav-text">6.5 MSE的最大似然函数？参考意义？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-6-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E9%80%9F%E5%BA%A6%E5%92%8C%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%8E%87%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">12.6.6.</span> <span class="nav-text">6.6 梯度下降的速度和学习的率的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-7-Little-Problems"><span class="nav-number">12.6.7.</span> <span class="nav-text">6.7 Little Problems</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">13.</span> <span class="nav-text">十一、多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">13.1.</span> <span class="nav-text">1、感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">13.2.</span> <span class="nav-text">2、多层感知机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E5%B8%B8%E8%A7%81%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">13.3.</span> <span class="nav-text">3、常见的激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">13.4.</span> <span class="nav-text">4、多层感知机代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0"><span class="nav-number">13.5.</span> <span class="nav-text">5、多层感知机简单实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81Q-amp-A"><span class="nav-number">13.6.</span> <span class="nav-text">6、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-Little-Problems"><span class="nav-number">13.6.1.</span> <span class="nav-text">6.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A2%9E%E5%8A%A0%E9%9A%90%E8%97%8F%E5%B1%82%E5%B1%82%E6%95%B0%EF%BC%8C%E8%80%8C%E4%B8%8D%E6%98%AF%E5%A2%9E%E5%8A%A0%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%AA%E6%95%B0%EF%BC%9F"><span class="nav-number">13.6.2.</span> <span class="nav-text">6.2 为什么要增加隐藏层层数，而不是增加神经元个数？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-ReLU%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E7%94%A8%EF%BC%9F%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%AC%E8%B4%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="nav-number">13.6.3.</span> <span class="nav-text">6.3 ReLU为什么有用？激活函数的本质是什么？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%BA%8C%E3%80%81%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E4%B8%8E%E8%BF%87-%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">14.</span> <span class="nav-text">十二、模型选择与过&#x2F;欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">14.1.</span> <span class="nav-text">1、模型选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-number">14.2.</span> <span class="nav-text">2、过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F%E5%8F%8A%E5%BD%B1%E5%93%8D"><span class="nav-number">14.2.1.</span> <span class="nav-text">模型容量及影响</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VC%E7%BB%B4"><span class="nav-number">14.2.2.</span> <span class="nav-text">VC维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VC%E7%BB%B4%E7%9A%84%E7%94%A8%E5%A4%84"><span class="nav-number">14.2.3.</span> <span class="nav-text">VC维的用处</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84VC%E7%BB%B4"><span class="nav-number">14.2.4.</span> <span class="nav-text">线性分类器的VC维</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="nav-number">14.2.5.</span> <span class="nav-text">数据复杂度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">14.2.6.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3"><span class="nav-number">14.3.</span> <span class="nav-text">3、代码理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81Q-amp-A"><span class="nav-number">14.4.</span> <span class="nav-text">4、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Little-Problems"><span class="nav-number">14.4.1.</span> <span class="nav-text">4.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%BD%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86"><span class="nav-number">14.4.2.</span> <span class="nav-text">4.2 验证数据集和训练数据体的数据清理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-3-%E5%85%B3%E4%BA%8EK%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-number">14.4.3.</span> <span class="nav-text">4.3 关于K折交叉验证</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%89%E3%80%81%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="nav-number">15.</span> <span class="nav-text">十三、权重衰退</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="nav-number">15.1.</span> <span class="nav-text">1、权重衰退</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3"><span class="nav-number">15.2.</span> <span class="nav-text">2、代码理解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="nav-number">15.2.1.</span> <span class="nav-text">2.1 从零实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">15.2.2.</span> <span class="nav-text">2.2 简洁实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81Q-amp-A"><span class="nav-number">15.3.</span> <span class="nav-text">3、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Problems"><span class="nav-number">15.3.1.</span> <span class="nav-text">3.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%82%E6%95%B0%E4%B8%8D%E5%A4%A7%EF%BC%8C%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%B0%B1%E4%BD%8E%EF%BC%9F"><span class="nav-number">15.3.2.</span> <span class="nav-text">3.2 为什么参数不大，复杂度就低？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E5%9B%9B%E3%80%81%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="nav-number">16.</span> <span class="nav-text">十四、丢弃法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81Dropout"><span class="nav-number">16.1.</span> <span class="nav-text">1、Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3-1"><span class="nav-number">16.2.</span> <span class="nav-text">2、代码理解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81Q-amp-A-1"><span class="nav-number">16.3.</span> <span class="nav-text">3、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Problems-1"><span class="nav-number">16.3.1.</span> <span class="nav-text">3.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Dropout%E5%87%BD%E6%95%B0%E8%BF%94%E5%9B%9E%E5%80%BC%E7%9B%B8%E5%85%B3"><span class="nav-number">16.3.2.</span> <span class="nav-text">3.2 Dropout函数返回值相关</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%BA%94%E3%80%81%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7%E3%80%81%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">17.</span> <span class="nav-text">十五、数值稳定性、模型初始化、激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="nav-number">17.1.</span> <span class="nav-text">1、数值稳定性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">17.2.</span> <span class="nav-text">2、模型初始化和激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-2"><span class="nav-number">17.2.1.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81Q-amp-A-2"><span class="nav-number">17.3.</span> <span class="nav-text">3、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Problems-2"><span class="nav-number">17.3.1.</span> <span class="nav-text">3.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E5%AF%B9%E4%BA%8E%E5%A4%8D%E6%9D%82%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E5%BA%94%E5%AF%B9%E5%8A%9E%E6%B3%95"><span class="nav-number">17.3.2.</span> <span class="nav-text">3.2 对于复杂数学公式的应对办法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-%E5%AE%9E%E7%8E%B0Xavier%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">17.3.3.</span> <span class="nav-text">3.3 实现Xavier初始化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E5%85%AD%E3%80%81Kaggle%E6%88%BF%E4%BB%B7%E9%A2%84%E6%B5%8B"><span class="nav-number">18.</span> <span class="nav-text">十六、Kaggle房价预测</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%83%E3%80%81%E6%9C%AA%E5%BD%92%E7%B1%BB%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB"><span class="nav-number">19.</span> <span class="nav-text">十七、未归类问题汇总</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%B2%90%E7%A5%9E%E4%BB%A3%E7%A0%81%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E4%BB%A3%E7%A0%81%EF%BC%8C%E4%B8%8D%E6%98%BE%E7%A4%BATrain-Loss"><span class="nav-number">19.1.</span> <span class="nav-text">1 沐神代码的可视化代码，不显示Train_Loss?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%B8%8E%E9%9A%90%E8%97%8F%E5%B1%82%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">19.2.</span> <span class="nav-text">2 全连接层与隐藏层的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BE%80%E6%AC%A1%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5"><span class="nav-number">20.</span> <span class="nav-text">*往次组会汇报</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-6%E7%BB%84%E4%BC%9A%E4%B8%8B%E5%91%A8%E8%AE%A1%E5%88%92%EF%BC%9A"><span class="nav-number">20.0.1.</span> <span class="nav-text">7.6组会下周计划：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-13%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5"><span class="nav-number">20.0.2.</span> <span class="nav-text">7.13组会汇报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-20%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5"><span class="nav-number">20.0.3.</span> <span class="nav-text">7.20组会汇报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-27%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5"><span class="nav-number">20.0.4.</span> <span class="nav-text">7.27组会汇报</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-10%E7%BB%84%E4%BC%9A%E6%B1%87%E6%8A%A5"><span class="nav-number">20.0.5.</span> <span class="nav-text">8.10组会汇报</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Moustache" src="/images/180-180.png"><p class="site-author-name" itemprop="name">Moustache</p><div class="site-description" itemprop="description">我是小胡子</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">79</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">36</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/hammerzer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hammerzer" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:stellar_lzu@163.com" title="E-Mail → mailto:stellar_lzu@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Chase</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">29:30</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.4" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JRehDoQ6pHXV1zKg09AMNLFt-gzGzoHsz',
      appKey     : 'cRAt4W15KiQdrIuHlQrRrtIl',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>