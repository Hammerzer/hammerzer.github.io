<!DOCTYPE html><html lang="zh-CN"><head><script src="https://lib.sinaapp.com/js/jquery/1.7.2/jquery.min.js"></script><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/180-180.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/32-32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/16-16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hammerzer.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CONTENT OUTLINE卷积网络"><meta property="og:type" content="article"><meta property="og:title" content="动手学深度学习-卷积神经网络"><meta property="og:url" content="https://hammerzer.github.io/2023/08/10/convolutional-neural-network/index.html"><meta property="og:site_name" content="Moustache&#39;s Blog"><meta property="og:description" content="CONTENT OUTLINE卷积网络"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hammerzer.github.io/.io//1-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//1-4.png"><meta property="og:image" content="https://hammerzer.github.io/.io//1-6.png"><meta property="og:image" content="https://hammerzer.github.io/.io//1-5.png"><meta property="og:image" content="https://hammerzer.github.io/.io//1-7.png"><meta property="og:image" content="https://hammerzer.github.io/.io//1-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-4.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-5.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-6.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-7.png"><meta property="og:image" content="https://hammerzer.github.io/.io//2-8.png"><meta property="og:image" content="https://hammerzer.github.io/.io//3-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//4-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//5-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//6-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//6-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//7-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//7-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//8-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//8-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//9-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//9-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//10-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//10-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//11-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//11-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//11-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//12-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//12-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//12-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//12-4.png"><meta property="og:image" content="https://hammerzer.github.io/.io//13-1.png"><meta property="article:published_time" content="2023-08-10T08:12:19.000Z"><meta property="article:modified_time" content="2025-01-19T03:19:16.971Z"><meta property="article:author" content="Moustache"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hammerzer.github.io/.io//1-1.png"><link rel="canonical" href="https://hammerzer.github.io/2023/08/10/convolutional-neural-network/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>动手学深度学习-卷积神经网络 | Moustache's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Moustache's Blog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Moustache's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">小胡子的私人空间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">78</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hammerzer.github.io/2023/08/10/convolutional-neural-network/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/180-180.png"><meta itemprop="name" content="Moustache"><meta itemprop="description" content="我是小胡子"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Moustache's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">动手学深度学习-卷积神经网络</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-08-10 16:12:19" itemprop="dateCreated datePublished" datetime="2023-08-10T16:12:19+08:00">2023-08-10</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 11:19:16" itemprop="dateModified" datetime="2025-01-19T11:19:16+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a> </span></span><span id="/2023/08/10/convolutional-neural-network/" class="post-meta-item leancloud_visitors" data-flag-title="动手学深度学习-卷积神经网络" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> <span>℃</span> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2023/08/10/convolutional-neural-network/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2023/08/10/convolutional-neural-network/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>45k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>41 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CONTENT-OUTLINE"><a href="#CONTENT-OUTLINE" class="headerlink" title="CONTENT OUTLINE"></a>CONTENT OUTLINE</h2><p><span style="background:#52e6fc"><i class="fas fa-star-half-alt" style="margin-right:10px"></i>卷积网络</span></p><span id="more"></span><h2 id="〇、目录"><a href="#〇、目录" class="headerlink" title="〇、目录"></a>〇、目录</h2><ul><li>Pytorch神经网络基础</li><li>卷积神经网络【卷积层 | 填充与步幅 | 通道 |池化层】</li><li>经典卷积神经网络【LeNet / AlexNet / VGG / NiN / GoogLeNet / ResNet】</li><li>批量归一化BN</li><li>竞赛</li></ul><br><h2 id="一、Pytorch神经网络基础"><a href="#一、Pytorch神经网络基础" class="headerlink" title="一、Pytorch神经网络基础"></a>一、Pytorch神经网络基础</h2><h3 id="1、模型构造"><a href="#1、模型构造" class="headerlink" title="1、模型构造"></a>1、模型构造</h3><h4 id="层和块"><a href="#层和块" class="headerlink" title="层和块"></a>层和块</h4><p>回顾一下多层感知机</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">20</span>)</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[-<span class="number">0.0592</span>,  <span class="number">0.0558</span>, -<span class="number">0.2263</span>, -<span class="number">0.1218</span>, -<span class="number">0.1035</span>,  <span class="number">0.0366</span>, -<span class="number">0.0849</span>, -<span class="number">0.0338</span>, <span class="number">0.1567</span>,  <span class="number">0.0243</span>],</span><br><span class="line">            [-<span class="number">0.0824</span>,  <span class="number">0.0313</span>, -<span class="number">0.1744</span>, -<span class="number">0.0658</span>, -<span class="number">0.1209</span>,  <span class="number">0.0315</span>, -<span class="number">0.0919</span>, -<span class="number">0.0731</span>, <span class="number">0.0743</span>, -<span class="number">0.0659</span>]],             grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p><code>nn.Sequential</code>定义了一种特殊的<code>Module</code>【任何神经网络层都应该是<code>Module</code>的子类】</p><h4 id="自定义块"><a href="#自定义块" class="headerlink" title="自定义块"></a>自定义块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):   <span class="comment"># MLP可继承 nn.Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 实现前向计算</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.out(F.relu(<span class="variable language_">self</span>.hidden(X)))</span><br></pre></td></tr></table></figure><p>实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = MLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[-<span class="number">0.1112</span>, -<span class="number">0.0417</span>,  <span class="number">0.1602</span>, -<span class="number">0.0653</span>, -<span class="number">0.1528</span>,  <span class="number">0.0481</span>,  <span class="number">0.4155</span>,  <span class="number">0.0348</span>, -<span class="number">0.0357</span>, -<span class="number">0.0241</span>],</span><br><span class="line">            [-<span class="number">0.1352</span>, -<span class="number">0.0162</span>,  <span class="number">0.1379</span>,  <span class="number">0.0162</span>, -<span class="number">0.1218</span>,  <span class="number">0.0981</span>,  <span class="number">0.3939</span>,  <span class="number">0.0406</span>, -<span class="number">0.0234</span>, -<span class="number">0.1018</span>]],                 grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure><h4 id="顺序块"><a href="#顺序块" class="headerlink" title="顺序块"></a>顺序块</h4><blockquote><p>实现与<code>nn.Sequential</code>一样的功能</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MySequential</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> args:</span><br><span class="line">            <span class="variable language_">self</span>._modules[block] = block</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">for</span> block <span class="keyword">in</span> <span class="variable language_">self</span>._modules.values():</span><br><span class="line">            X = block(X)</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line"></span><br><span class="line">net = MySequential(nn.Linear(<span class="number">20</span>, <span class="number">256</span>), nn.ReLU(), nn.Linear(<span class="number">256</span>, <span class="number">10</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[-<span class="number">0.1840</span>,  <span class="number">0.0114</span>,  <span class="number">0.1589</span>,  <span class="number">0.0729</span>, -<span class="number">0.1335</span>,  <span class="number">0.0517</span>,  <span class="number">0.0956</span>, -<span class="number">0.1545</span>, <span class="number">0.0713</span>,  <span class="number">0.1012</span>],</span><br><span class="line">        [-<span class="number">0.1319</span>,  <span class="number">0.0737</span>,  <span class="number">0.1547</span>,  <span class="number">0.1419</span>, -<span class="number">0.1117</span>, -<span class="number">0.0313</span>, -<span class="number">0.0084</span>, -<span class="number">0.0456</span>, <span class="number">0.0429</span>,  <span class="number">0.1718</span>]], </span><br><span class="line">           grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p>在正向传播函数中执行代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FixedHiddenMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 随机梯度，不参加梯度计算</span></span><br><span class="line">        <span class="variable language_">self</span>.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自定义执行操作</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = <span class="variable language_">self</span>.linear(X)</span><br><span class="line">        <span class="comment"># mm为矩阵乘法</span></span><br><span class="line">        X = F.relu(torch.mm(X, <span class="variable language_">self</span>.rand_weight) + <span class="number">1</span>)</span><br><span class="line">        X = <span class="variable language_">self</span>.linear(X)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> X.<span class="built_in">abs</span>().<span class="built_in">sum</span>() &gt; <span class="number">1</span>:</span><br><span class="line">            X /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> X.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">net = FixedHiddenMLP()</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor(<span class="number">0.2000</span>, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure><blockquote><p>🔺可以通过继承<code>nn.Module</code>的方法，可以比<code>nn.Sequential</code> 更灵活地做前向计算</p><p>当然，反向计算不需要定义，都是自动求导</p></blockquote><p>混合搭配各种组合块的方法【嵌套使用】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NestMLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.net = nn.Sequential(nn.Linear(<span class="number">20</span>, <span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                                 nn.Linear(<span class="number">64</span>, <span class="number">32</span>), nn.ReLU())</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(<span class="number">32</span>, <span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linear(<span class="variable language_">self</span>.net(X))</span><br><span class="line"></span><br><span class="line">chimera = nn.Sequential(NestMLP(), nn.Linear(<span class="number">16</span>, <span class="number">20</span>), FixedHiddenMLP())</span><br><span class="line">chimera(X)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    tensor(-<span class="number">0.1374</span>, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure><br><h3 id="2、参数管理"><a href="#2、参数管理" class="headerlink" title="2、参数管理"></a>2、参数管理</h3><p>我们首先关注具有单隐藏层的多层感知机</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">X = torch.rand(size=(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[-<span class="number">0.5876</span>],</span><br><span class="line">        [-<span class="number">0.5556</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><h4 id="参数访问"><a href="#参数访问" class="headerlink" title="参数访问"></a>参数访问</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].state_dict())</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    OrderedDict([(<span class="string">&#x27;weight&#x27;</span>, tensor([[ <span class="number">0.0181</span>,  <span class="number">0.0557</span>,  <span class="number">0.0219</span>, -<span class="number">0.3431</span>,  <span class="number">0.1738</span>, -<span class="number">0.0249</span>,  <span class="number">0.1345</span>, -<span class="number">0.2593</span>]])), (<span class="string">&#x27;bias&#x27;</span>, tensor([-<span class="number">0.3221</span>]))])</span><br></pre></td></tr></table></figure><p>目标参数【<code>Parameter</code> 被定义为是可以被优化的参数】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(net[<span class="number">2</span>].bias))</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].bias.data)      <span class="comment">#.data:参数本身</span></span><br><span class="line"></span><br><span class="line">net[<span class="number">2</span>].weight.grad == <span class="literal">None</span>  <span class="comment">#.grad:访问梯度</span></span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    &lt;<span class="keyword">class</span> <span class="string">&#x27;torch.nn.parameter.Parameter&#x27;</span>&gt;</span><br><span class="line">    Parameter containing:</span><br><span class="line">    tensor([-<span class="number">0.3221</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">    tensor([-<span class="number">0.3221</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>一次性访问所有参数【全部拿出 | 按层 | 按名 拿出参数】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters()])</span><br><span class="line"><span class="built_in">print</span>(*[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters()])</span><br><span class="line">net.state_dict()[<span class="string">&#x27;2.bias&#x27;</span>].data</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (<span class="string">&#x27;weight&#x27;</span>, torch.Size([<span class="number">8</span>, <span class="number">4</span>])) (<span class="string">&#x27;bias&#x27;</span>, torch.Size([<span class="number">8</span>]))</span><br><span class="line">    </span><br><span class="line">    (<span class="string">&#x27;0.weight&#x27;</span>, torch.Size([<span class="number">8</span>, <span class="number">4</span>])) (<span class="string">&#x27;0.bias&#x27;</span>, torch.Size([<span class="number">8</span>])) (<span class="string">&#x27;2.weight&#x27;</span>, torch.Size([<span class="number">1</span>, <span class="number">8</span>])) (<span class="string">&#x27;2.bias&#x27;</span>, torch.Size([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    tensor([-<span class="number">0.3221</span>])</span><br></pre></td></tr></table></figure><blockquote><p>ReLU是没有参数的</p></blockquote><p>从嵌套块收集参数</p><blockquote><p><code>add_module()</code>函数创建的<code>Sequential</code>，<strong>唯一区别</strong>在于可以指定层的名称</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">block1</span>():</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">4</span>),</span><br><span class="line">                         nn.ReLU())</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">block2</span>():</span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        net.add_module(<span class="string">f&#x27;block <span class="subst">&#123;i&#125;</span>&#x27;</span>, block1())</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">rgnet = nn.Sequential(block2(), nn.Linear(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">rgnet(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">0.2755</span>],</span><br><span class="line">            [<span class="number">0.2755</span>]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p>我们已经设计了网络，让我们看看它是如何组织的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(rgnet)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">  Sequential(</span><br><span class="line">  (<span class="number">0</span>): Sequential(</span><br><span class="line">    (block <span class="number">0</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">1</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">2</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">    (block <span class="number">3</span>): Sequential(</span><br><span class="line">      (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">8</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">1</span>): ReLU()</span><br><span class="line">      (<span class="number">2</span>): Linear(in_features=<span class="number">8</span>, out_features=<span class="number">4</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (<span class="number">3</span>): ReLU()</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)  </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rgnet[<span class="number">0</span>][<span class="number">1</span>][<span class="number">0</span>].bias.data</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([-<span class="number">0.0178</span>,  <span class="number">0.4432</span>,  <span class="number">0.4543</span>, -<span class="number">0.3561</span>, -<span class="number">0.0851</span>, -<span class="number">0.4227</span>,  <span class="number">0.3945</span>, -<span class="number">0.4169</span>])</span><br></pre></td></tr></table></figure><h4 id="内置初始化"><a href="#内置初始化" class="headerlink" title="内置初始化"></a>内置初始化</h4><p>随机初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_normal</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.normal_(m.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_normal)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (tensor([-<span class="number">0.0013</span>,  <span class="number">0.0037</span>, -<span class="number">0.0172</span>,  <span class="number">0.0156</span>]), tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure><blockquote><p>参数<code>m</code>表示<code>Module</code></p><p><code>nn.init.normal_()</code>：下划线表示为替换函数，即不返回值，而是替换参数</p><p><code>nn.init</code>中有大量初始化函数</p><p>🔺<code>net.apply(init_normal)</code>：将net中所有的Module作为参数<code>m</code>传入 <code>init_mormal(m)</code>，遍历初始化一遍参数</p></blockquote><p>对某些块初始话为常量【API的角度可以做，但不能这么干】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_constant</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">        nn.init.zeros_(m.bias)</span><br><span class="line"></span><br><span class="line">net.apply(init_constant)</span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>], net[<span class="number">0</span>].bias.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), tensor(<span class="number">0.</span>))</span><br></pre></td></tr></table></figure><p>对某些块应用不同的初始化方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">xavier</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.xavier_uniform_(m.weight)</span><br><span class="line">        <span class="comment">#nn.init.xavier_normal_(m.weight)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_42</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        nn.init.constant_(m.weight, <span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对不同的层使用不同的初始化函数</span></span><br><span class="line">net[<span class="number">0</span>].apply(xavier)</span><br><span class="line">net[<span class="number">2</span>].apply(init_42)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([-<span class="number">0.0453</span>, -<span class="number">0.3169</span>,  <span class="number">0.3091</span>, -<span class="number">0.2077</span>])</span><br><span class="line">    tensor([[<span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>, <span class="number">42.</span>]])</span><br></pre></td></tr></table></figure><h4 id="自定义初始化"><a href="#自定义初始化" class="headerlink" title="自定义初始化"></a>自定义初始化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_init</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Init&quot;</span>,</span><br><span class="line">            *[(name, param.shape) <span class="keyword">for</span> name, param <span class="keyword">in</span> m.named_parameters()][<span class="number">0</span>])</span><br><span class="line">        nn.init.uniform_(m.weight, -<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        m.weight.data *= m.weight.data.<span class="built_in">abs</span>() &gt;= <span class="number">5</span></span><br><span class="line">        <span class="comment"># 绝对值小于5：置0   （这种做法应该没用）</span></span><br><span class="line"></span><br><span class="line">net.apply(my_init)</span><br><span class="line">net[<span class="number">0</span>].weight[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    Init weight torch.Size([<span class="number">8</span>, <span class="number">4</span>])</span><br><span class="line">    Init weight torch.Size([<span class="number">1</span>, <span class="number">8</span>])</span><br><span class="line">    </span><br><span class="line">    tensor([[ <span class="number">0.0000</span>, -<span class="number">9.6254</span>,  <span class="number">0.0000</span>,  <span class="number">6.0600</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">7.9085</span>, -<span class="number">0.0000</span>, -<span class="number">0.0000</span>]], grad_fn=&lt;SliceBackward&gt;)</span><br></pre></td></tr></table></figure><p>直接修改权重参数，也是可以的实现的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data[:] += <span class="number">1</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">42</span></span><br><span class="line">net[<span class="number">0</span>].weight.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([<span class="number">42.0000</span>, -<span class="number">8.6254</span>,  <span class="number">1.0000</span>,  <span class="number">7.0600</span>])</span><br></pre></td></tr></table></figure><h4 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h4><p>【<strong>应用：在不同网络之间共享权重的方法</strong>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">shared = nn.Linear(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">8</span>), nn.ReLU(), shared, nn.ReLU(), shared,</span><br><span class="line">                    nn.ReLU(), nn.Linear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(X)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line">net[<span class="number">2</span>].weight.data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">2</span>].weight.data[<span class="number">0</span>] == net[<span class="number">4</span>].weight.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br><span class="line">    tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure><br><h3 id="3、自定义层"><a href="#3、自定义层" class="headerlink" title="3、自定义层"></a>3、自定义层</h3><p>构造一个没有任何参数的自定义层【自定义层和自定义网络层没有本质区别，都是 <code>nn.Module</code> 的子类】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CenteredLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment">#python3会自动继承父类</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> X - X.mean()</span><br><span class="line"></span><br><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([-<span class="number">2.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure><p>将层作为组件合并到构建更复杂的模型中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">8</span>, <span class="number">128</span>), CenteredLayer())</span><br><span class="line"></span><br><span class="line">Y = net(torch.rand(<span class="number">4</span>, <span class="number">8</span>))</span><br><span class="line">Y.mean()</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor(<span class="number">4.6566e-10</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure><p>带参数的层【<strong>必须要使用 <code>nn.Parameter</code> 构造参数</strong>】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLinear</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_units, units</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.randn(in_units, units))</span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.randn(units,)) <span class="comment"># 一般也是torch.zero()</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear = torch.matmul(X, <span class="variable language_">self</span>.weight.data) + <span class="variable language_">self</span>.bias.data</span><br><span class="line">        <span class="keyword">return</span> F.relu(linear)</span><br><span class="line"></span><br><span class="line">linear = MyLinear(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">linear.weight</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    Parameter containing:</span><br><span class="line">    tensor([[-<span class="number">0.5359</span>,  <span class="number">0.1707</span>,  <span class="number">0.1999</span>],</span><br><span class="line">            [-<span class="number">1.7083</span>,  <span class="number">0.9041</span>,  <span class="number">0.1031</span>],</span><br><span class="line">            [-<span class="number">0.9424</span>, -<span class="number">0.7027</span>,  <span class="number">0.7929</span>],</span><br><span class="line">            [ <span class="number">0.3570</span>, -<span class="number">0.8159</span>, -<span class="number">1.0664</span>],</span><br><span class="line">            [-<span class="number">2.1450</span>, -<span class="number">0.1423</span>,  <span class="number">1.0392</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>使用自定义层直接执行正向传播计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linear(torch.rand(<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">1.5329</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">1.4804</span>]])</span><br></pre></td></tr></table></figure><p>使用自定义层构建模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(MyLinear(<span class="number">64</span>, <span class="number">8</span>), MyLinear(<span class="number">8</span>, <span class="number">1</span>))</span><br><span class="line">net(torch.rand(<span class="number">2</span>, <span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><br><h3 id="4、读写文件"><a href="#4、读写文件" class="headerlink" title="4、读写文件"></a>4、读写文件</h3><h4 id="加载与保存举例"><a href="#加载与保存举例" class="headerlink" title="加载与保存举例"></a>加载与保存举例</h4><p>加载和保存<strong>张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x-file&#x27;</span>)</span><br><span class="line">x2</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>存储一个<strong>张量列表</strong>，然后把它们读回内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">x2, y2 = torch.load(<span class="string">&#x27;x-files&#x27;</span>)</span><br><span class="line">(x2, y2)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]))</span><br></pre></td></tr></table></figure><p>写入或读取<strong>从字符串映射到张量</strong>的<strong>字典</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mydict = &#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;</span><br><span class="line">torch.save(mydict, <span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2 = torch.load(<span class="string">&#x27;mydict&#x27;</span>)</span><br><span class="line">mydict2</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    &#123;<span class="string">&#x27;x&#x27;</span>: tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), <span class="string">&#x27;y&#x27;</span>: tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])&#125;</span><br></pre></td></tr></table></figure><h4 id="加载和保存模型参数"><a href="#加载和保存模型参数" class="headerlink" title="加载和保存模型参数"></a>加载和保存模型参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MLP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.hidden = nn.Linear(<span class="number">20</span>, <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.output(F.relu(<span class="variable language_">self</span>.hidden(x)))</span><br><span class="line"></span><br><span class="line">net = MLP()</span><br><span class="line">X = torch.randn(size=(<span class="number">2</span>, <span class="number">20</span>))</span><br><span class="line">Y = net(X)</span><br></pre></td></tr></table></figure><blockquote><p>此处，实例化 <code>MLP</code> 之后，直接 <code>net(X)</code>，实际上是 <code>net.__call__(X)</code>，是python里自己的映射，也可以使用 <code>net.forward(X)</code></p></blockquote><p>将模型的参数存储为一个叫做“<code>mlp.params</code>”的文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(net.state_dict(), <span class="string">&#x27;mlp.params&#x27;</span>)</span><br></pre></td></tr></table></figure><p>实例化了原始多层感知机模型的一个备份。 直接读取文件中存储的参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">clone = MLP()   <span class="comment"># MLP的定义需要提前声明！</span></span><br><span class="line">clone.load_state_dict(torch.load(<span class="string">&#x27;mlp.params&#x27;</span>))</span><br><span class="line">clone.<span class="built_in">eval</span>()    <span class="comment"># 执行</span></span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    MLP(</span><br><span class="line">      (hidden): Linear(in_features=<span class="number">20</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">      (output): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试参数是否相同</span></span><br><span class="line">Y_clone = clone(X)</span><br><span class="line">Y_clone == Y</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">            [<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]])</span><br></pre></td></tr></table></figure><br><h3 id="5、Q-amp-A"><a href="#5、Q-amp-A" class="headerlink" title="5、Q&amp;A"></a>5、Q&amp;A</h3><h4 id="5-1-Little-Problems"><a href="#5-1-Little-Problems" class="headerlink" title="5.1 Little Problems"></a>5.1 Little Problems</h4><ul><li>Xavier初始化、kaiming初始化，都是为了让训练的每一层的输入输出不要在一开始就是炸掉或消失</li><li>几乎没有函数是不可导的，只是说不是处处可导。而深度学习中基本均为数值运算，求导遇到不可导点的概率极低，所以就算碰到随便给0也ok</li></ul><h4 id="5-2-内存爆炸问题"><a href="#5-2-内存爆炸问题" class="headerlink" title="5.2 内存爆炸问题"></a>5.2 内存爆炸问题</h4><p>🔺在将类别转换成伪变量的时候，内存炸掉怎么办？</p><p>之前讲过如果特征是字符串，可以用<code>one-hot</code>编码，生成一个列表，当类别很多时，这个列表将极其庞大，内存可能会爆炸。</p><p>解决办法有两个。一是使用稀疏矩阵存储；二是就不应该使用<code>one-hot</code>，太低效了，当具有很多很多类别的时候，应该通过别的方法。比如竞赛中的地址项，可以做 <code>backforward</code>，即将地址中的每个词拿出来，而不是将整个句子拿出来</p><br><h3 id="6、GPU"><a href="#6、GPU" class="headerlink" title="6、GPU"></a>6、GPU</h3><blockquote><p><strong>什么是cuda？</strong></p><p>统一计算设备架构（<strong>Compute Unified Device Architecture</strong>, CUDA），是由NVIDIA推出的通用并行计算架构。解决的是用更加廉价的设备资源，实现更高效的并行计算。</p><p>CUDA是一种由NVIDIA推出的计算平台和编程模型，它使开发人员可以在NVIDIA GPU上实现高性能计算应用程序。CUDA主要用于加速计算密集型任务，例如科学计算、深度学习、图形处理和密码学等领域。CUDA提供了一组API和工具，包括CUDA C/C++编译器、CUDA库、CUDA驱动程序和CUDA Toolkit等，使开发人员可以使用C/C++编程语言来编写高效的GPU应用程序。</p></blockquote><h4 id="GPU与CUDA"><a href="#GPU与CUDA" class="headerlink" title="GPU与CUDA"></a>GPU与CUDA</h4><p>查看显卡信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!nvidia-smi</span><br><span class="line"><span class="comment"># 本人的显示 11.1</span></span><br></pre></td></tr></table></figure><p><img src="/.io//1-1.png"></p><blockquote><p>如果GPU利用率低于50%，可能是模型不优</p><p>以下假设有3块GPU可用，即编号0/1/2</p></blockquote><p>计算设备默认使用CPU，如需使用GPU计算，需要手动指定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#默认使用CPU; 第0个GPU; 第1个GPU</span></span><br><span class="line">torch.device(<span class="string">&#x27;cpu&#x27;</span>), torch.cuda.device(<span class="string">&#x27;cuda&#x27;</span>), torch.cuda.device(<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>),</span><br><span class="line">     &lt;torch.cuda.device at <span class="number">0x7f723468cdc0</span>&gt;,</span><br><span class="line">     &lt;torch.cuda.device at <span class="number">0x7f7234655310</span>&gt;)</span><br></pre></td></tr></table></figure><p>查询可用gpu的数量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    <span class="number">2</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>cuda常见用法：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available() <span class="comment"># 查看是否有可用GPU</span></span><br><span class="line">torch.cuda.device_count() <span class="comment"># 查看GPU数量</span></span><br><span class="line">torch.cuda.get_device_capability(device) <span class="comment"># 查看指定GPU容量</span></span><br><span class="line">torch.cuda.get_device_name(device) <span class="comment"># 查看指定GPU名称</span></span><br><span class="line">torch.cuda.empty_cache() <span class="comment"># 清空程序占用的GPU资源</span></span><br><span class="line">torch.cuda.manual_seed(seed) <span class="comment"># 设置随机种子</span></span><br><span class="line">torch.cuda.manual_seed_all(seed) <span class="comment"># 设置随机种子</span></span><br><span class="line">torch.cuda.get_device_properties(i) <span class="comment"># i为第几张卡，显示该卡的详细信息</span></span><br></pre></td></tr></table></figure></blockquote><p>这两个函数允许我们在请求的GPU不存在的情况下运行代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">try_gpu</span>(<span class="params">i=<span class="number">0</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;如果存在，则返回gpu(i)，否则返回cpu()。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.device_count() &gt;= i + <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">try_all_gpus</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。&quot;&quot;&quot;</span></span><br><span class="line">    devices = [</span><br><span class="line">        torch.device(<span class="string">f&#x27;cuda:<span class="subst">&#123;i&#125;</span>&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(torch.cuda.device_count())]</span><br><span class="line">    <span class="keyword">return</span> devices <span class="keyword">if</span> devices <span class="keyword">else</span> [torch.device(<span class="string">&#x27;cpu&#x27;</span>)]</span><br><span class="line"></span><br><span class="line">try_gpu(), try_gpu(<span class="number">10</span>), try_all_gpus()</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>),</span><br><span class="line">     device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>),</span><br><span class="line">     [device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>), device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">1</span>)])</span><br></pre></td></tr></table></figure><p>查询张量所在的设备</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">x.device</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    device(<span class="built_in">type</span>=<span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure><p>存储在GPU上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu())</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br></pre></td></tr></table></figure><p>第二个GPU上创建一个随机张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Y = torch.rand(<span class="number">2</span>, <span class="number">3</span>, device=try_gpu(<span class="number">1</span>))</span><br><span class="line">Y</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">0.9333</span>, <span class="number">0.8735</span>, <span class="number">0.7784</span>],</span><br><span class="line">               [<span class="number">0.3453</span>, <span class="number">0.5509</span>, <span class="number">0.3475</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><p>要计算<code>X + Y</code>，我们需要决定在哪里执行这个操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Z = X.cuda(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"><span class="built_in">print</span>(Z)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">    tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br></pre></td></tr></table></figure><p>现在<strong>数据在同一个GPU上（<code>Z</code>和<code>Y</code>都在），我们可以将它们相加</strong>【必要条件】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Y + Z</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">1.9333</span>, <span class="number">1.8735</span>, <span class="number">1.7784</span>],</span><br><span class="line">            [<span class="number">1.3453</span>, <span class="number">1.5509</span>, <span class="number">1.3475</span>]], device=<span class="string">&#x27;cuda:1&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">Z.cuda(<span class="number">1</span>) <span class="keyword">is</span> Z</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    <span class="literal">True</span></span><br></pre></td></tr></table></figure><h4 id="神经网络与GPU"><a href="#神经网络与GPU" class="headerlink" title="神经网络与GPU"></a>神经网络与GPU</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>)) <span class="comment"># 在CPU上创建数据</span></span><br><span class="line">net = net.to(device=try_gpu()) <span class="comment"># 迁移到其他</span></span><br><span class="line"></span><br><span class="line">net(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    net = nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">    net = net.to(device=try_gpu())</span><br><span class="line"></span><br><span class="line">    net(X)</span><br></pre></td></tr></table></figure><p>确认模型参数存储在同一个GPU上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">0</span>].weight.data.device</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    device(<span class="built_in">type</span>=<span class="string">&#x27;cuda&#x27;</span>, index=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><blockquote><p>Mac设备可以尝试 <code>X = torch.ones(2,3,device = torch.device(&#39;mps&#39;))</code></p></blockquote><br><blockquote><p>GPU购买</p><ul><li>显存</li><li>计算能力</li><li>Nvidia生态较好</li></ul></blockquote><h3 id="7、GPU：Q-amp-A"><a href="#7、GPU：Q-amp-A" class="headerlink" title="7、GPU：Q&amp;A"></a>7、GPU：Q&amp;A</h3><h4 id="7-1-Little-Problems"><a href="#7-1-Little-Problems" class="headerlink" title="7.1 Little Problems"></a>7.1 Little Problems</h4><ul><li>尽量在GPU上做运算</li><li>如果读入数据的时间 &gt; GPU运算时间，一般使用CPU；相反，如果读入数据的时间 &lt; GPU运算时间，一般使用GPU</li></ul><h4 id="7-2-device-count-为0"><a href="#7-2-device-count-为0" class="headerlink" title="7.2 device_count()为0"></a>7.2 device_count()为0</h4><p>两个原因。一是可能cuda没装好；二是可能装的是CPU版本的pytorch，但需要GPU版本的</p><br><h3 id="8、CUDA下载安装"><a href="#8、CUDA下载安装" class="headerlink" title="8、CUDA下载安装"></a>8、CUDA下载安装</h3><blockquote><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chen565884393/article/details/127905428">CUDA安装及环境配置</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41264055/article/details/132092447">GPU版PyTorch对应安装教程</a>【更优】</p></blockquote><h4 id="8-1-三个基本概念"><a href="#8-1-三个基本概念" class="headerlink" title="8.1 三个基本概念"></a>8.1 三个基本概念</h4><ul><li><p><a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E7%AE%97%E5%8A%9B&spm=1001.2101.3001.7020">算力</a>：需要先知道你的显卡，之后根据官网表格进行对应，得到算力</p><p><img src="/.io//1-4.png"></p></li><li><p><code>CUDA driver version</code>：电脑上显卡的硬件驱动</p></li><li><p><code>CUDA runtime version</code>：pytorch官网上所显示的CUDA版本号</p></li></ul><br><h4 id="8-2-操作"><a href="#8-2-操作" class="headerlink" title="8.2 操作"></a>8.2 操作</h4><h5 id="①-算力转换【官网查询-神仙相助】"><a href="#①-算力转换【官网查询-神仙相助】" class="headerlink" title="① 算力转换【官网查询 | 神仙相助】"></a>① 算力转换【<a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-gpus#collapseOne">官网查询</a> | <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_15615505/article/details/108499962">神仙相助</a>】</h5><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查询得到</span></span><br><span class="line"><span class="attribute">GeForce</span> GTX <span class="number">1050</span>    <span class="number">6</span>.<span class="number">1</span></span><br></pre></td></tr></table></figure><h5 id="②-确定CUDA版本所支持的算力"><a href="#②-确定CUDA版本所支持的算力" class="headerlink" title="② 确定CUDA版本所支持的算力"></a>② 确定CUDA版本所支持的算力</h5><blockquote><p>传送门：<a target="_blank" rel="noopener" href="https://blog.csdn.net/WenZhaoYang123/article/details/130849550">不同版本CUDA支持的算力</a></p></blockquote><p>我这个是<strong>6.1</strong>的算力，对应可以选择<strong>11.0</strong>和<strong>11.1 – 11.4</strong>的<strong>CUDA runtime version</strong></p><p><img src="/.io//1-6.png"></p><h5 id="③-查看自己的CUDA-driver-version"><a href="#③-查看自己的CUDA-driver-version" class="headerlink" title="③ 查看自己的CUDA driver version"></a>③ 查看自己的CUDA driver version</h5><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 查看CUDA driver version</span><br><span class="line">nvidia-smi.exe</span><br><span class="line"></span><br><span class="line"># 查看已安装CUDA的版本</span><br><span class="line">NVCC -V  </span><br></pre></td></tr></table></figure><p><img src="/.io//1-5.png"></p><blockquote><p>三者之间需要满足的关系：</p><ul><li><p>CUDA driver version 【11.1】 ≥ CUDA runtime version【已安装11.1】 【即：②≥③】</p></li><li><p><strong>CUDA runtime version得支持自己电脑GPU所对应的算力</strong>【即：②得支持①】</p></li></ul><p>最后选择下载了11.1版本的CUDA</p></blockquote><h5 id="④-在线安装自己的GPU版本的pytorch"><a href="#④-在线安装自己的GPU版本的pytorch" class="headerlink" title="④ 在线安装自己的GPU版本的pytorch"></a>④ 在线安装自己的GPU版本的pytorch</h5><p><code>CUDA runtime version</code>版本<code>11.1</code>找点低点的<code>torch</code>版本，例如<code>pytorch</code>为v1.2.0版本，因为这里是<code>Conda</code>进行安装的，最终确定命令为：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==<span class="number">1</span>.<span class="number">8</span>.<span class="number">0</span> torchvision==<span class="number">0</span>.<span class="number">9</span>.<span class="number">0</span> torchaudio==<span class="number">0</span>.<span class="number">8</span>.<span class="number">0</span> cudatoolkit=<span class="number">11</span>.<span class="number">1</span> -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><p>这里的<code>-c</code>是下载通道含义，<code>-c pytorch</code>表示从<code>pytorch</code>官网下载，因为是外国的服务器，一般会很慢。我们可以看到这条命令其实是下载了三个库， <code>pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0</code></p><p><strong>⭐️备选方案一：在线下载</strong></p><p>从清华源进行下载【<code>pytorch</code>和<code>torchvision</code>是一个地址，<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=cudatoolkit&spm=1001.2101.3001.7020">cudatoolkit</a>是另一个地址】</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==<span class="number">1</span>.<span class="number">8</span>.<span class="number">0</span> torchvision==<span class="number">0</span>.<span class="number">9</span>.<span class="number">0</span> -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/win-<span class="number">64</span>/ </span><br><span class="line"></span><br><span class="line">conda install cudatoolkit=<span class="number">11</span>.<span class="number">1</span> -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br></pre></td></tr></table></figure><p><strong>可能遇到的报错问题：CondaHTTPError: HTTP 403 FORBIDDEN for url</strong></p><p><img src="/.io//1-7.png"></p><p><strong>尝试解决方法</strong></p><p>最好的办法就是早上搞，家人们，尤其是早上6点多，网速飞起！像这种下载超时错误，一般都能解决！</p><p>重置配置文件：<code>conda config --remove-key channels</code>。添加清华源：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https:<span class="regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="regexp">/anaconda/</span>pkgs<span class="regexp">/free/</span></span><br><span class="line">conda config --add channels https:<span class="regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="regexp">/anaconda/</span>pkgs<span class="regexp">/main/</span></span><br><span class="line">conda config --add channels https:<span class="regexp">//mi</span>rrors.tuna.tsinghua.edu.cn<span class="regexp">/anaconda/</span>cloud<span class="regexp">/pytorch/</span></span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p>之后再次运行即可，也可参考博文：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41264055/article/details/120598436?ops_request_misc=&request_id=45397f0ee594444395fca7d25b5b8900&biz_id=&utm_medium=distribute.pc_search_result.none-task-blog-2~blog~koosearch~default-1-120598436-null-null.268%5Ev1%5Econtrol&utm_term=%E4%B8%8B%E8%BD%BD%E6%85%A2&spm=1018.2226.3001.4450">Anaconda中下载速度贼慢？</a></p><p><strong>⭐️备选方案二：本地安装PyTroch</strong></p><p>若在线安装老是出问题多半是服务器响应超时，那就直接本地下载安装得了。例如，通过torch官网找到的命令为：</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==<span class="number">1</span>.<span class="number">8</span>.<span class="number">0</span> torchvision==<span class="number">0</span>.<span class="number">9</span>.<span class="number">0</span> torchaudio==<span class="number">0</span>.<span class="number">8</span>.<span class="number">0</span> cudatoolkit=<span class="number">11</span>.<span class="number">1</span> -c pytorch -c conda-forge</span><br></pre></td></tr></table></figure><blockquote><p>下载链接<a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/cu111/torch_stable.html">传送门</a></p><ul><li>cu111：CUDA11.1</li><li>cp38：python3.8版本 需要根直接的python版本号一致</li></ul><p>找到对应的windows64位进行下载即可，python版本按实际情况进行下载即可</p></blockquote><p>下载完成之后，在环境空间下输入下面的命令进行本地安装</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 安装torch-<span class="number">1</span>.<span class="number">2</span>.<span class="number">0</span></span><br><span class="line">pip install D:\desktop\torch-<span class="number">1</span>.<span class="number">2</span>.<span class="number">0</span>-cp36-cp36m-win_amd64.whl</span><br><span class="line"></span><br><span class="line"># 再安装torchvision0.<span class="number">4</span>.<span class="number">0</span></span><br><span class="line">pip install D:\desktop\torchvision-<span class="number">0</span>.<span class="number">4</span>.<span class="number">0</span>-cp36-cp36m-win_amd64.whl</span><br></pre></td></tr></table></figure><h4 id="8-3-验证"><a href="#8-3-验证" class="headerlink" title="8.3 验证"></a>8.3 验证</h4><p>环境空间下依次输入以下命令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><p>若返回<code>True</code>表示安装GPU版本成功；<code>quit()</code>或<code>Ctrl+Z</code>退出编辑器</p><h4 id="8-4-额外操作"><a href="#8-4-额外操作" class="headerlink" title="8.4 额外操作"></a>8.4 额外操作</h4><h5 id="8-4-1-确定CUDA版本支持的VS版本"><a href="#8-4-1-确定CUDA版本支持的VS版本" class="headerlink" title="8.4.1 确定CUDA版本支持的VS版本"></a>8.4.1 确定CUDA版本支持的VS版本</h5><p>查询官方安装文档，这里给出<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/11.6.0/cuda-installation-guide-microsoft-windows/index.html">文档地址</a>，可知，支持的VS版本如下表：</p><p><img src="/.io//1-3.png"></p><h5 id="8-4-2-确定CUDA版本对应的cuDNN版本"><a href="#8-4-2-确定CUDA版本对应的cuDNN版本" class="headerlink" title="8.4.2 确定CUDA版本对应的cuDNN版本"></a>8.4.2 确定CUDA版本对应的cuDNN版本</h5><p>在CUDNN<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">下载页面</a>，我的cuda是11.1，这里就选择cuDNN版本的for CUDA11.1版本即可</p><p>三个安装版本都确定好了，安装的顺序是先安装vs2019、CUDA11.1、然后是cuDNNV10.0【如果 你安装的是别的版本，注意它们之间的版本对应就行，套路是一样的】</p><h5 id="8-4-3-测试CUDA"><a href="#8-4-3-测试CUDA" class="headerlink" title="8.4.3 测试CUDA"></a>8.4.3 测试CUDA</h5><p>以下是在 <code>D:\Software\NVIDIA-CUDA\extras\demo_suite</code> 文件夹下的测试Demo</p><ul><li>查询一下本机的GPU设备：<code>deviceQuery</code></li><li>测试带宽： <code>bandwidthTest</code></li><li>结果都是<code>PASS</code>，说明一切运行正常</li></ul><h5 id="8-4-4-安装CUDNN"><a href="#8-4-4-安装CUDNN" class="headerlink" title="8.4.4 安装CUDNN"></a>8.4.4 安装CUDNN</h5><p>下载安装包，在NVIDIA官方网站即可下载，地址<a target="_blank" rel="noopener" href="https://developer.nvidia.com/rdp/cudnn-archive">传送门</a>【如果没有NVIDIA开发者账号的话，需要按照提示注册一个，再登录即可下载】</p><p>下载之后直接解压缩，完成后点击去你能看到如下三个文件夹</p><p>把这三个文件夹的文件分别拷贝到CUDA安装目录对应的【<code>bin</code>、<code>include</code>、<code>lib</code>】文件夹中即可。CUDA的<code>lib</code>目录有<code>x64</code> 、<code>Win32</code>、<code>cmake</code>三个文件夹，拷到其中的<code>x64</code>这个文件夹中</p><br><h2 id="二、卷积层"><a href="#二、卷积层" class="headerlink" title="二、卷积层"></a>二、卷积层</h2><h3 id="1-从全连接到卷积"><a href="#1-从全连接到卷积" class="headerlink" title="1 从全连接到卷积"></a>1 从全连接到卷积</h3><p><img src="/.io//2-1.png"></p><br><h3 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2 卷积层"></a>2 卷积层</h3><p><img src="/.io//2-2.png"></p><p><strong>总结</strong></p><ul><li>卷积层将输入和核矩阵进行交叉相关，加上偏移后得到输出。这种方法大大减少了参数量。</li><li>核矩阵和偏移是可以学习的参数</li><li>核矩阵的大小是超参数</li><li>卷积会减小输出，应对办法很多，其中一个就是在图像外圈加一圈0</li></ul><br><h3 id="3-图像卷积"><a href="#3-图像卷积" class="headerlink" title="3 图像卷积"></a>3 图像卷积</h3><p>互相关运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算。</span></span><br><span class="line"><span class="string">       Y：目标矩阵 | K：卷积核</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))  <span class="comment"># 经卷积后的目标矩阵大小</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()   <span class="comment"># 与卷积核作点乘，再求和</span></span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证上述二维互相关运算的输出</span></span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">K = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]])</span><br><span class="line">corr2d(X, K)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">19.</span>, <span class="number">25.</span>],</span><br><span class="line">           [<span class="number">37.</span>, <span class="number">43.</span>]])</span><br></pre></td></tr></table></figure><p>实现二维卷积层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        <span class="variable language_">self</span>.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 做互相关运算</span></span><br><span class="line">        <span class="keyword">return</span> corr2d(x, <span class="variable language_">self</span>.weight) + <span class="variable language_">self</span>.bias</span><br></pre></td></tr></table></figure><p>卷积层的一个简单应用： 检测图像中不同颜色的边缘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">X[:, <span class="number">2</span>:<span class="number">6</span>] = <span class="number">0</span></span><br><span class="line">K = torch.tensor([[<span class="number">1.0</span>, -<span class="number">1.0</span>]])</span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">            [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><blockquote><p>该图像（矩阵）存在明显的边缘</p><p>1×2的卷积核<code>[1.0, -1.0]</code>：假设两个元素没有变，即没有边缘，输出为0；假设两个元素为<code>[1, 0]</code>或<code>[0, 1]</code>，那么输出为1或-1。</p></blockquote><p>输出<code>Y</code>中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Y = corr2d(X, K)</span><br><span class="line"></span><br><span class="line">Out：</span><br><span class="line">    tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">            [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">            [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">            [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">            [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">            [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p>卷积核<code>K</code>只可以检测垂直边缘</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(corr2d(X.t(), K))</span><br><span class="line"></span><br><span class="line">Out：</span><br><span class="line">    tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">            [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p><strong>学习</strong>由<code>X</code>生成<code>Y</code>的<strong>卷积核K</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 黑白通道为1，彩色通道为3【下均为1通道】</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">1</span>, <span class="number">2</span>), bias=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># X reshape 是为了加两个维度（批量大小维，通道维）</span></span><br><span class="line"><span class="comment"># conv2d的输入都是4D</span></span><br><span class="line">X = X.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">8</span>))</span><br><span class="line">Y = Y.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 迭代10轮</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    Y_hat = conv2d(X)</span><br><span class="line">    l = (Y_hat - Y)**<span class="number">2</span>  <span class="comment"># 均方误差</span></span><br><span class="line">    conv2d.zero_grad()  <span class="comment"># 梯度设置为0</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()  <span class="comment"># 求和之后求梯度</span></span><br><span class="line">    <span class="comment"># 手写梯度下降：访问梯度×3e-2</span></span><br><span class="line">    conv2d.weight.data[:] -= <span class="number">3e-2</span> * conv2d.weight.grad</span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 每两个batch print一次loss</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;batch <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;l.<span class="built_in">sum</span>():<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">Out:</span><br><span class="line">    batch <span class="number">2</span>, loss <span class="number">2.635</span></span><br><span class="line">    batch <span class="number">4</span>, loss <span class="number">0.445</span></span><br><span class="line">    batch <span class="number">6</span>, loss <span class="number">0.076</span></span><br><span class="line">    batch <span class="number">8</span>, loss <span class="number">0.013</span></span><br><span class="line">    batch <span class="number">10</span>, loss <span class="number">0.002</span></span><br></pre></td></tr></table></figure><p>所学的卷积核的权重张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">targetW = conv2d.weight.data.reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(targetW)</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    tensor([[ <span class="number">1.0109</span>, -<span class="number">0.9764</span>]])</span><br></pre></td></tr></table></figure><br><h3 id="4-Q-amp-A"><a href="#4-Q-amp-A" class="headerlink" title="4 Q&amp;A"></a>4 Q&amp;A</h3><h4 id="4-1-Little-Questions"><a href="#4-1-Little-Questions" class="headerlink" title="4.1 Little Questions"></a>4.1 Little Questions</h4><ul><li><p>做 <strong>权重变形</strong> 和 <strong>重新索引</strong> 是为了解释为什么卷积是特殊的全连接层</p></li><li><p>为什么不应该看的很远，感受野不应该越大越好吗？即卷积核大小越大越好吗？</p><p>这个问题很类似于，为什么全连接层不是一个很宽很浅的隐藏层，而是一般为很深很窄的全连接层。同样，卷积层也一样，大卷积核的效果不如小卷积核【一般用3×3，最多用5×5】</p></li><li><p>一个设计思路：同时使用两个不同尺寸的<code>Kernel</code>进行计算，然后再计算出一个更合适的<code>Kernel</code>，从而提高特征提取的性能</p></li><li><p>卷积，起源于信号处理，公式里的负号来源于傅里叶变换</p></li><li><p>卷积核的不变，体现了平移不变性；卷积核大小，体现了局部性</p></li></ul><blockquote><p>竞赛问题：迭代多次形成的Loss变化图抖动厉害，原因？</p><ol><li>数据多样性大，这样的抖动很正常【可以做平滑处理，或者增大批量大小】，抖动没关系，但光抖不下降是有问题的</li><li>学习率过大导致</li></ol></blockquote><h4 id="4-2-卷积概念的补充"><a href="#4-2-卷积概念的补充" class="headerlink" title="4.2 卷积概念的补充"></a>4.2 卷积概念的补充</h4><blockquote><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1VV411478E/?from=search&seid=1725700777641154181&vd_source=ad866fe26d18693e4132a3c33f8fba36">从“卷积”、到“图像卷积操作”、再到“卷积神经网络”，“卷积”意义的3次改变</a>：求解不断吃饭和消化之后的存量问题，该问题可以用来理解卷积公式。套入另一个例子可以用来理解对图片的卷积操作：求解蝴蝶扇动翅膀的影响力和影响力随时间衰退后的剩余影响力问题</p><p><img src="/.io//2-3.png"></p></blockquote><p><svg xmlns="http://www.w3.org/2000/svg" width="20.113ex" height="5.553ex" viewbox="0 -1500.3 8889.7 2454.6" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="display:block;margin:0 auto"><defs><path id="MJX-46-TEX-LO-222B" d="M114 -798Q132 -824 165 -824H167Q195 -824 223 -764T275 -600T320 -391T362 -164Q365 -143 367 -133Q439 292 523 655T645 1127Q651 1145 655 1157T672 1201T699 1257T733 1306T777 1346T828 1360Q884 1360 912 1325T944 1245Q944 1220 932 1205T909 1186T887 1183Q866 1183 849 1198T832 1239Q832 1287 885 1296L882 1300Q879 1303 874 1307T866 1313Q851 1323 833 1323Q819 1323 807 1311T775 1255T736 1139T689 936T633 628Q574 293 510 -5T410 -437T355 -629Q278 -862 165 -862Q125 -862 92 -831T55 -746Q55 -711 74 -698T112 -685Q133 -685 150 -700T167 -741Q167 -789 114 -798Z"/><path id="MJX-46-TEX-N-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/><path id="MJX-46-TEX-N-221E" d="M55 217Q55 305 111 373T254 442Q342 442 419 381Q457 350 493 303L507 284L514 294Q618 442 747 442Q833 442 888 374T944 214Q944 128 889 59T743 -11Q657 -11 580 50Q542 81 506 128L492 147L485 137Q381 -11 252 -11Q166 -11 111 57T55 217ZM907 217Q907 285 869 341T761 397Q740 397 720 392T682 378T648 359T619 335T594 310T574 285T559 263T548 246L543 238L574 198Q605 158 622 138T664 94T714 61T765 51Q827 51 867 100T907 217ZM92 214Q92 145 131 89T239 33Q357 33 456 193L425 233Q364 312 334 337Q285 380 233 380Q171 380 132 331T92 214Z"/><path id="MJX-46-TEX-N-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/><path id="MJX-46-TEX-I-1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/><path id="MJX-46-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/><path id="MJX-46-TEX-I-1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/><path id="MJX-46-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/><path id="MJX-46-TEX-I-1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/><path id="MJX-46-TEX-I-1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/><path id="MJX-46-TEX-I-1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mo"><use xlink:href="#MJX-46-TEX-LO-222B"/></g><g data-mml-node="TeXAtom" transform="translate(1013.4, 1088.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-46-TEX-N-2B"/></g><g data-mml-node="mi" transform="translate(778, 0)"><use xlink:href="#MJX-46-TEX-N-221E"/></g></g><g data-mml-node="TeXAtom" transform="translate(556, -896.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><use xlink:href="#MJX-46-TEX-N-2212"/></g><g data-mml-node="mi" transform="translate(778, 0)"><use xlink:href="#MJX-46-TEX-N-221E"/></g></g></g><g data-mml-node="mi" transform="translate(2487.3, 0)"><use xlink:href="#MJX-46-TEX-I-1D453"/></g><g data-mml-node="mo" transform="translate(3037.3, 0)"><use xlink:href="#MJX-46-TEX-N-28"/></g><g data-mml-node="mi" transform="translate(3426.3, 0)"><use xlink:href="#MJX-46-TEX-I-1D465"/></g><g data-mml-node="mo" transform="translate(3998.3, 0)"><use xlink:href="#MJX-46-TEX-N-29"/></g><g data-mml-node="mi" transform="translate(4387.3, 0)"><use xlink:href="#MJX-46-TEX-I-1D454"/></g><g data-mml-node="mo" transform="translate(4864.3, 0)"><use xlink:href="#MJX-46-TEX-N-28"/></g><g data-mml-node="mi" transform="translate(5253.3, 0)"><use xlink:href="#MJX-46-TEX-I-1D461"/></g><g data-mml-node="mo" transform="translate(5836.5, 0)"><use xlink:href="#MJX-46-TEX-N-2212"/></g><g data-mml-node="mi" transform="translate(6836.7, 0)"><use xlink:href="#MJX-46-TEX-I-1D465"/></g><g data-mml-node="mo" transform="translate(7408.7, 0)"><use xlink:href="#MJX-46-TEX-N-29"/></g><g data-mml-node="mi" transform="translate(7797.7, 0)"><use xlink:href="#MJX-46-TEX-I-1D451"/></g><g data-mml-node="mi" transform="translate(8317.7, 0)"><use xlink:href="#MJX-46-TEX-I-1D465"/></g></g></g></svg></p><blockquote><p>两个函数中的自变量相加可消除x，这是卷积的典型标志</p></blockquote><ul><li><p>f 函数 = 不稳定输入 = 图片输入【狭隘的、不太准确的理解，因为图片的输入怎么会不稳定呢】</p></li><li><p>g 函数 = 稳定输出 = 卷积核【狭隘的、不太准确的理解】</p></li></ul><p>拓展对到第二个例子来理解，图片的每个局部区域像素，与卷积核相乘再求和，可以理解为影响力大小，即原区域与卷积核是怎样产生影响的，<strong>而 <code>g</code> 函数规定了如何影响</strong></p><p><img src="/.io//2-4.png"></p><blockquote><p>3×3的卷积核，元素均为1/9，本质上是将9个像素点相加求平均，会让图像变得平滑、朦胧，称为<strong>平滑卷积</strong>操作。也可这样理解，如果该像素值与平均值相比较过大，那就拉低；过小，那就拉高。也就是说，间接规定了周围的像素点是如何对当前像素点产生影响的，而卷积核用来处理周围像素点和当前像素点的关系。</p><p><img src="/.io//2-5.png"></p><p>这类卷积核，也可以用于过滤像素，提取特征，也被称为过滤器。进行拟人化理解，就是说对周围像素点的主动试探和选择，即通过卷积核把周围有用的特征保留。</p><p><img src="/.io//2-6.png"></p></blockquote><p>现在再理解<strong>卷积神经网络</strong>，其是用来做图像识别的。具体来说，是专门用于识别非规整的情况，如下【以下先简化问题，只针对黑白图片，<code>1/-1</code>】：</p><p><img src="/.io//2-7.png"></p><p>以此为基础，利用卷积神经网络对图像识别的过程大概是</p><ul><li>使用特定卷积核，提取特征</li><li>使用神经网络进行识别度计算</li></ul><p><img src="/.io//2-8.png"></p><br><h5 id="小结：卷积的三层含义"><a href="#小结：卷积的三层含义" class="headerlink" title="小结：卷积的三层含义"></a>小结：卷积的三层含义</h5><ol><li>不稳定输入和稳定输出，求系统存量</li><li>【图像处理】卷积核规定了周围像素点对当前像素点如何产生影响</li><li>【过滤器】一个像素点如何试探周围像素点，如何筛选不同特征</li></ol><br><h2 id="三、卷积层里的填充和步幅"><a href="#三、卷积层里的填充和步幅" class="headerlink" title="三、卷积层里的填充和步幅"></a>三、卷积层里的填充和步幅</h2><h3 id="1-填充和步幅"><a href="#1-填充和步幅" class="headerlink" title="1 填充和步幅"></a>1 填充和步幅</h3><p><img src="/.io//3-1.png"></p><p><strong>总结</strong></p><ul><li>填充和步幅是卷积层的超参数</li><li>填充是在输入周围添加额外的行和列，来控制输出形状的减少量</li><li>步幅是每次滑动核窗口时的行和列的步长，可以成倍的减少输出的形状，从而控制计算量</li></ul><br><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>① 在所有侧边填充1个像素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    <span class="comment"># 为8×8的矩阵添加维度至(1,1,8,8)</span></span><br><span class="line">    <span class="comment"># 🔺注意写法</span></span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="comment"># 舍弃前两个维度并输入</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入通道 输出通道 核大小 填充</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d, X).shape)</span><br><span class="line"><span class="built_in">print</span>((<span class="number">1</span>,<span class="number">1</span>) + (<span class="number">8</span>,<span class="number">8</span>), <span class="built_in">type</span>((<span class="number">8</span>,<span class="number">8</span>)))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br><span class="line">    (<span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">8</span>) &lt;<span class="keyword">class</span> <span class="string">&#x27;tuple&#x27;</span>&gt;</span><br></pre></td></tr></table></figure><p>② 填充不同的高度和宽度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">8</span>, <span class="number">8</span>])</span><br></pre></td></tr></table></figure><p>③ 将高度和宽度的步幅设置为2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(comp_conv2d(conv2d, X).shape)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">4</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><p>④ 一个稍微复杂的例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure><br><h3 id="3-Q-amp-A"><a href="#3-Q-amp-A" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions"><a href="#3-1-Little-Questions" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>卷积核一般取奇数，是因为填充一般取【<code>核 - 1</code>】，这样上下填充一致。</li><li>步幅、填充、通道数、核大小，都是神经网络架构的一部分。其数值并不怎么调节，是因为比如ResNet等模型会去固定取值。</li><li>实际解决问题的过程中都会用经典的神经网络，一般不会自己去设计卷积核，也不会去自己设计或手写神经网络 。除非你的输入很特别的情况，如 <code>20×1000</code> 这种很特别的输入。【总之，网络结果在很多情况下，并没有那么关键】</li><li>NAS【网络结构搜索】：让超参数一起参加训练，属于自动机器学习的一块，很类似于寻找最优路径参数。【适应度有限，可能没必要。但适用于特殊硬件使用如使用手机去跑模型、特殊输入、特殊问题】【包括autogluon，不建议，这是烧钱的有钱人游戏，没必要，未来可能会便宜。】</li><li>多层卷积之后，特征会丢失吗？会丢失，机器学习的本质其实是一个极端压缩算法，给你很多特征，最后压缩为一个语意标号。</li><li></li><li>自动训练参数可能会带来过拟合的问题，如果验证集设置的不错，过拟合可以很好的避免。</li><li>多层 <code>3×3</code> 卷积 从效果上可以等价为 一些 <code>5×5</code> 卷积，但是 <code>3×3</code> 卷积会快一点，更便宜一点【这是GoogleNet的设计思路】</li><li>底层用大<code>Kernel</code>，上层用小<code>Kernel</code>，还是用同样的<code>Kernel</code>？主流的做法是，底层用相对来说大一点的 <code>Kernel</code>，之后还是会用 <code>3×3</code>，理由是简单、易控。但其实，最后的最后，被记住依然是最简单的模型。</li></ul><br><h4 id="3-2-这几个超参数的重要程度？"><a href="#3-2-这几个超参数的重要程度？" class="headerlink" title="3.2 这几个超参数的重要程度？"></a>3.2 这几个超参数的重要程度？</h4><p>【核大小最重要 | 填充 | 步幅】</p><ul><li>填充：会使得输入与输出大小一样，一般取【<code>核 - 1</code>】，如例子②中取了【<code>核 - 1</code>】并除以2（两侧），这样计算也比较方便。</li><li>步幅：通常取1。除非觉得计算量太大，为了快速减小计算量，通常会取2【如将这样的步幅为2的层，均匀地插在神经网络的中间】。</li></ul><br><h2 id="四、卷积层里的多输入多输出通道"><a href="#四、卷积层里的多输入多输出通道" class="headerlink" title="四、卷积层里的多输入多输出通道"></a>四、卷积层里的多输入多输出通道</h2><h3 id="1-多输入输出通道"><a href="#1-多输入输出通道" class="headerlink" title="1 多输入输出通道"></a>1 多输入输出通道</h3><p><img src="/.io//4-1.png"></p><p><strong>总结</strong></p><ul><li>输出通道数是卷积层的超参数，输入通道数是上一层的超参数。</li><li>每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果</li><li>每个输出通道有独立的三维卷积核</li></ul><blockquote><p>【注意】卷积后的高宽计算公式：</p><ul><li><code>原宽 - 卷积核宽 + padding × 2 + (stride = 1)</code></li><li><code>( 原宽 - 卷积核宽 + padding × 2 + stride ) / stride</code></li></ul></blockquote><br><h3 id="2-代码实现-1"><a href="#2-代码实现-1" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>实现一下多输入通道互相关运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="comment"># for x, k in zip(X, K):</span></span><br><span class="line">    <span class="comment">#     print(d2l.corr2d(x, k))</span></span><br><span class="line">    <span class="comment"># zip() 会遍历最外层通道</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(d2l.corr2d(x, k) <span class="keyword">for</span> x, k <span class="keyword">in</span> <span class="built_in">zip</span>(X, K))</span><br></pre></td></tr></table></figure><p>验证互相关运算的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]],</span><br><span class="line">                  [[<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], [<span class="number">4.0</span>, <span class="number">5.0</span>, <span class="number">6.0</span>], [<span class="number">7.0</span>, <span class="number">8.0</span>, <span class="number">9.0</span>]]])</span><br><span class="line">K = torch.tensor([[[<span class="number">0.0</span>, <span class="number">1.0</span>], [<span class="number">2.0</span>, <span class="number">3.0</span>]], [[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]]])</span><br><span class="line">corr2d_multi_in(X, K)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    <span class="comment">#tensor([[19., 25.],</span></span><br><span class="line">    <span class="comment">#           [37., 43.]])</span></span><br><span class="line">    <span class="comment">#tensor([[37., 47.],</span></span><br><span class="line">    <span class="comment">#        [67., 77.]])</span></span><br><span class="line">    tensor([[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">              [<span class="number">104.</span>, <span class="number">120.</span>]])</span><br></pre></td></tr></table></figure><p>计算多个通道的输出的互相关函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.stack([corr2d_multi_in(X, k) <span class="keyword">for</span> k <span class="keyword">in</span> K], <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">K = torch.stack((K, K + <span class="number">1</span>, K + <span class="number">2</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(K.shape)</span><br><span class="line"><span class="built_in">print</span>(corr2d_multi_in_out(X, K))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    tensor([[[ <span class="number">56.</span>,  <span class="number">72.</span>],</span><br><span class="line">             [<span class="number">104.</span>, <span class="number">120.</span>]],</span><br><span class="line"></span><br><span class="line">            [[ <span class="number">76.</span>, <span class="number">100.</span>],</span><br><span class="line">             [<span class="number">148.</span>, <span class="number">172.</span>]],</span><br><span class="line"></span><br><span class="line">            [[ <span class="number">96.</span>, <span class="number">128.</span>],</span><br><span class="line">             [<span class="number">192.</span>, <span class="number">224.</span>]]])</span><br></pre></td></tr></table></figure><p>1x1卷积【用全连接来实现】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out_1x1</span>(<span class="params">X, K</span>):</span><br><span class="line">    <span class="built_in">print</span>(X.shape, K.shape)</span><br><span class="line">    c_i, h, w = X.shape <span class="comment"># 3,3,3</span></span><br><span class="line">    c_o = K.shape[<span class="number">0</span>]    <span class="comment"># 2</span></span><br><span class="line">    X = X.reshape((c_i, h * w)) <span class="comment"># 3,9 高和宽拉成向量</span></span><br><span class="line">    K = K.reshape((c_o, c_i))   <span class="comment"># 2,3</span></span><br><span class="line">    Y = torch.matmul(K, X)      <span class="comment"># 2,9</span></span><br><span class="line">    <span class="keyword">return</span> Y.reshape((c_o, h, w))   <span class="comment"># 2,3,3</span></span><br><span class="line"></span><br><span class="line">X = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))       <span class="comment"># 3个通道 3×3输入</span></span><br><span class="line">K = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))    <span class="comment"># 两个输出 3个通道 1×1卷积核</span></span><br><span class="line"></span><br><span class="line">Y1 = corr2d_multi_in_out_1x1(X, K)  <span class="comment"># 等价于全连接计算</span></span><br><span class="line">Y2 = corr2d_multi_in_out(X, K)      <span class="comment"># 多通道的互相关函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不满足assert会报错</span></span><br><span class="line"><span class="keyword">assert</span> <span class="built_in">float</span>(torch.<span class="built_in">abs</span>(Y1 - Y2).<span class="built_in">sum</span>()) &lt; <span class="number">1e-6</span></span><br></pre></td></tr></table></figure><br><h3 id="3-Q-amp-A-1"><a href="#3-Q-amp-A-1" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-1"><a href="#3-1-Little-Questions-1" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>一般来讲，如果输入的高宽如果减半，会将通道数加倍【即空间信息压缩之后，将提取的信息在更多的通道上存下来】</li><li>网络越深，Padding-0越多，基本不会影响性能【计算性能：基本不影响，除非Padding很多0 | 模型性能：加入很多0不会有太多影响，0加入卷积后变成一个常数，变成了偏差值，可以完全忽略】</li><li>每个通道的卷积核是不一样的，不同通道<strong>卷积核大小</strong>是一样的【计算方便】</li><li>Bias偏移，有用，但未来的作用会越来越低。一般偏移会等价于数据均值的负数，当然由于我们做了很多均匀化的操作，所以实际上偏移没有那么大的影响。</li><li>本节课只讲到<strong>二维卷积</strong>，只有高宽两个。如果多一个深度的维度，需要用到3D卷积，这样的情况下，输入会是一个4D【通道数×深度×高×宽】，卷积核是一个5D张量【多输出个数×通道数×深度×高×宽】，输出也会是一个4D张量</li><li>卷积网络处理图片的过程中，一般不会考虑高频和低频的说法【信号处理中的概念】</li><li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42735631/article/details/121053364">MobileNets</a>，一组移动端优先的计算机视觉模型，此模型使用<strong>深度可分离卷积</strong>来构建轻量级深度神经网络。通过TensorFlow Mobile，这些模型可以在脱机状态下在移动设备上高效运行。显著优势是在手机端运行速度很快，计算复杂度非常低【<code>MobileNet</code>使用<code>3x3</code>深度可分离卷积，可在只需减少略小的准确率换来<strong>8-9</strong>倍的计算量的减少】，效果也很不错。思路是：使用3×3的深度卷积和多个1×1卷积。</li><li>卷积也能获得位置信息，而且对位置很敏感，甚至之后会讲到使用池化层去使得卷积对位置信息不那么敏感。</li><li>卷积的每个通道，不共享参数。</li><li>我们期望卷积能检测图像纹理信息，但是实际上卷积核是自己通过数据学习出来的。所以，你无法控制它到底能识别谁，也不能确定需要多少次。</li><li>输入通道不能动态变化。</li><li>RGB+视频可以使用3D卷积处理【效果好一点点，但计算量高很多】</li></ul><br><h2 id="五、池化层"><a href="#五、池化层" class="headerlink" title="五、池化层"></a>五、池化层</h2><h3 id="1-池化层"><a href="#1-池化层" class="headerlink" title="1 池化层"></a>1 池化层</h3><p><img src="/.io//5-1.png"></p><p><strong>总结</strong></p><ul><li>池化层与卷积层类似，都具有填充和步幅，但池化层没有可学习的参数</li><li>在每个输入通道作用池化层，以获得相应的输出通道，即不做通道融合，不改变通道数，输出通道数 = 输入通道数</li><li>池化层返回窗口的最大值或者平均值，来缓解卷积层对位置的敏感度</li></ul><br><h3 id="2-代码实现-2"><a href="#2-代码实现-2" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>实现池化层的正向传播</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>验证二维最大池化层的输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">Out: </span><br><span class="line">    tensor([[<span class="number">4.</span>, <span class="number">5.</span>],</span><br><span class="line">               [<span class="number">7.</span>, <span class="number">8.</span>]])</span><br></pre></td></tr></table></figure><p>验证平均池化层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>), <span class="string">&#x27;avg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[<span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">            [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure><p>填充和步幅</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">              [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure><p>深度学习框架中的步幅与池化窗口的大小相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[<span class="number">10.</span>]]]])</span><br></pre></td></tr></table></figure><p>填充和步幅可以手动设定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [<span class="number">13.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure><p>设定一个任意大小的矩形池化窗口，并分别设定填充和步幅的高度和宽度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[ <span class="number">1.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">9.</span>, <span class="number">11.</span>],</span><br><span class="line">              [<span class="number">13.</span>, <span class="number">15.</span>]]]])</span><br></pre></td></tr></table></figure><p>池化层在每个输入通道上单独运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">X</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">              [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">              [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">             [[ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>],</span><br><span class="line">              [ <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>],</span><br><span class="line">              [ <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>],</span><br><span class="line">              [<span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>]]]])</span><br><span class="line"></span><br><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    tensor([[[[ <span class="number">5.</span>,  <span class="number">7.</span>],</span><br><span class="line">              [<span class="number">13.</span>, <span class="number">15.</span>]],</span><br><span class="line"></span><br><span class="line">             [[ <span class="number">6.</span>,  <span class="number">8.</span>],</span><br><span class="line">              [<span class="number">14.</span>, <span class="number">16.</span>]]]])</span><br></pre></td></tr></table></figure><br><h3 id="3-Q-amp-A-2"><a href="#3-Q-amp-A-2" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-2"><a href="#3-1-Little-Questions-2" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>一般池化层是放在卷积后面</li><li>池化层，一般不做窗口重叠，但没有证据证明这样做效果会好【尽管现在使用池化层的频率越来越少，因为池化层有两个作用，一个是减弱位置信息对卷积层的影响，一个是减小计算量。后者也可以用卷积层实现；对于前者，本身就会对数据进行增强，做旋转、位移等等，基本不会使得其过拟合到某一个位置】</li></ul><br><h2 id="六、经典卷积神经网络-LeNet"><a href="#六、经典卷积神经网络-LeNet" class="headerlink" title="六、经典卷积神经网络 LeNet"></a>六、经典卷积神经网络 LeNet</h2><h3 id="1-LeNet"><a href="#1-LeNet" class="headerlink" title="1 LeNet"></a>1 LeNet</h3><p><img src="/.io//6-1.png"></p><p><strong>总结</strong>：LeNet是早期成功的神经网络。其先使用卷积层来学习图片空间信息，然后使用全连接层来转换到类别空间【现在不流行，原论文中的很多都还没有实现】</p><br><h3 id="2-代码实现-3"><a href="#2-代码实现-3" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>LeNet（LeNet-5）由两个部分组成： 卷积编码器和全连接层密集块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Reshape</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x.view(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line"></span><br><span class="line">net = torch.nn.Sequential(</span><br><span class="line">    Reshape(), nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>), <span class="comment"># 28+2+2=32，padding是为了可能数据在边框附近</span></span><br><span class="line">    nn.Sigmoid(),    <span class="comment"># 非线性性</span></span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),    <span class="comment">#窗口不重叠</span></span><br><span class="line">    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), </span><br><span class="line">    nn.Sigmoid(),</span><br><span class="line">    nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Flatten(), <span class="comment"># 保持批量，后面的维度直接拉成向量</span></span><br><span class="line">    nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),    <span class="comment"># 120的输出</span></span><br><span class="line">    nn.Sigmoid(),nn.Linear(<span class="number">120</span>, <span class="number">84</span>), </span><br><span class="line">    nn.Sigmoid(), nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>检查模型【Pytoch 有一个summarize的函数，功能同下】</p><p><span style="background:#ff0">【卷积就是把数据大小不断变小，通道不断变多，通道可以认为是一个空间的模式pattern。这样能把不断压缩的信息放到不同的通道里，然后通过MLP模型训练出一个理想输出】</span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), dtype=torch.float32)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape: \t&#x27;</span>, X.shape)</span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    Reshape output shape:      torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>]) </span><br><span class="line">    Conv2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    Sigmoid output shape:      torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">6</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">    Conv2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">    Sigmoid output shape:      torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br><span class="line">    AvgPool2d output shape:      torch.Size([<span class="number">1</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">    Flatten output shape:      torch.Size([<span class="number">1</span>, <span class="number">400</span>])</span><br><span class="line">    Linear output shape:      torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">    Sigmoid output shape:      torch.Size([<span class="number">1</span>, <span class="number">120</span>])</span><br><span class="line">    Linear output shape:      torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">    Sigmoid output shape:      torch.Size([<span class="number">1</span>, <span class="number">84</span>])</span><br><span class="line">    Linear output shape:      torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>LeNet在Fashion-MNIST数据集上的表现【这个数据集难一点】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br></pre></td></tr></table></figure><p>对 <code>evaluate_accuracy</code>函数进行轻微的修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_accuracy_gpu</span>(<span class="params">net, data_iter, device=<span class="literal">None</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        net.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> device:</span><br><span class="line">            device = <span class="built_in">next</span>(<span class="built_in">iter</span>(net.parameters())).device</span><br><span class="line">    metric = d2l.Accumulator(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>):</span><br><span class="line">            X = [x.to(device) <span class="keyword">for</span> x <span class="keyword">in</span> X]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        y = y.to(device)</span><br><span class="line">        metric.add(d2l.accuracy(net(X), y), y.numel())</span><br><span class="line">    <span class="keyword">return</span> metric[<span class="number">0</span>] / metric[<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>为了使用 GPU，我们还需要一点小改动</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_ch6</span>(<span class="params">net, train_iter, test_iter, num_epochs, lr, device</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;用GPU训练模型(在第六章定义)。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_weights</span>(<span class="params">m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear <span class="keyword">or</span> <span class="built_in">type</span>(m) == nn.Conv2d:</span><br><span class="line">            nn.init.xavier_uniform_(m.weight)    <span class="comment"># 保证模型开始不会炸，这个初始化很快</span></span><br><span class="line"></span><br><span class="line">    net.apply(init_weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>, device)</span><br><span class="line">    net.to(device)</span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr=lr)</span><br><span class="line">    loss = nn.CrossEntropyLoss()</span><br><span class="line">    animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, xlim=[<span class="number">1</span>, num_epochs],</span><br><span class="line">                            legend=[<span class="string">&#x27;train loss&#x27;</span>, <span class="string">&#x27;train acc&#x27;</span>, <span class="string">&#x27;test acc&#x27;</span>])</span><br><span class="line">    timer, num_batches = d2l.Timer(), <span class="built_in">len</span>(train_iter)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        metric = d2l.Accumulator(<span class="number">3</span>)</span><br><span class="line">        net.train()</span><br><span class="line">        <span class="keyword">for</span> i, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_iter):</span><br><span class="line">            timer.start()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                metric.add(l * X.shape[<span class="number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="number">0</span>])</span><br><span class="line">            timer.stop()</span><br><span class="line">            train_l = metric[<span class="number">0</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            train_acc = metric[<span class="number">1</span>] / metric[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> (i + <span class="number">1</span>) % (num_batches // <span class="number">5</span>) == <span class="number">0</span> <span class="keyword">or</span> i == num_batches - <span class="number">1</span>:</span><br><span class="line">                animator.add(epoch + (i + <span class="number">1</span>) / num_batches,</span><br><span class="line">                             (train_l, train_acc, <span class="literal">None</span>))</span><br><span class="line">        test_acc = evaluate_accuracy_gpu(net, test_iter)</span><br><span class="line">        animator.add(epoch + <span class="number">1</span>, (<span class="literal">None</span>, <span class="literal">None</span>, test_acc))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;loss <span class="subst">&#123;train_l:<span class="number">.3</span>f&#125;</span>, train acc <span class="subst">&#123;train_acc:<span class="number">.3</span>f&#125;</span>, &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;test acc <span class="subst">&#123;test_acc:<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;metric[<span class="number">2</span>] * num_epochs / timer.<span class="built_in">sum</span>():<span class="number">.1</span>f&#125;</span> examples/sec &#x27;</span></span><br><span class="line">          <span class="string">f&#x27;on <span class="subst">&#123;<span class="built_in">str</span>(device)&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><p>训练和评估LeNet-5模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.9</span>, <span class="number">10</span></span><br><span class="line">train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.453</span>, train acc <span class="number">0.832</span>, test acc <span class="number">0.817</span></span><br><span class="line">    <span class="number">71848.6</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//6-2.png"></p><blockquote><p>卷积【LeNet】是受限的全连接，意味着模型相对于MLP会小很多，几乎没有Overfitting</p></blockquote><br><h3 id="3-Q-amp-A-3"><a href="#3-Q-amp-A-3" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-3"><a href="#3-1-Little-Questions-3" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>卷积可以做图像、时序【如文本】</li><li>LeNet中通道变大，一般情况下，当图像高宽【每一个高宽的像素，都是对特定模式的识别，即同一个卷积核，使得用1个像素表示原来4或9个像素的值】减半，会将通道数【增加了一组参数】翻倍，整体上的数据量还是一个减小的过程</li><li>如何选择深度学习模型？数据量小，可以用MLP，速度快；但数据量大，参数量大的时候，用不了MLP</li><li>卷积可视化：<a target="_blank" rel="noopener" href="https://poloclub.github.io/cnn-explainer/">CNN-explainer</a></li></ul><br><h2 id="七、经典卷积神经网络-AlexNet"><a href="#七、经典卷积神经网络-AlexNet" class="headerlink" title="七、经典卷积神经网络 AlexNet"></a>七、经典卷积神经网络 AlexNet</h2><blockquote><p>2012年左右至今，AlexNet依然使用，变化看起来不大，但花费了将近20年的时间。</p><ul><li>AlexNet是更大更深的LeNet，10x参数个数，260x计算复杂度</li><li>加入了丢弃法、最大池化、数据增强</li><li>AlexNet赢下2012年imageNet竞赛后，标志着神经网络热潮的开始</li></ul></blockquote><h3 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1 AlexNet"></a>1 AlexNet</h3><p><img src="/.io//7-1.png"></p><br><h3 id="2-代码实现-4"><a href="#2-代码实现-4" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>深度卷积神经网络（AlexNet）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处使用的是Fashion-MNIST,所以只有一个通道</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(), <span class="comment"># 256 还是 384?</span></span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), nn.Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">6400</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(), nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">    nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))    <span class="comment">#Fashion-MNIST</span></span><br></pre></td></tr></table></figure><p>我们构造一个 单通道数据，来观察每一层输出的形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;Output shape:\t&#x27;</span>, X.shape)</span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    Conv2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">    MaxPool2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">    Conv2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">    MaxPool2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    Conv2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    Conv2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    Conv2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    MaxPool2d Output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">    Flatten Output shape:     torch.Size([<span class="number">1</span>, <span class="number">6400</span>])</span><br><span class="line">    Linear Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Dropout Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Linear Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    ReLU Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Dropout Output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Linear Output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>Fashion-MNIST图像的分辨率 低于ImageNet图像。 我们将它们增加到 224×224</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br></pre></td></tr></table></figure><p>训练AlexNet【Alex学习率会调第一点，LeNet当时调到了0.9】【Alex的GPU使用率可能大约百分之七八十，LeNet的GPU利用率很低】【Alex预测准确率达到 0.880】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs = <span class="number">0.01</span>, <span class="number">10</span></span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.332</span>, train acc <span class="number">0.877</span>, test acc <span class="number">0.880</span></span><br><span class="line">    <span class="number">4103.4</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//7-2.png"></p><br><h3 id="3-Q-amp-A-4"><a href="#3-Q-amp-A-4" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-4"><a href="#3-1-Little-Questions-4" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>AlexNet的目的就是为了筛选特征，以供全连接层进行分类。虽然，神经网络早期是说在模拟人脑神经的运行方式，但其实就是扯淡，现在早不提那一套了。总之，深度学习的可解释性的确很差。</li><li>AlexNet后的两个4096的全连接层Dense，是非常厉害的模型，因为前面的卷积的特征抽的不够好不够深，去掉一个后效果会差很多。</li><li><span style="background:#52e6fc">那帮搞深度学习的，最厉害的不是调参，最厉害的是包装！是宣传！所以，不要沉迷技术！对研究者来说，怎么宣传这一块的能力非常重要，不一定是产品有多厉害！</span> 比如：深度卷积网络这一词中的Deep，本身效果没有提升多少，但是被包装成划时代的提升，看看苹果的每年发布会。【万物转营销：技术都是要落地的】</li><li>在实际使用模型时，输入可能是不规则的，不一定是224×224。一般的做法是将短边补齐到224，然后在其中抽样方形样本</li></ul><br><br><h2 id="八、使用块的网络-VGG"><a href="#八、使用块的网络-VGG" class="headerlink" title="八、使用块的网络 VGG"></a>八、使用块的网络 VGG</h2><h3 id="1-VGG"><a href="#1-VGG" class="headerlink" title="1 VGG"></a>1 VGG</h3><blockquote><p><code>AlexNet</code> 说是变大的 <code>LeNet</code>，但变大的很随意。VGG是把AlexNet做的更深更大，更规则！</p><p><code>VGG</code>【<code>Visual Geometry Group</code>】的思路就是，将卷积层整合成<strong>块</strong>，再不断拼接。</p><p>值得一提，<code>VGG</code>是2013年<code>imageNet</code>的冠军。</p></blockquote><p><img src="/.io//8-1.png"></p><p><strong>总结</strong></p><ul><li>VGG 使用可重复使用的卷积块来构建深度卷积神经网络</li><li>不同的卷积块个数和超参数可以得到不同复杂度的变种</li></ul><br><h3 id="2-代码实现-5"><a href="#2-代码实现-5" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>VGG块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        layers.append(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        layers.append(nn.ReLU())</span><br><span class="line">        in_channels = out_channels</span><br><span class="line">    layers.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><p>VGG网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 5块</span></span><br><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">conv_arch</span>):</span><br><span class="line">    conv_blks = []</span><br><span class="line">    in_channels = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> (num_convs, out_channels) <span class="keyword">in</span> conv_arch:</span><br><span class="line">        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">        in_channels = out_channels</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*conv_blks, nn.Flatten(),</span><br><span class="line">                         nn.Linear(out_channels * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">                         nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),</span><br><span class="line">                         nn.Dropout(<span class="number">0.5</span>), nn.Linear(<span class="number">4096</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">net = vgg(conv_arch)</span><br></pre></td></tr></table></figure><p>观察每个层输出的形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">X = torch.randn(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> blk <span class="keyword">in</span> net:</span><br><span class="line">    X = blk(X)</span><br><span class="line">    <span class="built_in">print</span>(blk.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line"><span class="comment"># 第一层VGG：高宽减半，通道拉到64...</span></span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">112</span>, <span class="number">112</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">    Flatten output shape:     torch.Size([<span class="number">1</span>, <span class="number">25088</span>])</span><br><span class="line">    Linear output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    ReLU output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Dropout output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Linear output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    ReLU output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Dropout output shape:     torch.Size([<span class="number">1</span>, <span class="number">4096</span>])</span><br><span class="line">    Linear output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>由于VGG-11比AlexNet计算量更大，因此我们构建了一个通道数较少的网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ratio = <span class="number">4</span></span><br><span class="line">small_conv_arch = [(pair[<span class="number">0</span>], pair[<span class="number">1</span>] // ratio) <span class="keyword">for</span> pair <span class="keyword">in</span> conv_arch]</span><br><span class="line">net = vgg(small_conv_arch)</span><br></pre></td></tr></table></figure><p>模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.178</span>, train acc <span class="number">0.936</span>, test acc <span class="number">0.922</span></span><br><span class="line">    <span class="number">2538.3</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//8-2.png"></p><blockquote><p>用的是比VGG最小模型VGG11【8卷积+3全连接】还小的模型【通道除以4，计算量减少16倍】，比AlexNet慢一倍，所以VGG的确很贵，但精度提升很大</p></blockquote><br><h2 id="九、网络中的网络-NiN"><a href="#九、网络中的网络-NiN" class="headerlink" title="九、网络中的网络 NiN"></a>九、网络中的网络 NiN</h2><h3 id="1-NiN"><a href="#1-NiN" class="headerlink" title="1 NiN"></a>1 NiN</h3><blockquote><p>NiN【 Network In Network】，网络中的网络，现在用的不多，几乎很少用，但提出了很重要的概念。</p><p>NiN 基于 <code>AlexNet</code>，并认为上面两个模型的全连接层占用了大量参数，不是很好</p></blockquote><p><img src="/.io//9-1.png"></p><p><strong>总结</strong></p><ul><li>NiN 块使用卷积层加两个1×1卷积层，后者对每个像素增加了非线性性</li><li>NiN 使用全局平局池化层来代替 VGG 和 AlexNet 中的全连接层，【比较狠的做法】这样不容易拟合，更少的参数个数</li></ul><br><h3 id="2-代码实现-6"><a href="#2-代码实现-6" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>NiN块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, strides, padding</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">        nn.ReLU())</span><br></pre></td></tr></table></figure><p>NiN模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    <span class="comment"># 使用灰度图，故为1通道</span></span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, strides=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, strides=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(<span class="number">3</span>, stride=<span class="number">2</span>), nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, strides=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">    nn.Flatten())</span><br></pre></td></tr></table></figure><p>查看每个块的输出形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line">   </span><br><span class="line">Out:</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">54</span>, <span class="number">54</span>])</span><br><span class="line">    MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">96</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">26</span>, <span class="number">26</span>])</span><br><span class="line">    MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    MaxPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">    Dropout output shape:     torch.Size([<span class="number">1</span>, <span class="number">384</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">    AdaptiveAvgPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    Flatten output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">224</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.331</span>, train acc <span class="number">0.878</span>, test acc <span class="number">0.878</span></span><br><span class="line">    <span class="number">3182.2</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//9-2.png"></p><blockquote><p>精度不算高，训练速度也没有高【可能是数据集较小，实际在 imageNet 上会比 AlexNet 好一些】</p></blockquote><br><h3 id="3-Q-amp-A-5"><a href="#3-Q-amp-A-5" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-5"><a href="#3-1-Little-Questions-5" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li><p>网络结构里没有 Softmax 是因为写在了训练函数中</p></li><li><p>全局平均池化层的设计思想，给之后网络的设计带来很大影响。其可以把高宽直接压成1×1，计算变简单，最主要是会降低模型复杂度，提高模型泛化性，精度提高；坏处就是会使得后面的全连接层收敛变慢【相比LeNet 和 AlexNet 其后面的几层全连接层 fit 数据的能力太强了，所以收敛快】，但话说会来，收敛慢不是问题 ，只要精度高，一切都不是问题。</p></li><li><p>两个1×1的卷积层，对每个像素增加了非线性，如何理解？</p><p>1×1卷积层，相当于对每一个像素对应的那个通道向量，做一个全连接层。两个1×1的卷积层，相当于对每个像素的输入通道，做了一个两层隐含层的MLP，而MLP中间有一个ReLU函数。</p></li></ul><br><h2 id="十、含并行连接的网络-GoogLeNet"><a href="#十、含并行连接的网络-GoogLeNet" class="headerlink" title="十、含并行连接的网络 GoogLeNet"></a>十、含并行连接的网络 GoogLeNet</h2><h3 id="1-GoogLeNet"><a href="#1-GoogLeNet" class="headerlink" title="1 GoogLeNet"></a>1 GoogLeNet</h3><blockquote><p>第一个超过100层的卷积神经网络【取名是为了致敬 LeNet，实际上与 LeNet 一点关系都没有】，这个模型也是受到 NiN 的启发</p><p>🔺【<strong>卷积层可学习的参数个数</strong>】输入通道数 × 输出通道数 × Kernel大小</p></blockquote><p><img src="/.io//10-1.png"></p><p><strong>总结：</strong></p><ul><li><code>Inception</code> 块有4条不同超参数的卷积层和池化层的分支来抽取不同信息【它的一个主要优点就是模型参数小，计算复杂度低】；</li><li><code>GoogLeNet</code> 使用了9个 <code>Inception</code> 块，是第一个达到上百层的神经网络【不是纯深100，直到ResNet出现才可以】，后续有进一步的改进，如V2，V3；</li><li><code>GoogLeNet</code> 虽然精度高，但不是特别受欢迎的一个原因就是太复杂。</li></ul><br><h3 id="2-代码实现-7"><a href="#2-代码实现-7" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>Inception块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Inception, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p2_1 = nn.Conv2d(in_channels, c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p2_2 = nn.Conv2d(c2[<span class="number">0</span>], c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p3_1 = nn.Conv2d(in_channels, c3[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p3_2 = nn.Conv2d(c3[<span class="number">0</span>], c3[<span class="number">1</span>], kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        p1 = F.relu(<span class="variable language_">self</span>.p1_1(x))</span><br><span class="line">        p2 = F.relu(<span class="variable language_">self</span>.p2_2(F.relu(<span class="variable language_">self</span>.p2_1(x))))</span><br><span class="line">        p3 = F.relu(<span class="variable language_">self</span>.p3_2(F.relu(<span class="variable language_">self</span>.p3_1(x))))</span><br><span class="line">        p4 = F.relu(<span class="variable language_">self</span>.p4_2(<span class="variable language_">self</span>.p4_1(x)))</span><br><span class="line">        <span class="comment"># 在通道的维度上做连接，dim=0为批量大小</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>GoogLeNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.ReLU(), nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>,</span><br><span class="line">                                           padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b2 = nn.Sequential(nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">                   nn.Conv2d(<span class="number">64</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b3 = nn.Sequential(Inception(<span class="number">192</span>, <span class="number">64</span>, (<span class="number">96</span>, <span class="number">128</span>), (<span class="number">16</span>, <span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">                   Inception(<span class="number">256</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">192</span>), (<span class="number">32</span>, <span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b4 = nn.Sequential(Inception(<span class="number">480</span>, <span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">512</span>, <span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">                   Inception(<span class="number">528</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">b5 = nn.Sequential(Inception(<span class="number">832</span>, <span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   Inception(<span class="number">832</span>, <span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">                   nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)), nn.Flatten())</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="number">1024</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>为了使Fashion-MNIST上的训练短小精悍，我们将输入的高和宽从224降到96</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batchsize = 1 只是为了做演示，实际上为每次128张图片进行处理</span></span><br><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">24</span>, <span class="number">24</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">192</span>, <span class="number">12</span>, <span class="number">12</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">480</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">832</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">1024</span>])</span><br><span class="line">    Linear output shape:     torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.1</span>, <span class="number">10</span>, <span class="number">128</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.240</span>, train acc <span class="number">0.909</span>, test acc <span class="number">0.899</span></span><br><span class="line">    <span class="number">3403.7</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//10-2.png"></p><br><h3 id="3-Q-amp-A-6"><a href="#3-Q-amp-A-6" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-6"><a href="#3-1-Little-Questions-6" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>3×3卷积核、1×1卷积核都能降通道且保持高宽不变，但选择1×1卷积核的原因是：1×1卷积核计算量小；</li><li>3×3卷积改成两个3×1、1×3的好处是可以降低计算量；</li><li>相比之前的 LeNet 和 AlexNet ，GoogLeNet 的设计和之后网络的设计都更加精巧，不是直接加全连接层直接分类。当卷积层越来越深，通道数越来越多，可识别的模式越来越多，但一般1024、2048就差不多了【基于imageNet】，否则容易过拟合；</li><li>训练过程中的 <code>tricks</code> 很重要，可以在不改变模型复杂度的情况下，提高模型精度。</li></ul><br><br><h2 id="十一、批量归一化"><a href="#十一、批量归一化" class="headerlink" title="十一、批量归一化"></a>十一、批量归一化</h2><h3 id="1-批量归一化"><a href="#1-批量归一化" class="headerlink" title="1 批量归一化"></a>1 批量归一化</h3><blockquote><p>批量归一化的思想，就目前来说不太行；但批量归一化层【2016年才出现的概念】，或多活动少得用在目前主流的卷积神经网络中，特别是做特别深的网络时，该层也是很必要的。</p></blockquote><p><img src="/.io//11-1.png"></p><p><strong>总结</strong></p><ul><li>批量归一化固定小批量中的均值和方差，然后学习出适合的偏移和缩放</li><li>可加速收敛速度，但<strong>一般不改变模型精度</strong>【允许使用更大的学习率】</li></ul><br><h3 id="2-代码实现-8"><a href="#2-代码实现-8" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>从零实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># moving_mean moving_var : 全局的均值和方差（不是miniBatch的）</span></span><br><span class="line"><span class="comment"># eps: 防止除0</span></span><br><span class="line"><span class="comment"># momentum: 为了更新moving_mean moving_var，一般为0.9</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_norm</span>(<span class="params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> torch.is_grad_enabled(): <span class="comment"># 如果在inferrence-推理</span></span><br><span class="line">        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 2：全连接层  4：卷积层【暂时不考虑567...】</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(X.shape) <span class="keyword">in</span> (<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(X.shape) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># 按行求均值方差</span></span><br><span class="line">            mean = X.mean(dim=<span class="number">0</span>)</span><br><span class="line">            var = ((X - mean)**<span class="number">2</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 卷积：0-dim:批量标号;1-dim:通道;2-3-dim:高宽</span></span><br><span class="line">            mean = X.mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)    <span class="comment"># 结果为 1×n×1×1</span></span><br><span class="line">            var = ((X - mean)**<span class="number">2</span>).mean(dim=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        X_hat = (X - mean) / torch.sqrt(var + eps)</span><br><span class="line">        moving_mean = momentum * moving_mean + (<span class="number">1.0</span> - momentum) * mean</span><br><span class="line">        moving_var = momentum * moving_var + (<span class="number">1.0</span> - momentum) * var</span><br><span class="line">    Y = gamma * X_hat + beta</span><br><span class="line">    <span class="keyword">return</span> Y, moving_mean.data, moving_var.data</span><br></pre></td></tr></table></figure><p>创建一个正确的 <code>BatchNorm</code> 图层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BatchNorm</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 输入参数 num_dims:2 or 4</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_features, num_dims</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">if</span> num_dims == <span class="number">2</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape = (<span class="number">1</span>, num_features, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># γ与β 放入nn.Parameter，其值会随训练不断迭代</span></span><br><span class="line">        <span class="variable language_">self</span>.gamma = nn.Parameter(torch.ones(shape))</span><br><span class="line">        <span class="variable language_">self</span>.beta = nn.Parameter(torch.zeros(shape))</span><br><span class="line">        <span class="comment"># moving_mean 与 moving_var 不需要迭代</span></span><br><span class="line">        <span class="variable language_">self</span>.moving_mean = torch.zeros(shape)</span><br><span class="line">        <span class="variable language_">self</span>.moving_var = torch.ones(shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="comment"># moving_mean 与 moving_var 未放入nn.Parameter</span></span><br><span class="line">        <span class="comment"># 需要手动计算，将数据移动到对应的GPU上</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.moving_mean.device != X.device:</span><br><span class="line">            <span class="variable language_">self</span>.moving_mean = <span class="variable language_">self</span>.moving_mean.to(X.device)</span><br><span class="line">            <span class="variable language_">self</span>.moving_var = <span class="variable language_">self</span>.moving_var.to(X.device)</span><br><span class="line">        Y, <span class="variable language_">self</span>.moving_mean, <span class="variable language_">self</span>.moving_var = batch_norm(</span><br><span class="line">            X, <span class="variable language_">self</span>.gamma, <span class="variable language_">self</span>.beta, <span class="variable language_">self</span>.moving_mean, <span class="variable language_">self</span>.moving_var,</span><br><span class="line">            eps=<span class="number">1e-5</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">        <span class="keyword">return</span> Y</span><br></pre></td></tr></table></figure><p>应用<code>BatchNorm</code> 于LeNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">6</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">                    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), BatchNorm(<span class="number">16</span>, num_dims=<span class="number">4</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">16</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">                    BatchNorm(<span class="number">120</span>, num_dims=<span class="number">2</span>), nn.Sigmoid(),</span><br><span class="line">                    nn.Linear(<span class="number">120</span>, <span class="number">84</span>), BatchNorm(<span class="number">84</span>, num_dims=<span class="number">2</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>在Fashion-MNIST数据集上训练网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">1.0</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.246</span>, train acc <span class="number">0.910</span>, test acc <span class="number">0.887</span></span><br><span class="line">    <span class="number">35771.8</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//11-2.png"></p><p>拉伸参数 <code>gamma</code> 和偏移参数 <code>beta</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net[<span class="number">1</span>].gamma.reshape((-<span class="number">1</span>,)), net[<span class="number">1</span>].beta.reshape((-<span class="number">1</span>,))</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    (tensor([<span class="number">2.1534</span>, <span class="number">2.1612</span>, <span class="number">2.0096</span>, <span class="number">1.9473</span>, <span class="number">1.8451</span>, <span class="number">1.3328</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ViewBackward&gt;),</span><br><span class="line">     tensor([ <span class="number">0.0310</span>, -<span class="number">2.4748</span>,  <span class="number">0.5816</span>,  <span class="number">0.5764</span>, -<span class="number">1.6917</span>, -<span class="number">0.6970</span>], device=<span class="string">&#x27;cuda:0&#x27;</span>,</span><br><span class="line">        grad_fn=&lt;ViewBackward&gt;))</span><br></pre></td></tr></table></figure><p>简明实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">                    nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=<span class="number">5</span>), nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">256</span>, <span class="number">120</span>), nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.Linear(<span class="number">120</span>, <span class="number">84</span>), nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">                    nn.Sigmoid(), nn.Linear(<span class="number">84</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>使用相同超参数来训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.245</span>, train acc <span class="number">0.909</span>, test acc <span class="number">0.843</span></span><br><span class="line">    <span class="number">63402.8</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//11-3.png"></p><br><h3 id="3-Q-amp-A-7"><a href="#3-Q-amp-A-7" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-7"><a href="#3-1-Little-Questions-7" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li><p><code>Batch-Normalization</code> 的思路类似用 <code>Xvier</code> 时候的 <code>Normalization</code>，都是让模型稳定，这样收敛就不会变慢 【BN对数据做，XVier对初始化做】【归一化Normalization就是均值为0方差为1；加入一些防止梯度爆炸和消失的项，叫做正则项Reguarizatoin】</p></li><li><p>BN对较深的神经网络的效果很明显，对mlp等浅层的效果可能不明显；</p></li><li><p>BN是做了一个线性变换，这与加一个线性层有什么区别呢？</p><p>线性层不一定会学到你要的东西。对线性层来说，不需要做方差变1均值变0，但不做，很有可能数值不稳定，因而得不到好的值域里的参数</p></li><li><p>BN使得收敛变快的原因？</p><p>BN使得每一层的梯度差距不会过大，这样可以使用更大的学习率，使得权重更新变快</p></li><li><p>先根据内存大小调 batch_size【要么GPU利用率90%左右，要么更常用的是，调整batch_size，使得GPU每秒处理数趋于稳定即可】，再调学习率和epoch数【可中途停止】</p></li></ul><br><h2 id="十二、残差网络（ResNet）"><a href="#十二、残差网络（ResNet）" class="headerlink" title="十二、残差网络（ResNet）"></a>十二、残差网络（ResNet）</h2><h3 id="1-ResNet"><a href="#1-ResNet" class="headerlink" title="1 ResNet"></a>1 ResNet</h3><blockquote><p><code>ReNet</code>【2015】，源于一个思想：不断加深神经网络，但加深不一定会提高模型精度。所以，<code>ReNet</code> 的核心思想是保证：大模型的效果不会比小模型的效果差！</p></blockquote><p><img src="/.io//12-1.png"></p><p><strong>总结</strong></p><ul><li>残差块使得很深的网络更加容易训练，甚至可以训练一千层的网络【ReNet连接的存在，使得永远可以包含小网络】</li><li>残差网络对随后的深层神经网络设计产生了深远的影响，无论是卷积类网络，还是全连接类网络</li></ul><br><h3 id="2-代码实现-9"><a href="#2-代码实现-9" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>残差块</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_channels, num_channels, use_1x1conv=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 strides=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               padding=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=<span class="number">3</span>,</span><br><span class="line">                               padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            <span class="variable language_">self</span>.conv3 = nn.Conv2d(input_channels, num_channels,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>, stride=strides)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.conv3 = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 每个BN都有自己的参数要学，所以需要定义两个</span></span><br><span class="line">        <span class="variable language_">self</span>.bn1 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        <span class="variable language_">self</span>.bn2 = nn.BatchNorm2d(num_channels)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU(inplace=<span class="literal">True</span>)    <span class="comment"># inplace节省内存</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        Y = F.relu(<span class="variable language_">self</span>.bn1(<span class="variable language_">self</span>.conv1(X)))</span><br><span class="line">        Y = <span class="variable language_">self</span>.bn2(<span class="variable language_">self</span>.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.conv3:</span><br><span class="line">            X = <span class="variable language_">self</span>.conv3(X)</span><br><span class="line">        Y += X</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y)</span><br></pre></td></tr></table></figure><p>输入和输出形状一致</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">X = torch.rand(<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>)</span><br><span class="line">Y = blk(X)</span><br><span class="line">Y.shape</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">4</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure><p>增加输出通道数的同时，减半输出的高和宽</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blk = Residual(<span class="number">3</span>, <span class="number">6</span>, use_1x1conv=<span class="literal">True</span>, strides=<span class="number">2</span>)</span><br><span class="line">blk(X).shape</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    torch.Size([<span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><p>ResNet模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">b1 = nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">                   nn.BatchNorm2d(<span class="number">64</span>), nn.ReLU(),</span><br><span class="line">                   nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># stage</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet_block</span>(<span class="params">input_channels, num_channels, num_residuals,</span></span><br><span class="line"><span class="params">                 first_block=<span class="literal">False</span></span>):</span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(</span><br><span class="line">                Residual(input_channels, num_channels, use_1x1conv=<span class="literal">True</span>,</span><br><span class="line">                         strides=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(num_channels, num_channels))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="comment"># * 是python语法，可以展开【略】</span></span><br><span class="line">b2 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">b3 = nn.Sequential(*resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">b4 = nn.Sequential(*resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">b5 = nn.Sequential(*resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>)),</span><br><span class="line">                    nn.Flatten(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>观察一下ResNet中不同模块的输入形状是如何变化的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">X = torch.rand(size=(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X = layer(X)</span><br><span class="line">    <span class="built_in">print</span>(layer.__class__.__name__, <span class="string">&#x27;output shape:\t&#x27;</span>, X.shape)</span><br><span class="line">    </span><br><span class="line">Out:</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">64</span>, <span class="number">56</span>, <span class="number">56</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">128</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">256</span>, <span class="number">14</span>, <span class="number">14</span>])</span><br><span class="line">    Sequential output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">7</span>, <span class="number">7</span>])</span><br><span class="line">    AdaptiveAvgPool2d output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">    Flatten output shape:     torch.Size([<span class="number">1</span>, <span class="number">512</span>])</span><br><span class="line">    Linear output shape:	 torch.Size([<span class="number">1</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>训练模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr, num_epochs, batch_size = <span class="number">0.05</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="number">96</span>)</span><br><span class="line">d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">    loss <span class="number">0.023</span>, train acc <span class="number">0.993</span>, test acc <span class="number">0.912</span></span><br><span class="line">    <span class="number">4687.2</span> examples/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><p><img src="/.io//12-2.png"></p><blockquote><p><code>ReNet</code> 的确是很强的模型，即使overfitting接近1，测试精度都能到0.88</p></blockquote><br><h3 id="3-Q-amp-A-8"><a href="#3-Q-amp-A-8" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions-8"><a href="#3-1-Little-Questions-8" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li><p><code>batch_size &gt; 1000</code> 可能会导致收敛变慢且精度下降，主要是因为<code>batch</code>过大，一个<code>batch</code>中的样本相似度很高，多样性下降。</p></li><li><p>对于 <code>f(x) = x + g(x)</code>，如何理解当 <code>g(x)</code> 是更坏的模型，<code>f (x)</code> 依然保持原来小模型？</p><p>如果对于训练出来的 <code>g(x)</code> 更坏，做梯度反传【Back Forward】的时候拿不到任何梯度，所以 <code>g(x)</code> 的权重不会被更新，所以其权重很可能就是一些很小随机的值，贡献很小，梯度甚至为0，然后其权值趋于很小。所以，<code>ResNet</code> 会使得模型变深时，模型效果不会变差。</p></li><li><p>cos学习率挺好的，现在用的是固定学习率，现在还有阶梯式的学习率。</p></li><li><p>测试精度acc 是有可能大于训练acc的。比如，你在训练集中加入大量噪声。</p></li><li><p><strong>残差</strong>【<code>Residual</code>】：举个例子，对于<code>f(x) = x + g(x)</code>，先去训练使得 <code>fitting x</code> 的这个小模型，当这个小模型训练的差不多了，再去 fitting g(x)【残差 | Residual】，使得越来越逼近真实模型。【类似于牛顿法，不断逼近】【对于<code>ResNet-152</code>，它也是先训练好 <code>ResNet-18</code>，再不断去迭代残差 <code>g(x)</code>】</p><p><img src="/.io//12-3.png"></p></li></ul><br><h3 id="4-ResNet-为什么能训练出1000层的模型？"><a href="#4-ResNet-为什么能训练出1000层的模型？" class="headerlink" title="4 ResNet 为什么能训练出1000层的模型？"></a>4 ResNet 为什么能训练出1000层的模型？</h3><blockquote><p>深层网络最大的障碍就是梯度消失和爆炸，之前讲过一个办法就是避免梯度消失，即乘法变加法，其实<code>ResNet</code> 就是这么做的，特别是 Residual connection。</p></blockquote><p><img src="/.io//12-4.png"></p><br><h2 id="十三、竞赛：图片分类"><a href="#十三、竞赛：图片分类" class="headerlink" title="十三、竞赛：图片分类"></a>十三、竞赛：图片分类</h2><p><img src="/.io//13-1.png"></p><blockquote><p>竞赛地址<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/classify-leaves">Addr</a></p></blockquote><br><br><br><br><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Moustache 微信支付"><p>微信支付</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Moustache</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hammerzer.github.io/2023/08/10/convolutional-neural-network/" title="动手学深度学习-卷积神经网络">https://hammerzer.github.io/2023/08/10/convolutional-neural-network/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2023/07/21/Tool-Pytorch/" rel="prev" title="Tool-Pytorch"><i class="fa fa-chevron-left"></i> Tool-Pytorch</a></div><div class="post-nav-item"><a href="/2023/09/16/introduction-to-mathematical-modelling/" rel="next" title="入坑数学建模">入坑数学建模 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CONTENT-OUTLINE"><span class="nav-number">1.</span> <span class="nav-text">CONTENT OUTLINE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%87%E3%80%81%E7%9B%AE%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">〇、目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81Pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80"><span class="nav-number">3.</span> <span class="nav-text">一、Pytorch神经网络基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E6%A8%A1%E5%9E%8B%E6%9E%84%E9%80%A0"><span class="nav-number">3.1.</span> <span class="nav-text">1、模型构造</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B1%82%E5%92%8C%E5%9D%97"><span class="nav-number">3.1.1.</span> <span class="nav-text">层和块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%9D%97"><span class="nav-number">3.1.2.</span> <span class="nav-text">自定义块</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A1%BA%E5%BA%8F%E5%9D%97"><span class="nav-number">3.1.3.</span> <span class="nav-text">顺序块</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%8F%82%E6%95%B0%E7%AE%A1%E7%90%86"><span class="nav-number">3.2.</span> <span class="nav-text">2、参数管理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E8%AE%BF%E9%97%AE"><span class="nav-number">3.2.1.</span> <span class="nav-text">参数访问</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%86%85%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.2.2.</span> <span class="nav-text">内置初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">3.2.3.</span> <span class="nav-text">自定义初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A"><span class="nav-number">3.2.4.</span> <span class="nav-text">参数绑定</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82"><span class="nav-number">3.3.</span> <span class="nav-text">3、自定义层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4%E3%80%81%E8%AF%BB%E5%86%99%E6%96%87%E4%BB%B6"><span class="nav-number">3.4.</span> <span class="nav-text">4、读写文件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E4%B8%8E%E4%BF%9D%E5%AD%98%E4%B8%BE%E4%BE%8B"><span class="nav-number">3.4.1.</span> <span class="nav-text">加载与保存举例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="nav-number">3.4.2.</span> <span class="nav-text">加载和保存模型参数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E3%80%81Q-amp-A"><span class="nav-number">3.5.</span> <span class="nav-text">5、Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-Little-Problems"><span class="nav-number">3.5.1.</span> <span class="nav-text">5.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-%E5%86%85%E5%AD%98%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98"><span class="nav-number">3.5.2.</span> <span class="nav-text">5.2 内存爆炸问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6%E3%80%81GPU"><span class="nav-number">3.6.</span> <span class="nav-text">6、GPU</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GPU%E4%B8%8ECUDA"><span class="nav-number">3.6.1.</span> <span class="nav-text">GPU与CUDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8EGPU"><span class="nav-number">3.6.2.</span> <span class="nav-text">神经网络与GPU</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7%E3%80%81GPU%EF%BC%9AQ-amp-A"><span class="nav-number">3.7.</span> <span class="nav-text">7、GPU：Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-Little-Problems"><span class="nav-number">3.7.1.</span> <span class="nav-text">7.1 Little Problems</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-device-count-%E4%B8%BA0"><span class="nav-number">3.7.2.</span> <span class="nav-text">7.2 device_count()为0</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8%E3%80%81CUDA%E4%B8%8B%E8%BD%BD%E5%AE%89%E8%A3%85"><span class="nav-number">3.8.</span> <span class="nav-text">8、CUDA下载安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">3.8.1.</span> <span class="nav-text">8.1 三个基本概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-%E6%93%8D%E4%BD%9C"><span class="nav-number">3.8.2.</span> <span class="nav-text">8.2 操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A0-%E7%AE%97%E5%8A%9B%E8%BD%AC%E6%8D%A2%E3%80%90%E5%AE%98%E7%BD%91%E6%9F%A5%E8%AF%A2-%E7%A5%9E%E4%BB%99%E7%9B%B8%E5%8A%A9%E3%80%91"><span class="nav-number">3.8.2.1.</span> <span class="nav-text">① 算力转换【官网查询 | 神仙相助】</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A1-%E7%A1%AE%E5%AE%9ACUDA%E7%89%88%E6%9C%AC%E6%89%80%E6%94%AF%E6%8C%81%E7%9A%84%E7%AE%97%E5%8A%9B"><span class="nav-number">3.8.2.2.</span> <span class="nav-text">② 确定CUDA版本所支持的算力</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A2-%E6%9F%A5%E7%9C%8B%E8%87%AA%E5%B7%B1%E7%9A%84CUDA-driver-version"><span class="nav-number">3.8.2.3.</span> <span class="nav-text">③ 查看自己的CUDA driver version</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E2%91%A3-%E5%9C%A8%E7%BA%BF%E5%AE%89%E8%A3%85%E8%87%AA%E5%B7%B1%E7%9A%84GPU%E7%89%88%E6%9C%AC%E7%9A%84pytorch"><span class="nav-number">3.8.2.4.</span> <span class="nav-text">④ 在线安装自己的GPU版本的pytorch</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-%E9%AA%8C%E8%AF%81"><span class="nav-number">3.8.3.</span> <span class="nav-text">8.3 验证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-4-%E9%A2%9D%E5%A4%96%E6%93%8D%E4%BD%9C"><span class="nav-number">3.8.4.</span> <span class="nav-text">8.4 额外操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#8-4-1-%E7%A1%AE%E5%AE%9ACUDA%E7%89%88%E6%9C%AC%E6%94%AF%E6%8C%81%E7%9A%84VS%E7%89%88%E6%9C%AC"><span class="nav-number">3.8.4.1.</span> <span class="nav-text">8.4.1 确定CUDA版本支持的VS版本</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-4-2-%E7%A1%AE%E5%AE%9ACUDA%E7%89%88%E6%9C%AC%E5%AF%B9%E5%BA%94%E7%9A%84cuDNN%E7%89%88%E6%9C%AC"><span class="nav-number">3.8.4.2.</span> <span class="nav-text">8.4.2 确定CUDA版本对应的cuDNN版本</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-4-3-%E6%B5%8B%E8%AF%95CUDA"><span class="nav-number">3.8.4.3.</span> <span class="nav-text">8.4.3 测试CUDA</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-4-4-%E5%AE%89%E8%A3%85CUDNN"><span class="nav-number">3.8.4.4.</span> <span class="nav-text">8.4.4 安装CUDNN</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.</span> <span class="nav-text">二、卷积层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BB%8E%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%88%B0%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.1.</span> <span class="nav-text">1 从全连接到卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-number">4.2.</span> <span class="nav-text">2 卷积层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.3.</span> <span class="nav-text">3 图像卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Q-amp-A"><span class="nav-number">4.4.</span> <span class="nav-text">4 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-Little-Questions"><span class="nav-number">4.4.1.</span> <span class="nav-text">4.1 Little Questions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-%E5%8D%B7%E7%A7%AF%E6%A6%82%E5%BF%B5%E7%9A%84%E8%A1%A5%E5%85%85"><span class="nav-number">4.4.2.</span> <span class="nav-text">4.2 卷积概念的补充</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93%EF%BC%9A%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%B8%89%E5%B1%82%E5%90%AB%E4%B9%89"><span class="nav-number">4.4.2.1.</span> <span class="nav-text">小结：卷积的三层含义</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%8D%B7%E7%A7%AF%E5%B1%82%E9%87%8C%E7%9A%84%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="nav-number">5.</span> <span class="nav-text">三、卷积层里的填充和步幅</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="nav-number">5.1.</span> <span class="nav-text">1 填充和步幅</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A"><span class="nav-number">5.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions"><span class="nav-number">5.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-%E8%BF%99%E5%87%A0%E4%B8%AA%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E9%87%8D%E8%A6%81%E7%A8%8B%E5%BA%A6%EF%BC%9F"><span class="nav-number">5.3.2.</span> <span class="nav-text">3.2 这几个超参数的重要程度？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E5%8D%B7%E7%A7%AF%E5%B1%82%E9%87%8C%E7%9A%84%E5%A4%9A%E8%BE%93%E5%85%A5%E5%A4%9A%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">6.</span> <span class="nav-text">四、卷积层里的多输入多输出通道</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93"><span class="nav-number">6.1.</span> <span class="nav-text">1 多输入输出通道</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">6.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-1"><span class="nav-number">6.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-1"><span class="nav-number">6.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">7.</span> <span class="nav-text">五、池化层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">7.1.</span> <span class="nav-text">1 池化层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="nav-number">7.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-2"><span class="nav-number">7.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-2"><span class="nav-number">7.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD%E3%80%81%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-LeNet"><span class="nav-number">8.</span> <span class="nav-text">六、经典卷积神经网络 LeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-LeNet"><span class="nav-number">8.1.</span> <span class="nav-text">1 LeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="nav-number">8.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-3"><span class="nav-number">8.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-3"><span class="nav-number">8.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83%E3%80%81%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-AlexNet"><span class="nav-number">9.</span> <span class="nav-text">七、经典卷积神经网络 AlexNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-AlexNet"><span class="nav-number">9.1.</span> <span class="nav-text">1 AlexNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-4"><span class="nav-number">9.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-4"><span class="nav-number">9.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-4"><span class="nav-number">9.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB%E3%80%81%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C-VGG"><span class="nav-number">10.</span> <span class="nav-text">八、使用块的网络 VGG</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-VGG"><span class="nav-number">10.1.</span> <span class="nav-text">1 VGG</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-5"><span class="nav-number">10.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D%E3%80%81%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C-NiN"><span class="nav-number">11.</span> <span class="nav-text">九、网络中的网络 NiN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-NiN"><span class="nav-number">11.1.</span> <span class="nav-text">1 NiN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-6"><span class="nav-number">11.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-5"><span class="nav-number">11.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-5"><span class="nav-number">11.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E3%80%81%E5%90%AB%E5%B9%B6%E8%A1%8C%E8%BF%9E%E6%8E%A5%E7%9A%84%E7%BD%91%E7%BB%9C-GoogLeNet"><span class="nav-number">12.</span> <span class="nav-text">十、含并行连接的网络 GoogLeNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-GoogLeNet"><span class="nav-number">12.1.</span> <span class="nav-text">1 GoogLeNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-7"><span class="nav-number">12.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-6"><span class="nav-number">12.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-6"><span class="nav-number">12.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">13.</span> <span class="nav-text">十一、批量归一化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="nav-number">13.1.</span> <span class="nav-text">1 批量归一化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-8"><span class="nav-number">13.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-7"><span class="nav-number">13.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-7"><span class="nav-number">13.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%BA%8C%E3%80%81%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%EF%BC%88ResNet%EF%BC%89"><span class="nav-number">14.</span> <span class="nav-text">十二、残差网络（ResNet）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-ResNet"><span class="nav-number">14.1.</span> <span class="nav-text">1 ResNet</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-9"><span class="nav-number">14.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-8"><span class="nav-number">14.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions-8"><span class="nav-number">14.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-ResNet-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E8%AE%AD%E7%BB%83%E5%87%BA1000%E5%B1%82%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="nav-number">14.4.</span> <span class="nav-text">4 ResNet 为什么能训练出1000层的模型？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%89%E3%80%81%E7%AB%9E%E8%B5%9B%EF%BC%9A%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB"><span class="nav-number">15.</span> <span class="nav-text">十三、竞赛：图片分类</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Moustache" src="/images/180-180.png"><p class="site-author-name" itemprop="name">Moustache</p><div class="site-description" itemprop="description">我是小胡子</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">78</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/hammerzer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hammerzer" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:stellar_lzu@163.com" title="E-Mail → mailto:stellar_lzu@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Chase</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">29:01</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.4" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JRehDoQ6pHXV1zKg09AMNLFt-gzGzoHsz',
      appKey     : 'cRAt4W15KiQdrIuHlQrRrtIl',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>