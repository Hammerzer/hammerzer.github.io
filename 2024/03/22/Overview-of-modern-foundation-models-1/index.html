<!DOCTYPE html><html lang="zh-CN"><head><script src="https://lib.sinaapp.com/js/jquery/1.7.2/jquery.min.js"></script><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/180-180.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/32-32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/16-16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hammerzer.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CONTENT OUTLINE常用网络结构概述"><meta property="og:type" content="article"><meta property="og:title" content="动手学深度学习-现代基础模型概览一"><meta property="og:url" content="https://hammerzer.github.io/2024/03/22/Overview-of-modern-foundation-models-1/index.html"><meta property="og:site_name" content="Moustache&#39;s Blog"><meta property="og:description" content="CONTENT OUTLINE常用网络结构概述"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hammerzer.github.io/.io//10-1-a.png"><meta property="og:image" content="https://hammerzer.github.io/.io//10-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//17-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//20-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//20-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//20-3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//20-4.png"><meta property="og:image" content="https://hammerzer.github.io/.io//20-5.png"><meta property="og:image" content="https://hammerzer.github.io/.io//21-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//21-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//22-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//22-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//23-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//23-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//24-1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//24-2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//24-3.png"><meta property="article:published_time" content="2024-03-22T03:27:49.000Z"><meta property="article:modified_time" content="2025-01-19T03:19:17.292Z"><meta property="article:author" content="Moustache"><meta property="article:tag" content="Deep Learning"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hammerzer.github.io/.io//10-1-a.png"><link rel="canonical" href="https://hammerzer.github.io/2024/03/22/Overview-of-modern-foundation-models-1/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>动手学深度学习-现代基础模型概览一 | Moustache's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Moustache's Blog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Moustache's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">小胡子的私人空间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">78</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hammerzer.github.io/2024/03/22/Overview-of-modern-foundation-models-1/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/180-180.png"><meta itemprop="name" content="Moustache"><meta itemprop="description" content="我是小胡子"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Moustache's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">动手学深度学习-现代基础模型概览一</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-03-22 11:27:49" itemprop="dateCreated datePublished" datetime="2024-03-22T11:27:49+08:00">2024-03-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 11:19:17" itemprop="dateModified" datetime="2025-01-19T11:19:17+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a> </span></span><span id="/2024/03/22/Overview-of-modern-foundation-models-1/" class="post-meta-item leancloud_visitors" data-flag-title="动手学深度学习-现代基础模型概览一" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> <span>℃</span> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2024/03/22/Overview-of-modern-foundation-models-1/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2024/03/22/Overview-of-modern-foundation-models-1/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>32k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>29 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CONTENT-OUTLINE"><a href="#CONTENT-OUTLINE" class="headerlink" title="CONTENT OUTLINE"></a>CONTENT OUTLINE</h2><p><span style="background:#52e6fc"><i class="fas fa-star-half-alt" style="margin-right:10px"></i>常用网络结构概述</span></p><span id="more"></span><h2 id="〇、目录"><a href="#〇、目录" class="headerlink" title="〇、目录"></a>〇、目录</h2><ul><li>物体检测算法</li><li>SSD</li><li>语义分割和数据集</li><li>转置卷积</li><li>转置卷积也是卷积</li><li>FCN</li><li>样式迁移</li><li>序列模型</li><li>九 文本预处理</li><li>十 语言模型</li><li>循环神经网络RNN</li><li>门控循环单元GRU</li><li>长短期记忆网络LSTM</li><li>深层循环网络</li><li>双向循环神经网络</li><li>机器翻译与数据集</li><li>十七编码器-解码器结构</li><li>序列到序列学习seq2seq</li><li>束搜索</li><li>二十注意力机制</li><li>注意力分数</li><li>使用注意力机制的seq2seq</li><li>自注意力</li><li>Transformer</li><li>BERT预训练</li><li>BERT微调</li><li>优化算法</li></ul><br><h2 id="一-物体检测算法"><a href="#一-物体检测算法" class="headerlink" title="一 物体检测算法"></a>一 物体检测算法</h2><br><br><br><br><h2 id="九-文本预处理"><a href="#九-文本预处理" class="headerlink" title="九 文本预处理"></a>九 文本预处理</h2><h3 id="1-代码实现"><a href="#1-代码实现" class="headerlink" title="1 代码实现"></a>1 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>将数据集读取到由文本行组成的列表中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">d2l.DATA_HUB[<span class="string">&#x27;time_machine&#x27;</span>] = (d2l.DATA_URL + <span class="string">&#x27;timemachine.txt&#x27;</span>,</span><br><span class="line">                                <span class="string">&#x27;090b5e7e70c295757f55df93cb0a180b9691891a&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_time_machine</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;Load the time machine dataset into a list of text lines.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(d2l.download(<span class="string">&#x27;time_machine&#x27;</span>), <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> [re.sub(<span class="string">&#x27;[^A-Za-z]+&#x27;</span>, <span class="string">&#x27; &#x27;</span>, line).strip().lower() <span class="keyword">for</span> line <span class="keyword">in</span> lines] <span class="comment"># 简化文本【暴力预处理】</span></span><br><span class="line"></span><br><span class="line">lines = read_time_machine()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(lines[<span class="number">10</span>])</span><br><span class="line">      </span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="comment"># text lines: 3221</span></span><br><span class="line">the time machine by h g wells</span><br><span class="line">twinkled <span class="keyword">and</span> his usually pale face was flushed <span class="keyword">and</span> animated the  </span><br></pre></td></tr></table></figure><p>每个文本序列被拆分成一个标记列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">lines, token=<span class="string">&#x27;word&#x27;</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;将文本行拆分为单词或字符标记。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">&#x27;word&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [line.split() <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">elif</span> token == <span class="string">&#x27;char&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">list</span>(line) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;错误：未知令牌类型：&#x27;</span> + token)</span><br><span class="line"></span><br><span class="line">tokens = tokenize(lines)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">11</span>):</span><br><span class="line">    <span class="built_in">print</span>(tokens[i])</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;i&#x27;</span>]</span><br><span class="line">[]</span><br><span class="line">[]</span><br><span class="line">[<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>, <span class="string">&#x27;for&#x27;</span>, <span class="string">&#x27;so&#x27;</span>, <span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;will&#x27;</span>, <span class="string">&#x27;be&#x27;</span>, <span class="string">&#x27;convenient&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;speak&#x27;</span>, <span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;him&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;expounding&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;recondite&#x27;</span>, <span class="string">&#x27;matter&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;us&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;grey&#x27;</span>, <span class="string">&#x27;eyes&#x27;</span>, <span class="string">&#x27;shone&#x27;</span>, <span class="string">&#x27;and&#x27;</span>]</span><br><span class="line">[<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br></pre></td></tr></table></figure><p>构建一个字典，通常也叫做<em>词表</em>（vocabulary），用来将字符串标记映射到从00开始的数字索引中【模型训练都是基于下标的】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Vocab</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;文本词表&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, tokens=<span class="literal">None</span>, min_freq=<span class="number">0</span>, reserved_tokens=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            tokens = []</span><br><span class="line">        <span class="keyword">if</span> reserved_tokens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            reserved_tokens = []</span><br><span class="line">        counter = count_corpus(tokens)</span><br><span class="line">        <span class="variable language_">self</span>.token_freqs = <span class="built_in">sorted</span>(counter.items(), key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>],</span><br><span class="line">                                  reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="variable language_">self</span>.unk, uniq_tokens = <span class="number">0</span>, [<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] + reserved_tokens</span><br><span class="line">        uniq_tokens += [</span><br><span class="line">            token <span class="keyword">for</span> token, freq <span class="keyword">in</span> <span class="variable language_">self</span>.token_freqs</span><br><span class="line">            <span class="keyword">if</span> freq &gt;= min_freq <span class="keyword">and</span> token <span class="keyword">not</span> <span class="keyword">in</span> uniq_tokens]</span><br><span class="line">        <span class="variable language_">self</span>.idx_to_token, <span class="variable language_">self</span>.token_to_idx = [], <span class="built_in">dict</span>()</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> uniq_tokens:</span><br><span class="line">            <span class="variable language_">self</span>.idx_to_token.append(token)</span><br><span class="line">            <span class="variable language_">self</span>.token_to_idx[token] = <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token) - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.idx_to_token)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(tokens, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.token_to_idx.get(tokens, <span class="variable language_">self</span>.unk)</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.__getitem__(token) <span class="keyword">for</span> token <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">to_tokens</span>(<span class="params">self, indices</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(indices, (<span class="built_in">list</span>, <span class="built_in">tuple</span>)):</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.idx_to_token[indices]</span><br><span class="line">        <span class="keyword">return</span> [<span class="variable language_">self</span>.idx_to_token[index] <span class="keyword">for</span> index <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_corpus</span>(<span class="params">tokens</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;统计标记的频率。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(tokens) == <span class="number">0</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(tokens[<span class="number">0</span>], <span class="built_in">list</span>):</span><br><span class="line">        tokens = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">return</span> collections.Counter(tokens)</span><br></pre></td></tr></table></figure><p>构建词汇表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vocab = Vocab(tokens)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(vocab.token_to_idx.items())[:<span class="number">10</span>])</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">[(<span class="string">&#x27;&lt;unk&gt;&#x27;</span>, <span class="number">0</span>), (<span class="string">&#x27;the&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;i&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;and&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;of&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;to&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;was&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;in&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;that&#x27;</span>, <span class="number">9</span>)]</span><br></pre></td></tr></table></figure><p>将每一行文本转换成一个数字索引列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">10</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;words:&#x27;</span>, tokens[i])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;indices:&#x27;</span>, vocab[tokens[i]])</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">words: [<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>, <span class="string">&#x27;by&#x27;</span>, <span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;wells&#x27;</span>]</span><br><span class="line">indices: [<span class="number">1</span>, <span class="number">19</span>, <span class="number">50</span>, <span class="number">40</span>, <span class="number">2183</span>, <span class="number">2184</span>, <span class="number">400</span>]</span><br><span class="line">words: [<span class="string">&#x27;twinkled&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;his&#x27;</span>, <span class="string">&#x27;usually&#x27;</span>, <span class="string">&#x27;pale&#x27;</span>, <span class="string">&#x27;face&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;flushed&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;animated&#x27;</span>, <span class="string">&#x27;the&#x27;</span>]</span><br><span class="line">indices: [<span class="number">2186</span>, <span class="number">3</span>, <span class="number">25</span>, <span class="number">1044</span>, <span class="number">362</span>, <span class="number">113</span>, <span class="number">7</span>, <span class="number">1421</span>, <span class="number">3</span>, <span class="number">1045</span>, <span class="number">1</span>]</span><br></pre></td></tr></table></figure><p>将所有内容打包到<code>load_corpus_time_machine</code>函数中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_corpus_time_machine</span>(<span class="params">max_tokens=-<span class="number">1</span></span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的标记索引列表和词汇表。&quot;&quot;&quot;</span></span><br><span class="line">    lines = read_time_machine()</span><br><span class="line">    tokens = tokenize(lines, <span class="string">&#x27;char&#x27;</span>)</span><br><span class="line">    vocab = Vocab(tokens)</span><br><span class="line">    corpus = [vocab[token] <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">    <span class="keyword">if</span> max_tokens &gt; <span class="number">0</span>:</span><br><span class="line">        corpus = corpus[:max_tokens]</span><br><span class="line">    <span class="keyword">return</span> corpus, vocab</span><br><span class="line"></span><br><span class="line">corpus, vocab = load_corpus_time_machine()</span><br><span class="line"><span class="built_in">len</span>(corpus), <span class="built_in">len</span>(vocab)</span><br><span class="line"><span class="comment">#Output</span></span><br><span class="line">(<span class="number">170580</span>, <span class="number">28</span>)</span><br></pre></td></tr></table></figure><br><h2 id="十-语言模型"><a href="#十-语言模型" class="headerlink" title="十 语言模型"></a>十 语言模型</h2><h3 id="1-语言模型"><a href="#1-语言模型" class="headerlink" title="1 语言模型"></a>1 语言模型</h3><p><img src="/.io//10-1-a.png"></p><p>总结：</p><ul><li>语言模型估计文本序列的联合概率</li><li>使用统计方法时常采用n元方法</li></ul><br><h3 id="2-代码实现"><a href="#2-代码实现" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">tokens = d2l.tokenize(d2l.read_time_machine())</span><br><span class="line">corpus = [token <span class="keyword">for</span> line <span class="keyword">in</span> tokens <span class="keyword">for</span> token <span class="keyword">in</span> line]</span><br><span class="line">vocab = d2l.Vocab(corpus)</span><br><span class="line">vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">[(<span class="string">&#x27;the&#x27;</span>, <span class="number">2261</span>),</span><br><span class="line"> (<span class="string">&#x27;i&#x27;</span>, <span class="number">1267</span>),</span><br><span class="line"> (<span class="string">&#x27;and&#x27;</span>, <span class="number">1245</span>),</span><br><span class="line"> (<span class="string">&#x27;of&#x27;</span>, <span class="number">1155</span>),</span><br><span class="line"> (<span class="string">&#x27;a&#x27;</span>, <span class="number">816</span>),</span><br><span class="line"> (<span class="string">&#x27;to&#x27;</span>, <span class="number">695</span>),</span><br><span class="line"> (<span class="string">&#x27;was&#x27;</span>, <span class="number">552</span>),</span><br><span class="line"> (<span class="string">&#x27;in&#x27;</span>, <span class="number">541</span>),</span><br><span class="line"> (<span class="string">&#x27;that&#x27;</span>, <span class="number">443</span>),</span><br><span class="line"> (<span class="string">&#x27;my&#x27;</span>, <span class="number">440</span>)]</span><br></pre></td></tr></table></figure><p>最流行的词 被称为<em>停用词</em> 画出的词频图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> vocab.token_freqs]</span><br><span class="line">d2l.plot(freqs, xlabel=<span class="string">&#x27;token: x&#x27;</span>, ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         yscale=<span class="string">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>见下图左</p></blockquote><p><img src="/.io//10-2.png"></p><p>其他的词元组合，比如二元语法、三元语法等等，又会如何呢？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bigram_tokens = [pair <span class="keyword">for</span> pair <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">1</span>], corpus[<span class="number">1</span>:])]</span><br><span class="line">bigram_vocab = d2l.Vocab(bigram_tokens)</span><br><span class="line">bigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">[((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">309</span>),</span><br><span class="line"> ((<span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">169</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;had&#x27;</span>), <span class="number">130</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">112</span>),</span><br><span class="line"> ((<span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">109</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>), <span class="number">102</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>), <span class="number">99</span>),</span><br><span class="line"> ((<span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">85</span>),</span><br><span class="line"> ((<span class="string">&#x27;as&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), <span class="number">78</span>),</span><br><span class="line"> ((<span class="string">&#x27;of&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">73</span>)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">trigram_tokens = [</span><br><span class="line">    triple <span class="keyword">for</span> triple <span class="keyword">in</span> <span class="built_in">zip</span>(corpus[:-<span class="number">2</span>], corpus[<span class="number">1</span>:-<span class="number">1</span>], corpus[<span class="number">2</span>:])]</span><br><span class="line">trigram_vocab = d2l.Vocab(trigram_tokens)</span><br><span class="line">trigram_vocab.token_freqs[:<span class="number">10</span>]</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">[((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;traveller&#x27;</span>), <span class="number">59</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;time&#x27;</span>, <span class="string">&#x27;machine&#x27;</span>), <span class="number">30</span>),</span><br><span class="line"> ((<span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;medical&#x27;</span>, <span class="string">&#x27;man&#x27;</span>), <span class="number">24</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">16</span>),</span><br><span class="line"> ((<span class="string">&#x27;it&#x27;</span>, <span class="string">&#x27;was&#x27;</span>, <span class="string">&#x27;a&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;here&#x27;</span>, <span class="string">&#x27;and&#x27;</span>, <span class="string">&#x27;there&#x27;</span>), <span class="number">15</span>),</span><br><span class="line"> ((<span class="string">&#x27;seemed&#x27;</span>, <span class="string">&#x27;to&#x27;</span>, <span class="string">&#x27;me&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;did&#x27;</span>, <span class="string">&#x27;not&#x27;</span>), <span class="number">14</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;saw&#x27;</span>, <span class="string">&#x27;the&#x27;</span>), <span class="number">13</span>),</span><br><span class="line"> ((<span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;began&#x27;</span>, <span class="string">&#x27;to&#x27;</span>), <span class="number">13</span>)]</span><br></pre></td></tr></table></figure><p>直观地对比三种模型中的标记频率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> bigram_vocab.token_freqs]</span><br><span class="line">trigram_freqs = [freq <span class="keyword">for</span> token, freq <span class="keyword">in</span> trigram_vocab.token_freqs]</span><br><span class="line">d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel=<span class="string">&#x27;token: x&#x27;</span>,</span><br><span class="line">         ylabel=<span class="string">&#x27;frequency: n(x)&#x27;</span>, xscale=<span class="string">&#x27;log&#x27;</span>, yscale=<span class="string">&#x27;log&#x27;</span>,</span><br><span class="line">         legend=[<span class="string">&#x27;unigram&#x27;</span>, <span class="string">&#x27;bigram&#x27;</span>, <span class="string">&#x27;trigram&#x27;</span>])</span><br></pre></td></tr></table></figure><blockquote><p>见上图右</p></blockquote><p>随机地生成一个小批量数据的特征和标签以供读取。 在随机采样中，每个样本都是在原始的长序列上任意捕获的子序列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_random</span>(<span class="params">corpus, batch_size, num_steps</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用随机抽样生成一个小批量子序列。&quot;&quot;&quot;</span></span><br><span class="line">    corpus = corpus[random.randint(<span class="number">0</span>, num_steps - <span class="number">1</span>):]</span><br><span class="line">    num_subseqs = (<span class="built_in">len</span>(corpus) - <span class="number">1</span>) // num_steps</span><br><span class="line">    initial_indices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">0</span>, num_subseqs * num_steps, num_steps))</span><br><span class="line">    random.shuffle(initial_indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> corpus[pos:pos + num_steps]</span><br><span class="line"></span><br><span class="line">    num_batches = num_subseqs // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size * num_batches, batch_size):</span><br><span class="line">        initial_indices_per_batch = initial_indices[i:i + batch_size]</span><br><span class="line">        X = [data(j) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        Y = [data(j + <span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> initial_indices_per_batch]</span><br><span class="line">        <span class="keyword">yield</span> torch.tensor(X), torch.tensor(Y)</span><br></pre></td></tr></table></figure><p>生成一个从 0 到 34 的序列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">my_seq = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">35</span>))</span><br><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_random(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"><span class="comment">#output   </span></span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>]])</span><br><span class="line">X:  tensor([[<span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>],</span><br><span class="line">        [<span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>]]) </span><br><span class="line">Y: tensor([[<span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>],</span><br><span class="line">        [<span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>]])</span><br></pre></td></tr></table></figure><p>保证两个相邻的小批量中的子序列在原始序列上也是相邻的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seq_data_iter_sequential</span>(<span class="params">corpus, batch_size, num_steps</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用顺序分区生成一个小批量子序列。&quot;&quot;&quot;</span></span><br><span class="line">    offset = random.randint(<span class="number">0</span>, num_steps)</span><br><span class="line">    num_tokens = ((<span class="built_in">len</span>(corpus) - offset - <span class="number">1</span>) // batch_size) * batch_size</span><br><span class="line">    Xs = torch.tensor(corpus[offset:offset + num_tokens])</span><br><span class="line">    Ys = torch.tensor(corpus[offset + <span class="number">1</span>:offset + <span class="number">1</span> + num_tokens])</span><br><span class="line">    Xs, Ys = Xs.reshape(batch_size, -<span class="number">1</span>), Ys.reshape(batch_size, -<span class="number">1</span>)</span><br><span class="line">    num_batches = Xs.shape[<span class="number">1</span>] // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_steps * num_batches, num_steps):</span><br><span class="line">        X = Xs[:, i:i + num_steps]</span><br><span class="line">        Y = Ys[:, i:i + num_steps]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure><p>读取每个小批量的子序列的特征 <code>X</code> 和标签 <code>Y</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, Y <span class="keyword">in</span> seq_data_iter_sequential(my_seq, batch_size=<span class="number">2</span>, num_steps=<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>, X, <span class="string">&#x27;\nY:&#x27;</span>, Y)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">X:  tensor([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">        [<span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>]])</span><br><span class="line">X:  tensor([[ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">22</span>, <span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>]]) </span><br><span class="line">Y: tensor([[ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">23</span>, <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>]])</span><br><span class="line">X:  tensor([[<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>],</span><br><span class="line">        [<span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>]]) </span><br><span class="line">Y: tensor([[<span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>],</span><br><span class="line">        [<span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SeqDataLoader</span>:  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;加载序列数据的迭代器。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, batch_size, num_steps, use_random_iter, max_tokens</span>):</span><br><span class="line">        <span class="keyword">if</span> use_random_iter:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_random</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.data_iter_fn = d2l.seq_data_iter_sequential</span><br><span class="line">        <span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.vocab = d2l.load_corpus_time_machine(max_tokens)</span><br><span class="line">        <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps = batch_size, num_steps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data_iter_fn(<span class="variable language_">self</span>.corpus, <span class="variable language_">self</span>.batch_size, <span class="variable language_">self</span>.num_steps)</span><br></pre></td></tr></table></figure><p>最后，我们定义了一个函数 <code>load_data_time_machine</code> ，它同时返回数据迭代器和词汇表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_data_time_machine</span>(<span class="params">batch_size, num_steps,  </span></span><br><span class="line"><span class="params">                           use_random_iter=<span class="literal">False</span>, max_tokens=<span class="number">10000</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;返回时光机器数据集的迭代器和词汇表。&quot;&quot;&quot;</span></span><br><span class="line">    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,</span><br><span class="line">                              max_tokens)</span><br><span class="line">    <span class="keyword">return</span> data_iter, data_iter.vocab</span><br></pre></td></tr></table></figure><h3 id="3-Q-amp-A"><a href="#3-Q-amp-A" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><blockquote><p>且略</p></blockquote><br><br><br><br><br><br><h2 id="十七-编码器-解码器结构"><a href="#十七-编码器-解码器结构" class="headerlink" title="十七 编码器-解码器结构"></a>十七 编码器-解码器结构</h2><h3 id="1-编码器-解码器"><a href="#1-编码器-解码器" class="headerlink" title="1 编码器-解码器"></a>1 编码器-解码器</h3><p><img src="/.io//17-1.png"></p><h3 id="2-代码实现-1"><a href="#2-代码实现-1" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#编码器</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器结构的基本编码器接口。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line">        </span><br><span class="line"><span class="comment">#解码器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器结构的基本解码器接口。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, *args</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>合并编码器和解码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;编码器-解码器结构的基类。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_X, dec_X, *args</span>):</span><br><span class="line">        enc_outputs = <span class="variable language_">self</span>.encoder(enc_X, *args)</span><br><span class="line">        dec_state = <span class="variable language_">self</span>.decoder.init_state(enc_outputs, *args)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(dec_X, dec_state)</span><br></pre></td></tr></table></figure><br><h2 id="十八-序列到序列学习seq2seq"><a href="#十八-序列到序列学习seq2seq" class="headerlink" title="十八 序列到序列学习seq2seq"></a>十八 序列到序列学习seq2seq</h2><br><h2 id="十九-束搜索"><a href="#十九-束搜索" class="headerlink" title="十九 束搜索"></a>十九 束搜索</h2><br><h2 id="二十-注意力机制"><a href="#二十-注意力机制" class="headerlink" title="二十 注意力机制"></a>二十 注意力机制</h2><h3 id="1-Attention-Mechanism"><a href="#1-Attention-Mechanism" class="headerlink" title="1 Attention Mechanism"></a>1 Attention Mechanism</h3><blockquote><p>不随意：潜在特征和联系；随意：注意力、想法、查询query</p></blockquote><p><img src="/.io//20-1.png"></p><p>总结：</p><ul><li>心理学认为人通过随意线索和不随意线索选择注意点</li><li>注意力机制中，通过query（随意线索）和key（不随意线索）来有偏向性的选择输入<ul><li>可以一般的写作<code>f(x)=Σα(x,xi)yi</code>，这里<code>α(x,xi)</code> 是注意力权重</li><li>早在60年代就有非参数的注意力机制</li><li>接下来我们会介绍多个不同的权重设计</li></ul></li></ul><h3 id="2-代码实现-2"><a href="#2-代码实现-2" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>注意力汇聚：Nadaraya-Watson 核回归</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>生成数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_train = <span class="number">50</span></span><br><span class="line">x_train, _ = torch.sort(torch.rand(n_train) * <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span> * torch.sin(x) + x**<span class="number">0.8</span></span><br><span class="line"></span><br><span class="line">y_train = f(x_train) + torch.normal(<span class="number">0.0</span>, <span class="number">0.5</span>, (n_train,))</span><br><span class="line">x_test = torch.arange(<span class="number">0</span>, <span class="number">5</span>, <span class="number">0.1</span>)</span><br><span class="line">y_truth = f(x_test)</span><br><span class="line">n_test = <span class="built_in">len</span>(x_test)</span><br><span class="line">n_test</span><br><span class="line"><span class="comment">#Output:50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plot_kernel_reg</span>(<span class="params">y_hat</span>):</span><br><span class="line">    d2l.plot(x_test, [y_truth, y_hat], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, legend=[<span class="string">&#x27;Truth&#x27;</span>, <span class="string">&#x27;Pred&#x27;</span>],</span><br><span class="line">             xlim=[<span class="number">0</span>, <span class="number">5</span>], ylim=[-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    d2l.plt.plot(x_train, y_train, <span class="string">&#x27;o&#x27;</span>, alpha=<span class="number">0.5</span>);</span><br><span class="line"></span><br><span class="line">y_hat = torch.repeat_interleave(y_train.mean(), n_test)</span><br><span class="line">plot_kernel_reg(y_hat)<span class="comment">#如图左</span></span><br></pre></td></tr></table></figure><p><img src="/.io//20-2.png"></p><p>非参数注意力汇聚【pooling】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_repeat = x_test.repeat_interleave(n_train).reshape((-<span class="number">1</span>, n_train))</span><br><span class="line">attention_weights = nn.functional.softmax(-(X_repeat - x_train)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">y_hat = torch.matmul(attention_weights, y_train)</span><br><span class="line">plot_kernel_reg(y_hat)<span class="comment">#如图右</span></span><br></pre></td></tr></table></figure><blockquote><p>如图右，较集中的样本权重更大</p></blockquote><p>注意力权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">    xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>, ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/.io//20-3.png"></p><p>带参数注意力汇聚 假定两个张量的形状分别是<code>(n,a,b)</code> 和 <code>(n,b,c)</code> ，它们的批量矩阵乘法输出的形状为 <code>(n,a,c)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">Y = torch.ones((<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>))</span><br><span class="line">torch.bmm(X, Y).shape <span class="comment">#注意：bmm为批量mm</span></span><br><span class="line"><span class="comment">#output:torch.Size([2, 1, 6])</span></span><br></pre></td></tr></table></figure><p>使用小批量矩阵乘法来计算小批量数据中的加权平均值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.ones((<span class="number">2</span>, <span class="number">10</span>)) * <span class="number">0.1</span></span><br><span class="line">values = torch.arange(<span class="number">20.0</span>).reshape((<span class="number">2</span>, <span class="number">10</span>))</span><br><span class="line">torch.bmm(weights.unsqueeze(<span class="number">1</span>), values.unsqueeze(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line">tensor([[[ <span class="number">4.5000</span>]],</span><br><span class="line">        [[<span class="number">14.5000</span>]]])</span><br></pre></td></tr></table></figure><p>带参数的注意力汇聚</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NWKernelRegression</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.w = nn.Parameter(torch.rand((<span class="number">1</span>,), requires_grad=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values</span>):</span><br><span class="line">        queries = queries.repeat_interleave(keys.shape[<span class="number">1</span>]).reshape(</span><br><span class="line">            (-<span class="number">1</span>, keys.shape[<span class="number">1</span>]))</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = nn.functional.softmax(</span><br><span class="line">            -((queries - keys) * <span class="variable language_">self</span>.w)**<span class="number">2</span> / <span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.attention_weights.unsqueeze(<span class="number">1</span>),</span><br><span class="line">                         values.unsqueeze(-<span class="number">1</span>)).reshape(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>将训练数据集转换为键和值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_tile = x_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">Y_tile = y_train.repeat((n_train, <span class="number">1</span>))</span><br><span class="line">keys = X_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape(</span><br><span class="line">    (n_train, -<span class="number">1</span>))</span><br><span class="line">values = Y_tile[(<span class="number">1</span> - torch.eye(n_train)).<span class="built_in">type</span>(torch.<span class="built_in">bool</span>)].reshape(</span><br><span class="line">    (n_train, -<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练带参数的注意力汇聚模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = NWKernelRegression()</span><br><span class="line">loss = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">trainer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line">animator = d2l.Animator(xlabel=<span class="string">&#x27;epoch&#x27;</span>, ylabel=<span class="string">&#x27;loss&#x27;</span>, xlim=[<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    trainer.zero_grad()</span><br><span class="line">    l = loss(net(x_train, keys, values), y_train) / <span class="number">2</span></span><br><span class="line">    l.<span class="built_in">sum</span>().backward()</span><br><span class="line">    trainer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>, loss <span class="subst">&#123;<span class="built_in">float</span>(l.<span class="built_in">sum</span>()):<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    animator.add(epoch + <span class="number">1</span>, <span class="built_in">float</span>(l.<span class="built_in">sum</span>()))</span><br></pre></td></tr></table></figure><p><img src="/.io//20-4.png"></p><p>预测结果绘制</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keys = x_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">values = y_train.repeat((n_test, <span class="number">1</span>))</span><br><span class="line">y_hat = net(x_test, keys, values).unsqueeze(<span class="number">1</span>).detach()</span><br><span class="line">plot_kernel_reg(y_hat)</span><br><span class="line"><span class="comment"># 如图左</span></span><br></pre></td></tr></table></figure><p><img src="/.io//20-5.png"></p><p>曲线在注意力权重较大的区域变得更不平滑</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    net.attention_weights.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>),</span><br><span class="line">    xlabel=<span class="string">&#x27;Sorted training inputs&#x27;</span>, ylabel=<span class="string">&#x27;Sorted testing inputs&#x27;</span>)</span><br><span class="line"><span class="comment"># 如图右</span></span><br></pre></td></tr></table></figure><br><h2 id="二十一-注意力分数"><a href="#二十一-注意力分数" class="headerlink" title="二十一 注意力分数"></a>二十一 注意力分数</h2><h3 id="1-注意力分数"><a href="#1-注意力分数" class="headerlink" title="1 注意力分数"></a>1 注意力分数</h3><p><img src="/.io//21-1.png"></p><p>总结：</p><ul><li>注意力分数是query和key的相似度，注意力权重分数是softmax结果【表示给每一个key-value pair多少个多注意力】</li><li>两种常见的分数计算：<ul><li>将query和key合并起来进入一个单输出单隐藏层的MLP</li><li>直接将query和key做内积</li></ul></li></ul><h3 id="2-代码实现-3"><a href="#2-代码实现-3" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>注意力打分函数，遮蔽softmax操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 屏蔽padding部分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">masked_softmax</span>(<span class="params">X, valid_lens</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;通过在最后一个轴上遮蔽元素来执行 softmax 操作&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> valid_lens <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shape = X.shape</span><br><span class="line">        <span class="keyword">if</span> valid_lens.dim() == <span class="number">1</span>:</span><br><span class="line">            valid_lens = torch.repeat_interleave(valid_lens, shape[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            valid_lens = valid_lens.reshape(-<span class="number">1</span>)</span><br><span class="line">        X = d2l.sequence_mask(X.reshape(-<span class="number">1</span>, shape[-<span class="number">1</span>]), valid_lens,</span><br><span class="line">                              value=-<span class="number">1e6</span>)</span><br><span class="line">        <span class="keyword">return</span> nn.functional.softmax(X.reshape(shape), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>演示此函数是如何工作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="comment"># 共两个矩阵，第一个矩阵前两列是有效，第二个矩阵前三列有效</span></span><br><span class="line"><span class="comment">#output：</span></span><br><span class="line">tensor([[[<span class="number">0.5089</span>, <span class="number">0.4911</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.5767</span>, <span class="number">0.4233</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.2270</span>, <span class="number">0.4099</span>, <span class="number">0.3630</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2809</span>, <span class="number">0.3901</span>, <span class="number">0.3289</span>, <span class="number">0.0000</span>]]])</span><br><span class="line">masked_softmax(torch.rand(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>), torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">4</span>]]))</span><br><span class="line"><span class="comment">#output：</span></span><br><span class="line">tensor([[[<span class="number">1.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2178</span>, <span class="number">0.4802</span>, <span class="number">0.3020</span>, <span class="number">0.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">0.3727</span>, <span class="number">0.6273</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.2801</span>, <span class="number">0.2185</span>, <span class="number">0.2313</span>, <span class="number">0.2701</span>]]])</span><br></pre></td></tr></table></figure><p>加性注意力</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdditiveAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;加性注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, num_hiddens, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AdditiveAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.W_k = nn.Linear(key_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.W_q = nn.Linear(query_size, num_hiddens, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w_v = nn.Linear(num_hiddens, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens</span>):</span><br><span class="line">        queries, keys = <span class="variable language_">self</span>.W_q(queries), <span class="variable language_">self</span>.W_k(keys)</span><br><span class="line">        <span class="comment"># 长度不同的做法：在第二维加一维</span></span><br><span class="line">        features = queries.unsqueeze(<span class="number">2</span>) + keys.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 做激活</span></span><br><span class="line">        features = torch.tanh(features)</span><br><span class="line">        <span class="comment"># 去掉最后一个无效维度</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.w_v(features).squeeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br></pre></td></tr></table></figure><p>演示上面的<code>AdditiveAttention</code>类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">queries, keys = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">20</span>)), torch.ones((<span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>))</span><br><span class="line">values = torch.arange(<span class="number">40</span>, dtype=torch.float32).reshape(<span class="number">1</span>, <span class="number">10</span>,</span><br><span class="line">                                                       <span class="number">4</span>).repeat(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">valid_lens = torch.tensor([<span class="number">2</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">attention = AdditiveAttention(key_size=<span class="number">2</span>, query_size=<span class="number">20</span>, num_hiddens=<span class="number">8</span>,</span><br><span class="line">                              dropout=<span class="number">0.1</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"><span class="comment">#output：</span></span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]], grad_fn=&lt;BmmBackward0&gt;)</span><br></pre></td></tr></table></figure><p>注意力权重【如图左】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/.io//21-2.png"></p><p>缩放点积注意力</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;缩放点积注意力&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DotProductAttention, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, queries, keys, values, valid_lens=<span class="literal">None</span></span>):</span><br><span class="line">        d = queries.shape[-<span class="number">1</span>]</span><br><span class="line">        scores = torch.bmm(queries, keys.transpose(<span class="number">1</span>, <span class="number">2</span>)) / math.sqrt(d)</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = masked_softmax(scores, valid_lens)</span><br><span class="line">        <span class="keyword">return</span> torch.bmm(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.attention_weights), values)</span><br></pre></td></tr></table></figure><p>演示上述的<code>DotProductAttention</code>类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queries = torch.normal(<span class="number">0</span>, <span class="number">1</span>, (<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">attention = DotProductAttention(dropout=<span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line">attention(queries, keys, values, valid_lens)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">tensor([[[ <span class="number">2.0000</span>,  <span class="number">3.0000</span>,  <span class="number">4.0000</span>,  <span class="number">5.0000</span>]],</span><br><span class="line"></span><br><span class="line">        [[<span class="number">10.0000</span>, <span class="number">11.0000</span>, <span class="number">12.0000</span>, <span class="number">13.0000</span>]]])</span><br></pre></td></tr></table></figure><p>均匀的注意力权重【如图右】</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(attention.attention_weights.reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">10</span>)),</span><br><span class="line">                  xlabel=<span class="string">&#x27;Keys&#x27;</span>, ylabel=<span class="string">&#x27;Queries&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="3-Q-amp-A-1"><a href="#3-Q-amp-A-1" class="headerlink" title="3 Q&amp;A"></a>3 Q&amp;A</h3><h4 id="3-1-Little-Questions"><a href="#3-1-Little-Questions" class="headerlink" title="3.1 Little Questions"></a>3.1 Little Questions</h4><ul><li>query、key、value的区别？在不同网络中，这三者被赋予了不同的含义。</li><li>query和key的相似度作为注意力分数，要查询一个query的值，应该看谁的value比较相近，其重要性相应更高，注意力权重也更高</li></ul><br><h2 id="二十二-使用注意力机制的seq2seq"><a href="#二十二-使用注意力机制的seq2seq" class="headerlink" title="二十二 使用注意力机制的seq2seq"></a>二十二 使用注意力机制的seq2seq</h2><h3 id="1-seq2seq"><a href="#1-seq2seq" class="headerlink" title="1 seq2seq"></a>1 seq2seq</h3><p><img src="/.io//22-1.png"></p><p>总结：</p><ul><li>Seq2seq中通过隐状态在编码器和解码器中传递信息</li><li>注意力机制可以根据解码器RNN的输出来匹配到合适的编码器RNN的输出来更有效的传递信息</li><li></li></ul><h3 id="2-代码实现-4"><a href="#2-代码实现-4" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><p>Bahdanau 注意力</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>带有注意力机制的解码器基本接口</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionDecoder</span>(d2l.Decoder):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;带有注意力机制的解码器基本接口&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AttentionDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br></pre></td></tr></table></figure><p>实现带有Bahdanau注意力的循环神经网络解码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2SeqAttentionDecoder</span>(<span class="title class_ inherited__">AttentionDecoder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span><br><span class="line"><span class="params">                 dropout=<span class="number">0</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2SeqAttentionDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.AdditiveAttention(num_hiddens, num_hiddens,</span><br><span class="line">                                               num_hiddens, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        <span class="variable language_">self</span>.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,</span><br><span class="line">                          dropout=dropout)</span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        outputs, hidden_state = enc_outputs</span><br><span class="line">        <span class="keyword">return</span> (outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state, enc_valid_lens)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, hidden_state, enc_valid_lens = state</span><br><span class="line">        X = <span class="variable language_">self</span>.embedding(X).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">        outputs, <span class="variable language_">self</span>._attention_weights = [], []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> X:</span><br><span class="line">            query = torch.unsqueeze(hidden_state[-<span class="number">1</span>], dim=<span class="number">1</span>)</span><br><span class="line">            context = <span class="variable language_">self</span>.attention(query, enc_outputs, enc_outputs,</span><br><span class="line">                                     enc_valid_lens)</span><br><span class="line">            x = torch.cat((context, torch.unsqueeze(x, dim=<span class="number">1</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">            out, hidden_state = <span class="variable language_">self</span>.rnn(x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), hidden_state)</span><br><span class="line">            outputs.append(out)</span><br><span class="line">            <span class="variable language_">self</span>._attention_weights.append(<span class="variable language_">self</span>.attention.attention_weights)</span><br><span class="line">        outputs = <span class="variable language_">self</span>.dense(torch.cat(outputs, dim=<span class="number">0</span>))</span><br><span class="line">        <span class="keyword">return</span> outputs.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), [</span><br><span class="line">            enc_outputs, hidden_state, enc_valid_lens]</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._attention_weights</span><br></pre></td></tr></table></figure><p>测试Bahdanau 注意力解码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoder = d2l.Seq2SeqEncoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                             num_layers=<span class="number">2</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(vocab_size=<span class="number">10</span>, embed_size=<span class="number">8</span>, num_hiddens=<span class="number">16</span>,</span><br><span class="line">                                  num_layers=<span class="number">2</span>)</span><br><span class="line">decoder.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.zeros((<span class="number">4</span>, <span class="number">7</span>), dtype=torch.long)</span><br><span class="line">state = decoder.init_state(encoder(X), <span class="literal">None</span>)</span><br><span class="line">output, state = decoder(X, state)</span><br><span class="line">output.shape, <span class="built_in">len</span>(state), state[<span class="number">0</span>].shape, <span class="built_in">len</span>(state[<span class="number">1</span>]), state[<span class="number">1</span>][<span class="number">0</span>].shape</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">(torch.Size([<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>]), <span class="number">3</span>, torch.Size([<span class="number">4</span>, <span class="number">7</span>, <span class="number">16</span>]), <span class="number">2</span>, torch.Size([<span class="number">4</span>, <span class="number">16</span>]))</span><br></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">embed_size, num_hiddens, num_layers, dropout = <span class="number">32</span>, <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span></span><br><span class="line">batch_size, num_steps = <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">250</span>, d2l.try_gpu()</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line">encoder = d2l.Seq2SeqEncoder(<span class="built_in">len</span>(src_vocab), embed_size, num_hiddens,</span><br><span class="line">                             num_layers, dropout)</span><br><span class="line">decoder = Seq2SeqAttentionDecoder(<span class="built_in">len</span>(tgt_vocab), embed_size, num_hiddens,</span><br><span class="line">                                  num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">loss <span class="number">0.021</span>, <span class="number">4793.5</span> tokens/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>见图左</p></blockquote><p><img src="/.io//22-2.png"></p><p>将几个英语句子翻译成法语</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; j<span class="string">&#x27;ai perdu .,  bleu 1.000</span></span><br><span class="line"><span class="string">he&#x27;</span>s calm . =&gt; il est mouillé .,  bleu <span class="number">0.658</span></span><br><span class="line">i<span class="string">&#x27;m home . =&gt; je suis chez moi .,  bleu 1.000</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">attention_weights = torch.cat(</span><br><span class="line">    [step[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq], <span class="number">0</span>).reshape(</span><br><span class="line">        (<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>, num_steps))</span><br></pre></td></tr></table></figure><p>可视化注意力权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    attention_weights[:, :, :, :<span class="built_in">len</span>(engs[-<span class="number">1</span>].split()) + <span class="number">1</span>].cpu(),</span><br><span class="line">    xlabel=<span class="string">&#x27;Key posistions&#x27;</span>, ylabel=<span class="string">&#x27;Query posistions&#x27;</span>)</span><br></pre></td></tr></table></figure><blockquote><p>见图右</p></blockquote><br><h2 id="二十三-自注意力和位置编码"><a href="#二十三-自注意力和位置编码" class="headerlink" title="二十三 自注意力和位置编码"></a>二十三 自注意力和位置编码</h2><h3 id="1-SA"><a href="#1-SA" class="headerlink" title="1 SA"></a>1 SA</h3><p><img src="/.io//23-1.png"></p><p><strong>总结：</strong></p><ul><li>自注意力池化层将xi当做key、value、query来对序列抽取特征</li><li>完全并行、最长序列为1【能看到整个序列的信息】、但对长序列计算复杂度高</li><li>位置编码在输入中加入位置信息，使得自注意力能够记忆位置信息</li></ul><h3 id="2-代码实现-5"><a href="#2-代码实现-5" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>自注意力</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_heads = <span class="number">100</span>, <span class="number">5</span></span><br><span class="line">attention = d2l.MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,</span><br><span class="line">                                   num_hiddens, num_heads, <span class="number">0.5</span>)</span><br><span class="line">attention.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">MultiHeadAttention(</span><br><span class="line">  (attention): DotProductAttention(</span><br><span class="line">    (dropout): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (W_q): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_k): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_v): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (W_o): Linear(in_features=<span class="number">100</span>, out_features=<span class="number">100</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size, num_queries, valid_lens = <span class="number">2</span>, <span class="number">4</span>, torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">X = torch.ones((batch_size, num_queries, num_hiddens))</span><br><span class="line">attention(X, X, X, valid_lens).shape</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">100</span>])</span><br></pre></td></tr></table></figure><p>位置编码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_hiddens, dropout, max_len=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.P = torch.zeros((<span class="number">1</span>, max_len, num_hiddens))</span><br><span class="line">        X = torch.arange(max_len, dtype=torch.float32).reshape(</span><br><span class="line">            -<span class="number">1</span>, <span class="number">1</span>) / torch.<span class="built_in">pow</span>(</span><br><span class="line">                <span class="number">10000</span>,</span><br><span class="line">                torch.arange(<span class="number">0</span>, num_hiddens, <span class="number">2</span>, dtype=torch.float32) /</span><br><span class="line">                num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(X)</span><br><span class="line">        <span class="variable language_">self</span>.P[:, :, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        X = X + <span class="variable language_">self</span>.P[:, :X.shape[<span class="number">1</span>], :].to(X.device)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(X)</span><br></pre></td></tr></table></figure><p>行代表标记在序列中的位置，列代表位置编码的不同维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">encoding_dim, num_steps = <span class="number">32</span>, <span class="number">60</span></span><br><span class="line">pos_encoding = PositionalEncoding(encoding_dim, <span class="number">0</span>)</span><br><span class="line">pos_encoding.<span class="built_in">eval</span>()</span><br><span class="line">X = pos_encoding(torch.zeros((<span class="number">1</span>, num_steps, encoding_dim)))</span><br><span class="line">P = pos_encoding.P[:, :X.shape[<span class="number">1</span>], :]</span><br><span class="line">d2l.plot(torch.arange(num_steps), P[<span class="number">0</span>, :, <span class="number">6</span>:<span class="number">10</span>].T, xlabel=<span class="string">&#x27;Row (position)&#x27;</span>,</span><br><span class="line">         figsize=(<span class="number">6</span>, <span class="number">2.5</span>), legend=[<span class="string">&quot;Col %d&quot;</span> % d <span class="keyword">for</span> d <span class="keyword">in</span> torch.arange(<span class="number">6</span>, <span class="number">10</span>)])</span><br></pre></td></tr></table></figure><p><img src="/.io//23-2.png"></p><p>二进制表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;i&#125;</span> in binary is <span class="subst">&#123;i:&gt;03b&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="number">0</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">000</span></span><br><span class="line"><span class="number">1</span> <span class="keyword">in</span> binary <span class="keyword">is</span> 001</span><br><span class="line"><span class="number">2</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">0</span>10</span><br><span class="line"><span class="number">3</span> <span class="keyword">in</span> binary <span class="keyword">is</span> 011</span><br><span class="line"><span class="number">4</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">100</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">101</span></span><br><span class="line"><span class="number">6</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">110</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">in</span> binary <span class="keyword">is</span> <span class="number">111</span></span><br></pre></td></tr></table></figure><p>在编码维度上降低频率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">P = P[<span class="number">0</span>, :, :].unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">d2l.show_heatmaps(P, xlabel=<span class="string">&#x27;Column (encoding dimension)&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Row (position)&#x27;</span>, figsize=(<span class="number">3.5</span>, <span class="number">4</span>), cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br></pre></td></tr></table></figure><br><h2 id="二十四-Transformer"><a href="#二十四-Transformer" class="headerlink" title="二十四 Transformer"></a>二十四 Transformer</h2><h3 id="1-Transformer"><a href="#1-Transformer" class="headerlink" title="1 Transformer"></a>1 Transformer</h3><p><img src="/.io//24-1.png"></p><p><strong>总结：</strong></p><ul><li>Transformer是一个纯使用注意力的编码-解码器</li><li>编码器和解码器都有n个transformer块</li><li>每个块里使用多头（自）注意力，基于位置的前馈网络，和层归一化</li></ul><h3 id="2-代码实现-6"><a href="#2-代码实现-6" class="headerlink" title="2 代码实现"></a>2 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>基于位置的前馈网络</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionWiseFFN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,</span></span><br><span class="line"><span class="params">                 **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionWiseFFN, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()</span><br><span class="line">        <span class="variable language_">self</span>.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dense2(<span class="variable language_">self</span>.relu(<span class="variable language_">self</span>.dense1(X)))</span><br></pre></td></tr></table></figure><p>改变张量的最里层维度的尺寸</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ffn = PositionWiseFFN(<span class="number">4</span>, <span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">ffn.<span class="built_in">eval</span>()</span><br><span class="line">ffn(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)))[<span class="number">0</span>]</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">tensor([[-<span class="number">0.2075</span>, -<span class="number">0.5694</span>, -<span class="number">0.4765</span>,  <span class="number">0.6563</span>,  <span class="number">0.1155</span>, -<span class="number">0.1294</span>, -<span class="number">0.2060</span>, -<span class="number">0.3361</span>],</span><br><span class="line">        [-<span class="number">0.2075</span>, -<span class="number">0.5694</span>, -<span class="number">0.4765</span>,  <span class="number">0.6563</span>,  <span class="number">0.1155</span>, -<span class="number">0.1294</span>, -<span class="number">0.2060</span>, -<span class="number">0.3361</span>],</span><br><span class="line">        [-<span class="number">0.2075</span>, -<span class="number">0.5694</span>, -<span class="number">0.4765</span>,  <span class="number">0.6563</span>,  <span class="number">0.1155</span>, -<span class="number">0.1294</span>, -<span class="number">0.2060</span>, -<span class="number">0.3361</span>]],</span><br><span class="line">       grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure><p>对比不同维度的层归一化和批量归一化的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ln = nn.LayerNorm(<span class="number">2</span>)</span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;layer norm:&#x27;</span>, ln(X), <span class="string">&#x27;\nbatch norm:&#x27;</span>, bn(X))</span><br><span class="line"></span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">layer norm: tensor([[-<span class="number">1.0000</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [-<span class="number">1.0000</span>,  <span class="number">1.0000</span>]], grad_fn=&lt;NativeLayerNormBackward&gt;) </span><br><span class="line">batch norm: tensor([[-<span class="number">1.0000</span>, -<span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">1.0000</span>,  <span class="number">1.0000</span>]], grad_fn=&lt;NativeBatchNormBackward&gt;)</span><br></pre></td></tr></table></figure><p>使用残差连接和层归一化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AddNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, normalized_shape, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(AddNorm, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ln = nn.LayerNorm(normalized_shape)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, Y</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.ln(<span class="variable language_">self</span>.dropout(Y) + X)</span><br></pre></td></tr></table></figure><p>加法操作后输出张量的形状相同</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">add_norm = AddNorm([<span class="number">3</span>, <span class="number">4</span>], <span class="number">0.5</span>)</span><br><span class="line">add_norm.<span class="built_in">eval</span>()</span><br><span class="line">add_norm(torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)), torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))).shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><p>实现编码器中的一个层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderBlock, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.attention = d2l.MultiHeadAttention(key_size, query_size,</span><br><span class="line">                                                value_size, num_hiddens,</span><br><span class="line">                                                num_heads, dropout, use_bias)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens</span>):</span><br><span class="line">        Y = <span class="variable language_">self</span>.addnorm1(X, <span class="variable language_">self</span>.attention(X, X, X, valid_lens))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.addnorm2(Y, <span class="variable language_">self</span>.ffn(Y))</span><br></pre></td></tr></table></figure><p>Transformer编码器中的任何层都不会改变其输入的形状</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">valid_lens = torch.tensor([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">encoder_blk = EncoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>)</span><br><span class="line">encoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">encoder_blk(X, valid_lens).shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><p>Transformer编码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerEncoder</span>(d2l.Encoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, use_bias=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerEncoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(</span><br><span class="line">                <span class="string">&quot;block&quot;</span> + <span class="built_in">str</span>(i),</span><br><span class="line">                EncoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, use_bias))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, valid_lens, *args</span>):</span><br><span class="line">        X = <span class="variable language_">self</span>.pos_encoding(<span class="variable language_">self</span>.embedding(X) * math.sqrt(<span class="variable language_">self</span>.num_hiddens))</span><br><span class="line">        <span class="variable language_">self</span>.attention_weights = [<span class="literal">None</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.blks)</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.blks):</span><br><span class="line">            X = blk(X, valid_lens)</span><br><span class="line">            <span class="variable language_">self</span>.attention_weights[</span><br><span class="line">                i] = blk.attention.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><p>创建一个两层的Transformer编码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">encoder = TransformerEncoder(<span class="number">200</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">2</span>,</span><br><span class="line">                             <span class="number">0.5</span>)</span><br><span class="line">encoder.<span class="built_in">eval</span>()</span><br><span class="line">encoder(torch.ones((<span class="number">2</span>, <span class="number">100</span>), dtype=torch.long), valid_lens).shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><p>Transformer解码器也是由多个相同的层组成</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;解码器中第 i 个块&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key_size, query_size, value_size, num_hiddens,</span></span><br><span class="line"><span class="params">                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,</span></span><br><span class="line"><span class="params">                 dropout, i, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderBlock, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.i = i</span><br><span class="line">        <span class="variable language_">self</span>.attention1 = d2l.MultiHeadAttention(key_size, query_size,</span><br><span class="line">                                                 value_size, num_hiddens,</span><br><span class="line">                                                 num_heads, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm1 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.attention2 = d2l.MultiHeadAttention(key_size, query_size,</span><br><span class="line">                                                 value_size, num_hiddens,</span><br><span class="line">                                                 num_heads, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm2 = AddNorm(norm_shape, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                                   num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.addnorm3 = AddNorm(norm_shape, dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        enc_outputs, enc_valid_lens = state[<span class="number">0</span>], state[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> state[<span class="number">2</span>][<span class="variable language_">self</span>.i] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            key_values = X</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            key_values = torch.cat((state[<span class="number">2</span>][<span class="variable language_">self</span>.i], X), axis=<span class="number">1</span>)</span><br><span class="line">        state[<span class="number">2</span>][<span class="variable language_">self</span>.i] = key_values</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.training:</span><br><span class="line">            batch_size, num_steps, _ = X.shape</span><br><span class="line">            dec_valid_lens = torch.arange(<span class="number">1</span>, num_steps + <span class="number">1</span>,</span><br><span class="line">                                          device=X.device).repeat(</span><br><span class="line">                                              batch_size, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dec_valid_lens = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        X2 = <span class="variable language_">self</span>.attention1(X, key_values, key_values, dec_valid_lens)</span><br><span class="line">        Y = <span class="variable language_">self</span>.addnorm1(X, X2)</span><br><span class="line">        Y2 = <span class="variable language_">self</span>.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)</span><br><span class="line">        Z = <span class="variable language_">self</span>.addnorm2(Y, Y2)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.addnorm3(Z, <span class="variable language_">self</span>.ffn(Z)), state</span><br></pre></td></tr></table></figure><p>编码器和解码器的特征维度都是<code>num_hiddens</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">decoder_blk = DecoderBlock(<span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, <span class="number">24</span>, [<span class="number">100</span>, <span class="number">24</span>], <span class="number">24</span>, <span class="number">48</span>, <span class="number">8</span>, <span class="number">0.5</span>, <span class="number">0</span>)</span><br><span class="line">decoder_blk.<span class="built_in">eval</span>()</span><br><span class="line">X = torch.ones((<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>))</span><br><span class="line">state = [encoder_blk(X, valid_lens), valid_lens, [<span class="literal">None</span>]]</span><br><span class="line">decoder_blk(X, state)[<span class="number">0</span>].shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">100</span>, <span class="number">24</span>])</span><br></pre></td></tr></table></figure><p>Transformer解码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerDecoder</span>(d2l.AttentionDecoder):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, key_size, query_size, value_size,</span></span><br><span class="line"><span class="params">                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,</span></span><br><span class="line"><span class="params">                 num_heads, num_layers, dropout, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(TransformerDecoder, <span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.num_hiddens = num_hiddens</span><br><span class="line">        <span class="variable language_">self</span>.num_layers = num_layers</span><br><span class="line">        <span class="variable language_">self</span>.embedding = nn.Embedding(vocab_size, num_hiddens)</span><br><span class="line">        <span class="variable language_">self</span>.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)</span><br><span class="line">        <span class="variable language_">self</span>.blks = nn.Sequential()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            <span class="variable language_">self</span>.blks.add_module(</span><br><span class="line">                <span class="string">&quot;block&quot;</span> + <span class="built_in">str</span>(i),</span><br><span class="line">                DecoderBlock(key_size, query_size, value_size, num_hiddens,</span><br><span class="line">                             norm_shape, ffn_num_input, ffn_num_hiddens,</span><br><span class="line">                             num_heads, dropout, i))</span><br><span class="line">        <span class="variable language_">self</span>.dense = nn.Linear(num_hiddens, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_state</span>(<span class="params">self, enc_outputs, enc_valid_lens, *args</span>):</span><br><span class="line">        <span class="keyword">return</span> [enc_outputs, enc_valid_lens, [<span class="literal">None</span>] * <span class="variable language_">self</span>.num_layers]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X, state</span>):</span><br><span class="line">        X = <span class="variable language_">self</span>.pos_encoding(<span class="variable language_">self</span>.embedding(X) * math.sqrt(<span class="variable language_">self</span>.num_hiddens))</span><br><span class="line">        <span class="variable language_">self</span>._attention_weights = [[<span class="literal">None</span>] * <span class="built_in">len</span>(<span class="variable language_">self</span>.blks) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">for</span> i, blk <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.blks):</span><br><span class="line">            X, state = blk(X, state)</span><br><span class="line">            <span class="variable language_">self</span>._attention_weights[<span class="number">0</span>][</span><br><span class="line">                i] = blk.attention1.attention.attention_weights</span><br><span class="line">            <span class="variable language_">self</span>._attention_weights[<span class="number">1</span>][</span><br><span class="line">                i] = blk.attention2.attention.attention_weights</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dense(X), state</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention_weights</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>._attention_weights</span><br></pre></td></tr></table></figure><p>训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens, num_layers, dropout, batch_size, num_steps = <span class="number">32</span>, <span class="number">2</span>, <span class="number">0.1</span>, <span class="number">64</span>, <span class="number">10</span></span><br><span class="line">lr, num_epochs, device = <span class="number">0.005</span>, <span class="number">200</span>, d2l.try_gpu()</span><br><span class="line">ffn_num_input, ffn_num_hiddens, num_heads = <span class="number">32</span>, <span class="number">64</span>, <span class="number">4</span></span><br><span class="line">key_size, query_size, value_size = <span class="number">32</span>, <span class="number">32</span>, <span class="number">32</span></span><br><span class="line">norm_shape = [<span class="number">32</span>]</span><br><span class="line"></span><br><span class="line">train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)</span><br><span class="line"></span><br><span class="line">encoder = TransformerEncoder(<span class="built_in">len</span>(src_vocab), key_size, query_size, value_size,</span><br><span class="line">                             num_hiddens, norm_shape, ffn_num_input,</span><br><span class="line">                             ffn_num_hiddens, num_heads, num_layers, dropout)</span><br><span class="line">decoder = TransformerDecoder(<span class="built_in">len</span>(tgt_vocab), key_size, query_size, value_size,</span><br><span class="line">                             num_hiddens, norm_shape, ffn_num_input,</span><br><span class="line">                             ffn_num_hiddens, num_heads, num_layers, dropout)</span><br><span class="line">net = d2l.EncoderDecoder(encoder, decoder)</span><br><span class="line">d2l.train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">loss <span class="number">0.030</span>, <span class="number">4917.3</span> tokens/sec on cuda:<span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>如下如左</p></blockquote><p><img src="/.io//24-2.png"></p><p>将一些英语句子翻译成法语</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">engs = [<span class="string">&#x27;go .&#x27;</span>, <span class="string">&quot;i lost .&quot;</span>, <span class="string">&#x27;he\&#x27;s calm .&#x27;</span>, <span class="string">&#x27;i\&#x27;m home .&#x27;</span>]</span><br><span class="line">fras = [<span class="string">&#x27;va !&#x27;</span>, <span class="string">&#x27;j\&#x27;ai perdu .&#x27;</span>, <span class="string">&#x27;il est calme .&#x27;</span>, <span class="string">&#x27;je suis chez moi .&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> eng, fra <span class="keyword">in</span> <span class="built_in">zip</span>(engs, fras):</span><br><span class="line">    translation, dec_attention_weight_seq = d2l.predict_seq2seq(</span><br><span class="line">        net, eng, src_vocab, tgt_vocab, num_steps, device, <span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;eng&#125;</span> =&gt; <span class="subst">&#123;translation&#125;</span>, &#x27;</span>,</span><br><span class="line">          <span class="string">f&#x27;bleu <span class="subst">&#123;d2l.bleu(translation, fra, k=<span class="number">2</span>):<span class="number">.3</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">go . =&gt; va !,  bleu <span class="number">1.000</span></span><br><span class="line">i lost . =&gt; je suis &lt;unk&gt; .,  bleu <span class="number">0.000</span></span><br><span class="line">he<span class="string">&#x27;s calm . =&gt; il est calme .,  bleu 1.000</span></span><br><span class="line"><span class="string">i&#x27;</span>m home . =&gt; je suis chez moi .,  bleu <span class="number">1.000</span></span><br></pre></td></tr></table></figure><p>可视化Transformer 的注意力权重</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">enc_attention_weights = torch.cat(net.encoder.attention_weights, <span class="number">0</span>).reshape(</span><br><span class="line">    (num_layers, num_heads, -<span class="number">1</span>, num_steps))</span><br><span class="line">enc_attention_weights.shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">10</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(enc_attention_weights.cpu(), xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">                  titles=[<span class="string">&#x27;Head %d&#x27;</span> % i</span><br><span class="line">                          <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><blockquote><p>如上图 右</p></blockquote><p>为了可视化解码器的自注意力权重和“编码器－解码器”的注意力权重，我们需要完成更多的数据操作工作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dec_attention_weights_2d = [</span><br><span class="line">    head[<span class="number">0</span>].tolist() <span class="keyword">for</span> step <span class="keyword">in</span> dec_attention_weight_seq <span class="keyword">for</span> attn <span class="keyword">in</span> step</span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> attn <span class="keyword">for</span> head <span class="keyword">in</span> blk]</span><br><span class="line">dec_attention_weights_filled = torch.tensor(</span><br><span class="line">    pd.DataFrame(dec_attention_weights_2d).fillna(<span class="number">0.0</span>).values)</span><br><span class="line">dec_attention_weights = dec_attention_weights_filled.reshape(</span><br><span class="line">    (-<span class="number">1</span>, <span class="number">2</span>, num_layers, num_heads, num_steps))</span><br><span class="line">dec_self_attention_weights, dec_inter_attention_weights = \</span><br><span class="line">    dec_attention_weights.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">4</span>)</span><br><span class="line">dec_self_attention_weights.shape, dec_inter_attention_weights.shape</span><br><span class="line"><span class="comment">#output:</span></span><br><span class="line">(torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">10</span>]), torch.Size([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">10</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(</span><br><span class="line">    dec_self_attention_weights[:, :, :, :<span class="built_in">len</span>(translation.split()) + <span class="number">1</span>],</span><br><span class="line">    xlabel=<span class="string">&#x27;Key positions&#x27;</span>, ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">    titles=[<span class="string">&#x27;Head %d&#x27;</span> % i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><blockquote><p>如下图左</p></blockquote><p><img src="/.io//24-3.png"></p><p>输出序列的查询不会与输入序列中填充位置的标记进行注意力计算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d2l.show_heatmaps(dec_inter_attention_weights, xlabel=<span class="string">&#x27;Key positions&#x27;</span>,</span><br><span class="line">                  ylabel=<span class="string">&#x27;Query positions&#x27;</span>,</span><br><span class="line">                  titles=[<span class="string">&#x27;Head %d&#x27;</span> % i</span><br><span class="line">                          <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>)], figsize=(<span class="number">7</span>, <span class="number">3.5</span>))</span><br></pre></td></tr></table></figure><blockquote><p>如上图右</p></blockquote><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Moustache 微信支付"><p>微信支付</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Moustache</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hammerzer.github.io/2024/03/22/Overview-of-modern-foundation-models-1/" title="动手学深度学习-现代基础模型概览一">https://hammerzer.github.io/2024/03/22/Overview-of-modern-foundation-models-1/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Deep-Learning/" rel="tag"><i class="fa fa-tag"></i> Deep Learning</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2024/03/22/Deep-learning-model-training/" rel="prev" title="动手学深度学习-深度学习模型训练"><i class="fa fa-chevron-left"></i> 动手学深度学习-深度学习模型训练</a></div><div class="post-nav-item"><a href="/2024/04/13/Linux-Quick-Start/" rel="next" title="Linux快速入门">Linux快速入门 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CONTENT-OUTLINE"><span class="nav-number">1.</span> <span class="nav-text">CONTENT OUTLINE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%87%E3%80%81%E7%9B%AE%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">〇、目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">一 物体检测算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B9%9D-%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">4.</span> <span class="nav-text">九 文本预处理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.1.</span> <span class="nav-text">1 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">十 语言模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.1.</span> <span class="nav-text">1 语言模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A"><span class="nav-number">5.3.</span> <span class="nav-text">3 Q&amp;A</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B8%83-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84"><span class="nav-number">6.</span> <span class="nav-text">十七 编码器-解码器结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">6.1.</span> <span class="nav-text">1 编码器-解码器</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">6.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E5%85%AB-%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0seq2seq"><span class="nav-number">7.</span> <span class="nav-text">十八 序列到序列学习seq2seq</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%81%E4%B9%9D-%E6%9D%9F%E6%90%9C%E7%B4%A2"><span class="nav-number">8.</span> <span class="nav-text">十九 束搜索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%8D%81-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="nav-number">9.</span> <span class="nav-text">二十 注意力机制</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Attention-Mechanism"><span class="nav-number">9.1.</span> <span class="nav-text">1 Attention Mechanism</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="nav-number">9.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%8D%81%E4%B8%80-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="nav-number">10.</span> <span class="nav-text">二十一 注意力分数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="nav-number">10.1.</span> <span class="nav-text">1 注意力分数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-3"><span class="nav-number">10.2.</span> <span class="nav-text">2 代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Q-amp-A-1"><span class="nav-number">10.3.</span> <span class="nav-text">3 Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-Little-Questions"><span class="nav-number">10.3.1.</span> <span class="nav-text">3.1 Little Questions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%8D%81%E4%BA%8C-%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84seq2seq"><span class="nav-number">11.</span> <span class="nav-text">二十二 使用注意力机制的seq2seq</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-seq2seq"><span class="nav-number">11.1.</span> <span class="nav-text">1 seq2seq</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-4"><span class="nav-number">11.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%8D%81%E4%B8%89-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">12.</span> <span class="nav-text">二十三 自注意力和位置编码</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-SA"><span class="nav-number">12.1.</span> <span class="nav-text">1 SA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-5"><span class="nav-number">12.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E5%8D%81%E5%9B%9B-Transformer"><span class="nav-number">13.</span> <span class="nav-text">二十四 Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Transformer"><span class="nav-number">13.1.</span> <span class="nav-text">1 Transformer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-6"><span class="nav-number">13.2.</span> <span class="nav-text">2 代码实现</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Moustache" src="/images/180-180.png"><p class="site-author-name" itemprop="name">Moustache</p><div class="site-description" itemprop="description">我是小胡子</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">78</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/hammerzer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hammerzer" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:stellar_lzu@163.com" title="E-Mail → mailto:stellar_lzu@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Chase</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">29:01</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.4" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JRehDoQ6pHXV1zKg09AMNLFt-gzGzoHsz',
      appKey     : 'cRAt4W15KiQdrIuHlQrRrtIl',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>