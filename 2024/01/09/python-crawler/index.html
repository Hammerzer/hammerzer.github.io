<!DOCTYPE html><html lang="zh-CN"><head><script src="https://lib.sinaapp.com/js/jquery/1.7.2/jquery.min.js"></script><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/180-180.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/32-32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/16-16.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css"><script src="//cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hammerzer.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:"flat"},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:"valine",storage:!0,lazyload:!1,nav:null,activeClass:"valine"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="CONTENT OUTLINE浅学一下爬虫  发现一个将内容文本生成电子书的项目，在npm中名为 GitBook"><meta property="og:type" content="article"><meta property="og:title" content="Python爬虫"><meta property="og:url" content="https://hammerzer.github.io/2024/01/09/python-crawler/index.html"><meta property="og:site_name" content="Moustache&#39;s Blog"><meta property="og:description" content="CONTENT OUTLINE浅学一下爬虫  发现一个将内容文本生成电子书的项目，在npm中名为 GitBook"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://hammerzer.github.io/.io//p1.png"><meta property="og:image" content="https://hammerzer.github.io/.io//p2.png"><meta property="og:image" content="https://hammerzer.github.io/.io//p3.png"><meta property="og:image" content="https://hammerzer.github.io/.io//p4.png"><meta property="article:published_time" content="2024-01-09T07:25:11.000Z"><meta property="article:modified_time" content="2025-01-19T03:19:15.841Z"><meta property="article:author" content="Moustache"><meta property="article:tag" content="Python"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hammerzer.github.io/.io//p1.png"><link rel="canonical" href="https://hammerzer.github.io/2024/01/09/python-crawler/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>Python爬虫 | Moustache's Blog</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript><link rel="alternate" href="/atom.xml" title="Moustache's Blog" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">Moustache's Blog</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">小胡子的私人空间</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">34</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">9</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">78</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://hammerzer.github.io/2024/01/09/python-crawler/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/180-180.png"><meta itemprop="name" content="Moustache"><meta itemprop="description" content="我是小胡子"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Moustache's Blog"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Python爬虫</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-01-09 15:25:11" itemprop="dateCreated datePublished" datetime="2024-01-09T15:25:11+08:00">2024-01-09</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2025-01-19 11:19:15" itemprop="dateModified" datetime="2025-01-19T11:19:15+08:00">2025-01-19</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Artificial-Intelligence/" itemprop="url" rel="index"><span itemprop="name">Artificial Intelligence</span></a> </span></span><span id="/2024/01/09/python-crawler/" class="post-meta-item leancloud_visitors" data-flag-title="Python爬虫" title="阅读次数"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span class="leancloud-visitors-count"></span> <span>℃</span> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Valine：</span> <a title="valine" href="/2024/01/09/python-crawler/#valine-comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2024/01/09/python-crawler/" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>36k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>33 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="CONTENT-OUTLINE"><a href="#CONTENT-OUTLINE" class="headerlink" title="CONTENT OUTLINE"></a>CONTENT OUTLINE</h2><p>浅学一下爬虫</p><blockquote><p>发现一个将内容文本生成电子书的项目，在npm中名为 <code>GitBook</code></p></blockquote><span id="more"></span><h2 id="〇-目录"><a href="#〇-目录" class="headerlink" title="〇 目录"></a>〇 目录</h2><p>略</p><br><h2 id="一-爬虫前奏"><a href="#一-爬虫前奏" class="headerlink" title="一 爬虫前奏"></a>一 爬虫前奏</h2><p>爬虫的实际例子：</p><ol><li>搜索引擎（百度、谷歌、360搜索等）。</li><li>伯乐在线。</li><li>惠惠购物助手。</li><li>数据分析与研究（数据冰山知乎专栏）。</li><li>抢票软件等。</li></ol><h3 id="什么是网络爬虫："><a href="#什么是网络爬虫：" class="headerlink" title="什么是网络爬虫："></a>什么是网络爬虫：</h3><ol><li>通俗理解：爬虫是一个模拟人类请求网站行为的程序。可以自动请求网页、并数据抓取下来，然后使用一定的规则提取有价值的数据。</li><li>专业介绍：<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711?fr=aladdin">百度百科</a>。</li></ol><h3 id="通用爬虫和聚焦爬虫"><a href="#通用爬虫和聚焦爬虫" class="headerlink" title="通用爬虫和聚焦爬虫"></a>通用爬虫和聚焦爬虫</h3><ol><li>通用爬虫：通用爬虫是搜索引擎抓取系统（百度、谷歌、搜狗等）的重要组成部分。主要是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份。</li><li>聚焦爬虫：是面向特定需求的一种网络爬虫程序，他与通用爬虫的区别在于：聚焦爬虫在实施网页抓取的时候会对内容进行筛选和处理，尽量保证只抓取与需求相关的网页信息。</li></ol><h3 id="为什么用Python写爬虫程序"><a href="#为什么用Python写爬虫程序" class="headerlink" title="为什么用Python写爬虫程序"></a>为什么用Python写爬虫程序</h3><ol><li>PHP：PHP是世界是最好的语言，但他天生不是做这个的，而且对多线程、异步支持不是很好，并发处理能力弱。爬虫是工具性程序，对速度和效率要求比较高。</li><li>Java：生态圈很完善，是Python爬虫最大的竞争对手。但是Java语言本身很笨重，代码量很大。重构成本比较高，任何修改会导致代码大量改动。爬虫经常要修改采集代码。</li><li>C/C++：运行效率是无敌的。但是学习和开发成本高。写个小爬虫程序可能要大半天时间。</li><li>Python：语法优美、代码简洁、开发效率高、支持的模块多。相关的HTTP请求模块和HTML解析模块非常丰富。还有Scrapy和Scrapy-redis框架让我们开发爬虫变得异常简单。</li></ol><h3 id="准备工具"><a href="#准备工具" class="headerlink" title="准备工具"></a>准备工具</h3><ol><li>Python3.6开发环境。</li><li>Pycharm 2017 professional版。</li><li>虚拟环境。<code>virtualenv/virtualenvwrapper</code>。</li></ol><br><h2 id="二-http协议和Chrome抓包工具"><a href="#二-http协议和Chrome抓包工具" class="headerlink" title="二 http协议和Chrome抓包工具"></a>二 http协议和Chrome抓包工具</h2><h3 id="什么是http和https协议："><a href="#什么是http和https协议：" class="headerlink" title="什么是http和https协议："></a>什么是http和https协议：</h3><p>HTTP协议：全称是<code>HyperText Transfer Protocol</code>，中文意思是超文本传输协议，是一种发布和接收HTML页面的方法。服务器端口号是<code>80</code>端口。<br>HTTPS协议：是HTTP协议的加密版本，在HTTP下加入了SSL层。服务器端口号是<code>443</code>端口。</p><h3 id="在浏览器中发送一个http请求的过程："><a href="#在浏览器中发送一个http请求的过程：" class="headerlink" title="在浏览器中发送一个http请求的过程："></a>在浏览器中发送一个http请求的过程：</h3><ol><li>当用户在浏览器的地址栏中输入一个URL并按回车键之后，浏览器会向HTTP服务器发送HTTP请求。HTTP请求主要分为“Get”和“Post”两种方法。</li><li>当我们在浏览器输入URL <a target="_blank" rel="noopener" href="http://www.baidu.com/">http://www.baidu.com</a> 的时候，浏览器发送一个Request请求去获取 <a target="_blank" rel="noopener" href="http://www.baidu.com/">http://www.baidu.com</a> 的html文件，服务器把Response文件对象发送回给浏览器。</li><li>浏览器分析Response中的 HTML，发现其中引用了很多其他文件，比如Images文件，CSS文件，JS文件。 浏览器会自动再次发送Request去获取图片，CSS文件，或者JS文件。</li><li>当所有的文件都下载成功后，网页会根据HTML语法结构，完整的显示出来了。</li></ol><h3 id="url详解"><a href="#url详解" class="headerlink" title="url详解"></a>url详解</h3><p><code>URL</code>是<code>Uniform Resource Locator</code>的简写，统一资源定位符。一个<code>URL</code>由以下几部分组成：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheme:<span class="regexp">//</span>host:port<span class="regexp">/path/</span>?query-string=xxx<span class="comment">#anchor</span></span><br></pre></td></tr></table></figure><ul><li><strong>scheme</strong>：代表的是访问的协议，一般为<code>http</code>或者<code>https</code>以及<code>ftp</code>等。</li><li><strong>host</strong>：主机名，域名，比如<code>www.baidu.com</code>。</li><li><strong>port</strong>：端口号。当你访问一个网站的时候，浏览器默认使用80端口。</li><li><strong>path</strong>：查找路径。比如：<code>www.jianshu.com/trending/now</code>，后面的<code>trending/now</code>就是<code>path</code>。</li><li><strong>query-string</strong>：查询字符串，比如：<code>www.baidu.com/s?wd=python</code>，后面的<code>wd=python</code>就是查询字符串。</li><li><strong>anchor</strong>：锚点，后台一般不用管，前端用来做页面定位的。</li></ul><p>在浏览器中请求一个<code>url</code>，浏览器会对这个url进行一个编码。除英文字母，数字和部分符号外，其他的全部使用百分号+十六进制码值进行编码。</p><h4 id="常用的请求方法："><a href="#常用的请求方法：" class="headerlink" title="常用的请求方法："></a>常用的请求方法：</h4><p>在<code>Http</code>协议中，定义了八种请求方法。这里介绍两种常用的请求方法，分别是<code>get</code>请求和<code>post</code>请求。</p><ol><li><code>get</code>请求：一般情况下，只从服务器获取数据下来，并不会对服务器资源产生任何影响的时候会使用<code>get</code>请求。</li><li><code>post</code>请求：向服务器发送数据（登录）、上传文件等，会对服务器资源产生影响的时候会使用<code>post</code>请求。<br>以上是在网站开发中常用的两种方法。并且一般情况下都会遵循使用的原则。但是有的网站和服务器为了做反爬虫机制，也经常会不按常理出牌，有可能一个应该使用<code>get</code>方法的请求就一定要改成<code>post</code>请求，这个要视情况而定。</li></ol><h4 id="请求头常见参数："><a href="#请求头常见参数：" class="headerlink" title="请求头常见参数："></a>请求头常见参数：</h4><p>在<code>http</code>协议中，向服务器发送一个请求，数据分为三部分，第一个是把数据放在url中，第二个是把数据放在<code>body</code>中（在<code>post</code>请求中），第三个就是把数据放在<code>head</code>中。这里介绍在网络爬虫中经常会用到的一些请求头参数：</p><ol><li><code>User-Agent</code>：浏览器名称。这个在网络爬虫中经常会被使用到。请求一个网页的时候，服务器通过这个参数就可以知道这个请求是由哪种浏览器发送的。如果我们是通过爬虫发送请求，那么我们的<code>User-Agent</code>就是<code>Python</code>，这对于那些有反爬虫机制的网站来说，可以轻易的判断你这个请求是爬虫。因此我们要经常设置这个值为一些浏览器的值，来伪装我们的爬虫。</li><li><code>Referer</code>：表明当前这个请求是从哪个<code>url</code>过来的。这个一般也可以用来做反爬虫技术。如果不是从指定页面过来的，那么就不做相关的响应。</li><li><code>Cookie</code>：<code>http</code>协议是无状态的。也就是同一个人发送了两次请求，服务器没有能力知道这两个请求是否来自同一个人。因此这时候就用<code>cookie</code>来做标识。一般如果想要做登录后才能访问的网站，那么就需要发送<code>cookie</code>信息了。</li></ol><h4 id="常见响应状态码："><a href="#常见响应状态码：" class="headerlink" title="常见响应状态码："></a>常见响应状态码：</h4><ol><li><code>200</code>：请求正常，服务器正常的返回数据。</li><li><code>301</code>：永久重定向。比如在访问<code>www.jingdong.com</code>的时候会重定向到<code>www.jd.com</code>。</li><li><code>302</code>：临时重定向。比如在访问一个需要登录的页面的时候，而此时没有登录，那么就会重定向到登录页面。</li><li><code>400</code>：请求的<code>url</code>在服务器上找不到。换句话说就是请求<code>url</code>错误。</li><li><code>403</code>：服务器拒绝访问，权限不够。</li><li><code>500</code>：服务器内部错误。可能是服务器出现<code>bug</code>了。</li></ol><p><img src="/.io//p1.png"></p><h4 id="Chrome抓包工具："><a href="#Chrome抓包工具：" class="headerlink" title="Chrome抓包工具："></a>Chrome抓包工具：</h4><p><code>Chrome</code>浏览器是一个非常亲近开发者的浏览器。可以方便的查看网络请求以及发送的参数。对着网页<code>右键-&gt;检查</code>。</p><br><h2 id="三-urllib库"><a href="#三-urllib库" class="headerlink" title="三 urllib库"></a>三 urllib库</h2><p><code>urllib</code>库是<code>Python</code>中一个最基本的网络请求库。可以模拟浏览器的行为，向指定的服务器发送一个请求，并可以保存服务器返回的数据。</p><h3 id="urlopen函数："><a href="#urlopen函数：" class="headerlink" title="urlopen函数："></a>urlopen函数：</h3><p>在<code>Python3</code>的<code>urllib</code>库中，所有和网络请求相关的方法，都被集到<code>urllib.request</code>模块下面了，以先来看下<code>urlopen</code>函数基本的使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">resp = request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br></pre></td></tr></table></figure><p>实际上，使用浏览器访问百度，右键查看源代码。你会发现，跟我们刚才打印出来的数据是一模一样的。也就是说，上面的三行代码就已经帮我们把百度的首页的全部代码爬下来了。一个基本的url请求对应的python代码真的非常简单。</p><p>以下对<code>urlopen</code>函数的进行详细讲解：</p><ol><li><code>url</code>：请求的url。</li><li><code>data</code>：请求的<code>data</code>，如果设置了这个值，那么将变成<code>post</code>请求。</li><li>返回值：返回值是一个<code>http.client.HTTPResponse</code>对象，这个对象是一个类文件句柄对象。有<code>read(size)</code>、<code>readline</code>、<code>readlines</code>以及<code>getcode</code>等方法。</li></ol><h3 id="urlretrieve函数："><a href="#urlretrieve函数：" class="headerlink" title="urlretrieve函数："></a>urlretrieve函数：</h3><p>这个函数可以方便的将网页上的一个文件保存到本地。以下代码可以非常方便的将百度的首页<strong>下载到本地</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line">request.urlretrieve(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>,<span class="string">&#x27;baidu.html&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="urlencode函数："><a href="#urlencode函数：" class="headerlink" title="urlencode函数："></a>urlencode函数：</h3><p>用浏览器发送请求的时候，如果url中包含了中文或者其他特殊字符，那么浏览器会自动的给我们进行编码。而如果使用代码发送请求，那么就<strong>必须手动的进行编码</strong>，这时候就应该使用<code>urlencode</code>函数来实现。<code>urlencode</code>可以把字典数据转换为<code>URL</code>编码的数据。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">data = &#123;<span class="string">&#x27;name&#x27;</span>:<span class="string">&#x27;爬虫基础&#x27;</span>,<span class="string">&#x27;greet&#x27;</span>:<span class="string">&#x27;hello world&#x27;</span>,<span class="string">&#x27;age&#x27;</span>:<span class="number">100</span>&#125;</span><br><span class="line">qs = parse.urlencode(data)</span><br><span class="line"><span class="built_in">print</span>(qs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Out: name=%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80&amp;greet=hello+world&amp;age=100</span></span><br></pre></td></tr></table></figure><h3 id="parse-qs函数："><a href="#parse-qs函数：" class="headerlink" title="parse_qs函数："></a>parse_qs函数：</h3><p>可以将经过编码后的<strong>url参数进行解码</strong>。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line">qs = <span class="string">&quot;name=%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80&amp;greet=hello+world&amp;age=100&quot;</span></span><br><span class="line"><span class="built_in">print</span>(parse.parse_qs(qs))</span><br></pre></td></tr></table></figure><h3 id="urlparse和urlsplit："><a href="#urlparse和urlsplit：" class="headerlink" title="urlparse和urlsplit："></a>urlparse和urlsplit：</h3><p>有时候拿到一个url，想要对这个url中的各个组成部分进行分割，那么这时候就可以使用<code>urlparse</code>或者是<code>urlsplit</code>来进行分割。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?username=zhiliao&#x27;</span></span><br><span class="line"></span><br><span class="line">result = parse.urlsplit(url)</span><br><span class="line"><span class="comment"># result = parse.urlparse(url)</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;scheme:&#x27;</span>,result.scheme)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;netloc:&#x27;</span>,result.netloc)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;path:&#x27;</span>,result.path)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;query:&#x27;</span>,result.query)</span><br></pre></td></tr></table></figure><p><code>urlparse</code>和<code>urlsplit</code>基本上是一模一样的。唯一不一样的地方是，<code>urlparse</code>里面多了一个<code>params</code>属性，而<code>urlsplit</code>没有这个<code>params</code>属性。比如有一个<code>url</code>为：<code>url = &#39;http://www.baidu.com/s/hello?wd=python&amp;username=abc#1&#39;</code>，<br>那么<code>urlparse</code>可以获取到<code>hello</code>，而<code>urlsplit</code>不可以获取到。<code>url</code>中的<code>params</code>也用得比较少。</p><h3 id="request-Request类："><a href="#request-Request类：" class="headerlink" title="request.Request类："></a>request.Request类：</h3><p>如果想要在请求的时候增加一些请求头，那么就必须使用<code>request.Request</code>类来实现。比如要增加一个<code>User-Agent</code>，示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line"><span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">req = request.Request(<span class="string">&quot;http://www.baidu.com/&quot;</span>,headers=headers)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br></pre></td></tr></table></figure><h3 id="ProxyHandler处理器（代理设置）"><a href="#ProxyHandler处理器（代理设置）" class="headerlink" title="ProxyHandler处理器（代理设置）"></a>ProxyHandler处理器（代理设置）</h3><p>很多网站会检测某一段时间某个IP的访问次数(通过流量统计，系统日志等)，如果访问次数多的不像正常人，它会禁止这个IP的访问。 所以我们可以设置一些代理服务器，每隔一段时间换一个代理，就算IP被禁止，依然可以换个IP继续爬取。</p><p>urllib中通过ProxyHandler来<strong>设置使用代理服务器</strong>，下面代码说明如何使用自定义opener来使用代理：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个是没有使用代理的</span></span><br><span class="line"><span class="comment"># resp = request.urlopen(&#x27;http://httpbin.org/get&#x27;)</span></span><br><span class="line"><span class="comment"># print(resp.read().decode(&quot;utf-8&quot;))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个是使用了代理的</span></span><br><span class="line">handler = request.ProxyHandler(&#123;<span class="string">&quot;http&quot;</span>:<span class="string">&quot;218.66.161.88:31769&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line">req = request.Request(<span class="string">&quot;http://httpbin.org/ip&quot;</span>)</span><br><span class="line">resp = opener.<span class="built_in">open</span>(req)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br></pre></td></tr></table></figure><p><strong>常用的代理有：</strong></p><ul><li>西刺免费代理IP：<a target="_blank" rel="noopener" href="http://www.xicidaili.com/">http://www.xicidaili.com/</a></li><li>快代理：<a target="_blank" rel="noopener" href="http://www.kuaidaili.com/">http://www.kuaidaili.com/</a></li><li>代理云：<a target="_blank" rel="noopener" href="http://www.dailiyun.com/">http://www.dailiyun.com/</a></li></ul><h3 id="什么是cookie："><a href="#什么是cookie：" class="headerlink" title="什么是cookie："></a>什么是cookie：</h3><p>在网站中，http请求是无状态的。也就是说即使第一次和服务器连接后并且登录成功后，第二次请求服务器依然不能知道当前请求是哪个用户。<code>cookie</code>的出现就是为了解决这个问题，第一次登录后服务器返回一些数据（cookie）给浏览器，然后浏览器保存在本地，当该用户发送第二次请求的时候，就会自动的把上次请求存储的<code>cookie</code>数据自动的携带给服务器，服务器通过浏览器携带的数据就能判断当前用户是哪个了。<code>cookie</code>存储的数据量有限，不同的浏览器有不同的存储大小，但一般不超过4KB。因此使用<code>cookie</code>只能存储一些小量的数据。</p><h4 id="cookie的格式："><a href="#cookie的格式：" class="headerlink" title="cookie的格式："></a>cookie的格式：</h4><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Set</span>-Cookie: <span class="type">NAME</span>=<span class="keyword">VALUE</span>；Expires/Max-age=<span class="type">DATE</span>；<span class="type">Path</span>=<span class="type">PATH</span>；<span class="keyword">Domain</span>=DOMAIN_NAME；SECURE</span><br></pre></td></tr></table></figure><p>参数意义：</p><ul><li>NAME：cookie的名字。</li><li>VALUE：cookie的值。</li><li>Expires：cookie的过期时间。</li><li>Path：cookie作用的路径。</li><li>Domain：cookie作用的域名。</li><li>SECURE：是否只在https协议下起作用。</li></ul><h3 id="使用cookielib库和HTTPCookieProcessor模拟登录："><a href="#使用cookielib库和HTTPCookieProcessor模拟登录：" class="headerlink" title="使用cookielib库和HTTPCookieProcessor模拟登录："></a>使用cookielib库和HTTPCookieProcessor模拟登录：</h3><p>Cookie 是指网站服务器为了辨别用户身份和进行Session跟踪，而储存在用户浏览器上的文本文件，Cookie可以保持登录信息到用户下次与服务器的会话。</p><p>这里以人人网为例。人人网中，要访问某个人的主页，必须先登录才能访问，登录说白了就是要有cookie信息。那么如果我们想要用代码的方式访问，就必须要有正确的cookie信息才能访问。解决方案有两种，第一种是使用浏览器访问，然后将cookie信息复制下来，放到headers中。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;anonymid=jacdwz2x-8bjldx; depovince=GW; _r01_=1; _ga=GA1.2.1455063316.1511436360; _gid=GA1.2.862627163.1511436360; wp=1; JSESSIONID=abczwY8ecd4xz8RJcyP-v; jebecookies=d4497791-9d41-4269-9e2b-3858d4989785|||||; ick_login=884e75d4-f361-4cff-94bb-81fe6c42b220; _de=EA5778F44555C091303554EBBEB4676C696BF75400CE19CC; p=61a3c7d0d4b2d1e991095353f83fa2141; first_login_flag=1; ln_uact=970138074@qq.com; ln_hurl=http://hdn.xnimg.cn/photos/hdn121/20170428/1700/main_nhiB_aebd0000854a1986.jpg; t=3dd84a3117737e819dd2c32f1cdb91d01; societyguester=3dd84a3117737e819dd2c32f1cdb91d01; id=443362311; xnsid=169efdc0; loginfrom=syshome; ch_id=10016; jebe_key=9c062f5a-4335-4a91-bf7a-970f8b86a64e%7Ca022c303305d1b2ab6b5089643e4b5de%7C1511449232839%7C1; wp_fold=0&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.renren.com/880151247/profile&#x27;</span></span><br><span class="line"></span><br><span class="line">req = request.Request(url,headers=headers)</span><br><span class="line">resp = request.urlopen(req)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;renren.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(resp.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><p>但是<strong>每次在访问需要cookie的页面都要从浏览器中复制cookie比较麻烦</strong>。</p><p>在Python处理Cookie，一般是通过<code>http.cookiejar</code>模块和<code>urllib模块的HTTPCookieProcessor</code>处理器类一起使用。<code>http.cookiejar</code>模块主要作用是提供用于存储cookie的对象。而<code>HTTPCookieProcessor</code>处理器主要作用是处理这些cookie对象，并构建handler对象。</p><h4 id="http-cookiejar模块："><a href="#http-cookiejar模块：" class="headerlink" title="http.cookiejar模块："></a>http.cookiejar模块：</h4><p>该模块主要的类有CookieJar、FileCookieJar、MozillaCookieJar、LWPCookieJar。这四个类的作用分别如下：</p><ol><li>CookieJar：管理HTTP cookie值、存储HTTP请求生成的cookie、向传出的HTTP请求添加cookie的对象。整个cookie都存储在内存中，对CookieJar实例进行垃圾回收后cookie也将丢失。</li><li>FileCookieJar (filename,delayload=None,policy=None)：从CookieJar派生而来，用来创建FileCookieJar实例，检索cookie信息并将cookie存储到文件中。filename是存储cookie的文件名。delayload为True时支持延迟访问访问文件，即只有在需要时才读取文件或在文件中存储数据。</li><li>MozillaCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与Mozilla浏览器 cookies.txt兼容的FileCookieJar实例。</li><li>LWPCookieJar (filename,delayload=None,policy=None)：从FileCookieJar派生而来，创建与libwww-perl标准的 Set-Cookie3 文件格式兼容的FileCookieJar实例。</li></ol><h4 id="登录人人网："><a href="#登录人人网：" class="headerlink" title="登录人人网："></a>登录人人网：</h4><p>利用<code>http.cookiejar</code>和<code>request.HTTPCookieProcessor</code>登录人人网。相关示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,parse</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> CookieJar</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_opener</span>():</span><br><span class="line">    cookiejar = CookieJar()</span><br><span class="line">    handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">    opener = request.build_opener(handler)</span><br><span class="line">    <span class="keyword">return</span> opener</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">login_renren</span>(<span class="params">opener</span>):</span><br><span class="line">    data = &#123;<span class="string">&quot;email&quot;</span>: <span class="string">&quot;970138074@qq.com&quot;</span>, <span class="string">&quot;password&quot;</span>: <span class="string">&quot;pythonspider&quot;</span>&#125;</span><br><span class="line">    data = parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    login_url = <span class="string">&quot;http://www.renren.com/PLogin.do&quot;</span></span><br><span class="line">    req = request.Request(login_url, headers=headers, data=data)</span><br><span class="line">    opener.<span class="built_in">open</span>(req)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visit_profile</span>(<span class="params">opener</span>):</span><br><span class="line">    url = <span class="string">&#x27;http://www.renren.com/880151247/profile&#x27;</span></span><br><span class="line">    req = request.Request(url,headers=headers)</span><br><span class="line">    resp = opener.<span class="built_in">open</span>(req)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;renren.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(resp.read().decode(<span class="string">&quot;utf-8&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    opener = get_opener()</span><br><span class="line">    login_renren(opener)</span><br><span class="line">    visit_profile(opener)</span><br></pre></td></tr></table></figure><h4 id="保存cookie到本地："><a href="#保存cookie到本地：" class="headerlink" title="保存cookie到本地："></a>保存cookie到本地：</h4><p>保存<code>cookie</code>到本地，可以使用<code>cookiejar</code>的<code>save</code>方法，并且需要指定一个文件名：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> MozillaCookieJar</span><br><span class="line"></span><br><span class="line">cookiejar = MozillaCookieJar(<span class="string">&quot;cookie.txt&quot;</span>)</span><br><span class="line">handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">req = request.Request(<span class="string">&#x27;http://httpbin.org/cookies&#x27;</span>,headers=headers)</span><br><span class="line"></span><br><span class="line">resp = opener.<span class="built_in">open</span>(req)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br><span class="line">cookiejar.save(ignore_discard=<span class="literal">True</span>,ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="从本地加载cookie："><a href="#从本地加载cookie：" class="headerlink" title="从本地加载cookie："></a>从本地加载cookie：</h4><p>从本地加载<code>cookie</code>，需要使用<code>cookiejar</code>的<code>load</code>方法，并且也需要指定方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> http.cookiejar <span class="keyword">import</span> MozillaCookieJar</span><br><span class="line"></span><br><span class="line">cookiejar = MozillaCookieJar(<span class="string">&quot;cookie.txt&quot;</span>)</span><br><span class="line">cookiejar.load(ignore_expires=<span class="literal">True</span>,ignore_discard=<span class="literal">True</span>)</span><br><span class="line">handler = request.HTTPCookieProcessor(cookiejar)</span><br><span class="line">opener = request.build_opener(handler)</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">req = request.Request(<span class="string">&#x27;http://httpbin.org/cookies&#x27;</span>,headers=headers)</span><br><span class="line"></span><br><span class="line">resp = opener.<span class="built_in">open</span>(req)</span><br><span class="line"><span class="built_in">print</span>(resp.read())</span><br></pre></td></tr></table></figure><br><h2 id="四-requests库"><a href="#四-requests库" class="headerlink" title="四 requests库"></a>四 requests库</h2><p>虽然Python的标准库中 urllib模块已经包含了平常我们使用的大多数功能，但是它的 API 使用起来让人感觉不太好，而 Requests宣传是 “HTTP for Humans”，说明使用更简洁方便。</p><h3 id="安装和文档地址："><a href="#安装和文档地址：" class="headerlink" title="安装和文档地址："></a>安装和文档地址：</h3><p>利用<code>pip</code>可以非常方便的安装：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> requests</span><br></pre></td></tr></table></figure><blockquote><p>中文文档：<a target="_blank" rel="noopener" href="http://docs.python-requests.org/zh_CN/latest/index.html">http://docs.python-requests.org/zh_CN/latest/index.html</a></p><p>github地址：<a target="_blank" rel="noopener" href="https://github.com/requests/requests">https://github.com/requests/requests</a></p></blockquote><h3 id="发送GET请求："><a href="#发送GET请求：" class="headerlink" title="发送GET请求："></a>发送GET请求：</h3><ol><li><p>最简单的发送<code>get</code>请求就是通过<code>requests.get</code>来调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.get(<span class="string">&quot;http://www.baidu.com/&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p>添加headers和查询参数：</p><p>如果想添加 headers，可以传入headers参数来增加请求头中的headers信息。如果要将参数放在url中传递，可以利用 params 参数。相关示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">   </span><br><span class="line">kw = &#123;<span class="string">&#x27;wd&#x27;</span>:<span class="string">&#x27;中国&#x27;</span>&#125;</span><br><span class="line">   </span><br><span class="line">headers = &#123;<span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36&quot;</span>&#125;</span><br><span class="line">   </span><br><span class="line"><span class="comment"># params 接收一个字典或者字符串的查询参数，字典类型自动转换为url编码，不需要urlencode()</span></span><br><span class="line">response = requests.get(<span class="string">&quot;http://www.baidu.com/s&quot;</span>, params = kw, headers = headers)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 查看响应内容，response.text 返回的是Unicode格式的数据</span></span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 查看响应内容，response.content返回的字节流数据</span></span><br><span class="line"><span class="built_in">print</span>(response.content)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 查看完整url地址</span></span><br><span class="line"><span class="built_in">print</span>(response.url)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 查看响应头部字符编码</span></span><br><span class="line"><span class="built_in">print</span>(response.encoding)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># 查看响应码</span></span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br></pre></td></tr></table></figure></li></ol><h3 id="发送POST请求："><a href="#发送POST请求：" class="headerlink" title="发送POST请求："></a>发送POST请求：</h3><ol><li><p>最基本的POST请求可以使用<code>post</code>方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response = requests.post(<span class="string">&quot;http://www.baidu.com/&quot;</span>,data=data)</span><br></pre></td></tr></table></figure></li><li><p>传入data数据：</p><p>这时候就不要再使用<code>urlencode</code>进行编码了，直接传入一个字典进去就可以了。比如请求拉勾网的数据的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">   </span><br><span class="line">url = <span class="string">&quot;https://www.lagou.com/jobs/positionAjax.json?city=%E6%B7%B1%E5%9C%B3&amp;needAddtionalResult=false&amp;isSchoolJob=0&quot;</span></span><br><span class="line">   </span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.lagou.com/jobs/list_python?labelWords=&amp;fromSearch=true&amp;suginput=&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">   </span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;first&#x27;</span>: <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pn&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;kd&#x27;</span>: <span class="string">&#x27;python&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">   </span><br><span class="line">resp = requests.post(url,headers=headers,data=data)</span><br><span class="line"><span class="comment"># 如果是json数据，直接可以调用json方法</span></span><br><span class="line"><span class="built_in">print</span>(resp.json())</span><br></pre></td></tr></table></figure></li></ol><h3 id="使用代理："><a href="#使用代理：" class="headerlink" title="使用代理："></a>使用代理：</h3><p>使用<code>requests</code>添加代理也非常简单，只要在请求的方法中（比如<code>get</code>或者<code>post</code>）传递<code>proxies</code>参数就可以了。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://httpbin.org/get&quot;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">proxy = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;171.14.209.180:27829&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp = requests.get(url,headers=headers,proxies=proxy)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;xx.html&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(resp.text)</span><br></pre></td></tr></table></figure><h3 id="cookie："><a href="#cookie：" class="headerlink" title="cookie："></a>cookie：</h3><p>如果在一个响应中包含了<code>cookie</code>，那么可以利用<code>cookies</code>属性拿到这个返回的<code>cookie</code>值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://www.renren.com/PLogin.do&quot;</span></span><br><span class="line">data = &#123;<span class="string">&quot;email&quot;</span>:<span class="string">&quot;970138074@qq.com&quot;</span>,<span class="string">&#x27;password&#x27;</span>:<span class="string">&quot;pythonspider&quot;</span>&#125;</span><br><span class="line">resp = requests.get(<span class="string">&#x27;http://www.baidu.com/&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(resp.cookies)</span><br><span class="line"><span class="built_in">print</span>(resp.cookies.get_dict())</span><br></pre></td></tr></table></figure><h3 id="session："><a href="#session：" class="headerlink" title="session："></a>session：</h3><p>之前使用<code>urllib</code>库，是可以使用<code>opener</code>发送多个请求，多个请求之间是可以共享<code>cookie</code>的。那么如果使用<code>requests</code>，也要达到共享<code>cookie</code>的目的，那么可以使用<code>requests</code>库给我们提供的<code>session</code>对象。</p><p>注意，这里的<code>session</code>不是web开发中的那个session，这个地方只是一个会话的对象而已。还是以登录人人网为例，使用<code>requests</code>来实现。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://www.renren.com/PLogin.do&quot;</span></span><br><span class="line">data = &#123;<span class="string">&quot;email&quot;</span>:<span class="string">&quot;970138074@qq.com&quot;</span>,<span class="string">&#x27;password&#x27;</span>:<span class="string">&quot;pythonspider&quot;</span>&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录</span></span><br><span class="line">session = requests.session()</span><br><span class="line">session.post(url,data=data,headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问大鹏个人中心</span></span><br><span class="line">resp = session.get(<span class="string">&#x27;http://www.renren.com/880151247/profile&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(resp.text)</span><br></pre></td></tr></table></figure><h3 id="处理不信任的SSL证书："><a href="#处理不信任的SSL证书：" class="headerlink" title="处理不信任的SSL证书："></a>处理不信任的SSL证书：</h3><p>对于那些已经被信任的SSL整数的网站，比如<code>https://www.baidu.com/</code>，那么使用<code>requests</code>直接就可以正常的返回响应。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">resp = requests.get(<span class="string">&#x27;http://www.12306.cn/mormhweb/&#x27;</span>,verify=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(resp.content.decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br></pre></td></tr></table></figure><br><h2 id="五-页面解析和数据提取"><a href="#五-页面解析和数据提取" class="headerlink" title="五 页面解析和数据提取"></a>五 页面解析和数据提取</h2><h3 id="1-两种数据处理"><a href="#1-两种数据处理" class="headerlink" title="1 两种数据处理"></a>1 两种数据处理</h3><p><strong>非结构化的数据处理</strong></p><ul><li>文本、电话号码、邮箱地址：<strong>正则表达式</strong></li><li>Html文件：<strong>正则表达式、XPath、CSS选择器</strong></li></ul><p><strong>非结构化的数据处理</strong>：JSON字符串</p><br><h3 id="2-XPath语法和lxml模块"><a href="#2-XPath语法和lxml模块" class="headerlink" title="2 XPath语法和lxml模块"></a>2 XPath语法和lxml模块</h3><p><strong>什么是XPath？</strong></p><p>xpath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。</p><p><strong>XPath开发工具</strong></p><ol><li>Chrome插件XPath Helper。</li><li>Firefox插件Try XPath。</li></ol><h3 id="3-XPath语法"><a href="#3-XPath语法" class="headerlink" title="3 XPath语法"></a>3 XPath语法</h3><h4 id="选取节点："><a href="#选取节点：" class="headerlink" title="选取节点："></a>选取节点：</h4><p>XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的电脑文件系统中看到的表达式非常相似。</p><table><thead><tr><th>表达式</th><th>描述</th><th>示例</th><th>结果</th></tr></thead><tbody><tr><td>nodename</td><td>选取此节点的所有子节点</td><td>bookstore</td><td>选取bookstore下所有的子节点</td></tr><tr><td>/</td><td>如果是在最前面，代表从根节点选取。否则选择某节点下的某个节点</td><td>/bookstore</td><td>选取根元素下所有的bookstore节点</td></tr><tr><td>//</td><td>从全局节点中选择节点，随便在哪个位置</td><td>//book</td><td>从全局节点中找到所有的book节点</td></tr><tr><td>@</td><td>选取某个节点的属性</td><td>//book[@price]</td><td>选择所有拥有price属性的book节点</td></tr><tr><td>.</td><td>当前节点</td><td>./a</td><td>选取当前节点下的a标签</td></tr></tbody></table><h4 id="谓语："><a href="#谓语：" class="headerlink" title="谓语："></a>谓语：</h4><p>谓语用来查找某个特定的节点或者包含某个指定的值的节点，被嵌在方括号中。<br>在下面的表格中，我们列出了带有谓语的一些路径表达式，以及表达式的结果：</p><table><thead><tr><th>路径表达式</th><th>描述</th></tr></thead><tbody><tr><td>/bookstore/book[1]</td><td>选取bookstore下的第一个子元素</td></tr><tr><td>/bookstore/book[last()]</td><td>选取bookstore下的倒数第二个book元素。</td></tr><tr><td>bookstore/book[position()&lt;3]</td><td>选取bookstore下前面两个子元素。</td></tr><tr><td>//book[@price]</td><td>选取拥有price属性的book元素</td></tr><tr><td>//book[@price=10]</td><td>选取所有属性price等于10的book元素</td></tr></tbody></table><h4 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h4><p>*表示通配符。</p><table><thead><tr><th align="left">通配符</th><th align="left">描述</th><th align="left">示例</th><th align="left">结果</th></tr></thead><tbody><tr><td align="left">*</td><td align="left">匹配任意节点</td><td align="left">/bookstore/*</td><td align="left">选取bookstore下的所有子元素。</td></tr><tr><td align="left">@*</td><td align="left">匹配节点中的任何属性</td><td align="left">//book[@*]</td><td align="left">选取所有带有属性的book元素。</td></tr></tbody></table><h4 id="选取多个路径："><a href="#选取多个路径：" class="headerlink" title="选取多个路径："></a>选取多个路径：</h4><p>通过在路径表达式中使用“|”运算符，可以选取若干个路径。<br>示例如下：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">//</span>bookstore<span class="regexp">/book | /</span><span class="regexp">/book/</span>title</span><br><span class="line"><span class="comment"># 选取所有book元素以及book元素下所有的title元素</span></span><br></pre></td></tr></table></figure><h4 id="运算符："><a href="#运算符：" class="headerlink" title="运算符："></a>运算符：</h4><p><img src="/.io//p2.png" alt="img"></p><br><h3 id="4-lxml库"><a href="#4-lxml库" class="headerlink" title="4 lxml库"></a>4 lxml库</h3><p>lxml 是 一个HTML/XML的解析器，主要的功能是如何解析和提取 HTML/XML 数据。</p><p>lxml和正则一样，也是用 C 实现的，是一款高性能的 Python HTML/XML 解析器，我们可以利用之前学习的XPath语法，来快速的定位特定元素以及节点信息。</p><blockquote><p>lxml python 官方文档：<a target="_blank" rel="noopener" href="http://lxml.de/index.html">http://lxml.de/index.html</a></p><p>需要安装C语言库，可使用 pip 安装：pip install lxml【或用wheel安装】</p></blockquote><h4 id="基本使用："><a href="#基本使用：" class="headerlink" title="基本使用："></a>基本使用：</h4><p>我们可以利用他来解析HTML代码，并且在解析HTML代码的时候，如果HTML代码不规范，他会自动的进行补全。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 lxml 的 etree 库</span></span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree </span><br><span class="line"></span><br><span class="line">text = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;div&gt;</span></span><br><span class="line"><span class="string">    &lt;ul&gt;</span></span><br><span class="line"><span class="string">         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">         &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt; # 注意，此处缺少一个 &lt;/li&gt; 闭合标签</span></span><br><span class="line"><span class="string">     &lt;/ul&gt;</span></span><br><span class="line"><span class="string"> &lt;/div&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#利用etree.HTML，将字符串解析为HTML文档</span></span><br><span class="line">html = etree.HTML(text) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 按字符串序列化HTML文档</span></span><br><span class="line">result = etree.tostring(html) </span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输入结果如下：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link1.html&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-inactive&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到，lxml会自动修改HTML代码【自动补全】</p><blockquote><p>例子中不仅补全了li标签，还添加了body，html标签</p></blockquote><h4 id="从文件中读取html代码："><a href="#从文件中读取html代码：" class="headerlink" title="从文件中读取html代码："></a>从文件中读取html代码：</h4><p>除了直接使用字符串进行解析，lxml还支持从文件中读取内容。我们新建一个hello.html文件：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hello.html --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link1.html&quot;</span>&gt;</span>first item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link2.html&quot;</span>&gt;</span>second item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-inactive&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link3.html&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;bold&quot;</span>&gt;</span>third item<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-1&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link4.html&quot;</span>&gt;</span>fourth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">li</span> <span class="attr">class</span>=<span class="string">&quot;item-0&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;link5.html&quot;</span>&gt;</span>fifth item<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure><p>然后利用<code>etree.parse()</code>方法来读取文件。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取外部文件 hello.html</span></span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = etree.tostring(html, pretty_print=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>输入结果和之前是相同的。</p><h4 id="在lxml中使用XPath语法："><a href="#在lxml中使用XPath语法：" class="headerlink" title="在lxml中使用XPath语法："></a>在lxml中使用XPath语法：</h4><ol><li><p>获取所有li标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(html)  <span class="comment"># 显示etree.parse() 返回类型</span></span><br><span class="line">   </span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)  <span class="comment"># 打印&lt;li&gt;标签的元素集合</span></span><br></pre></td></tr></table></figure></li><li><p>获取所有li元素下的所有class属性的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li/@class&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>获取li标签下href为<code>www.baidu.com</code>的a标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li/a[@href=&quot;www.baidu.com&quot;]&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>获取li标签下所有span标签：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="comment">#result = html.xpath(&#x27;//li/span&#x27;)</span></span><br><span class="line"><span class="comment">#注意这么写是不对的：</span></span><br><span class="line"><span class="comment">#因为 / 是用来获取子元素的，而 &lt;span&gt; 并不是 &lt;li&gt; 的子元素，所以，要用双斜杠</span></span><br><span class="line">   </span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li//span&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>获取li标签下的a标签里的所有class：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li/a//@class&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>获取最后一个li的a的href属性对应的值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li[last()]/a/@href&#x27;</span>)</span><br><span class="line"><span class="comment"># 谓语 [last()] 可以找到最后一个元素</span></span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>获取倒数第二个li元素的内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li[last()-1]/a&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="comment"># text 方法可以获取元素内容</span></span><br><span class="line"><span class="built_in">print</span>(result[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure></li><li><p>获取倒数第二个li元素的内容的第二种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li[last()-1]/a/text()&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>寻找符合要求的li标签，要求其属性class中包含 <code>item-</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">   </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li[contain(@class, &#x27;</span>item-<span class="string">&#x27;)]&#x27;</span>)</span><br><span class="line">   </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li><li><p>寻找符合要求的li标签，要求其内容中包含 <code>item</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line">    </span><br><span class="line">html = etree.parse(<span class="string">&#x27;hello.html&#x27;</span>)</span><br><span class="line">result = html.xpath(<span class="string">&#x27;//li[contain(txt(), &#x27;</span>item<span class="string">&#x27;)]&#x27;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure></li></ol><br><h3 id="5-BeautifulSoup4-库"><a href="#5-BeautifulSoup4-库" class="headerlink" title="5 BeautifulSoup4 库"></a>5 BeautifulSoup4 库</h3><p>和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。</p><p>lxml 只会局部遍历，而Beautiful Soup 是基于HTML DOM（Document Object Model）的，会载入整个文档，解析整个DOM树，因此时间和内存开销都会大很多，所以性能要低于lxml。</p><p>BeautifulSoup 用来解析 HTML 比较简单，API非常人性化，支持CSS选择器、Python标准库中的HTML解析器，也支持 lxml 的 XML解析器。</p><h4 id="安装和文档："><a href="#安装和文档：" class="headerlink" title="安装和文档："></a>安装和文档：</h4><blockquote><p>Beautiful Soup 3 目前已经停止开发，推荐现在的项目使用Beautiful Soup 4。</p><p>视频教程<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jM4y1j7rb/?p=56&spm_id_from=pageDriver&vd_source=ad866fe26d18693e4132a3c33f8fba36">参考此处</a></p></blockquote><ol><li>安装：<code>pip install bs4</code>。</li><li>中文文档：<a target="_blank" rel="noopener" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html">https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html</a></li></ol><h4 id="几大解析工具对比："><a href="#几大解析工具对比：" class="headerlink" title="几大解析工具对比："></a>几大解析工具对比：</h4><table><thead><tr><th>解析工具</th><th>解析速度</th><th>使用难度</th></tr></thead><tbody><tr><td>BeautifulSoup</td><td>最慢</td><td>最简单</td></tr><tr><td>lxml</td><td>快</td><td>简单</td></tr><tr><td>正则</td><td>最快</td><td>最难</td></tr></tbody></table><h4 id="简单使用："><a href="#简单使用：" class="headerlink" title="简单使用："></a>简单使用：</h4><p>常用解析器：</p><p>【Python标准库】<code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></p><p>【lxml HTML 解析器】<code>BeautifulSoup(markup, &quot;lxml&quot;)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建 Beautiful Soup 对象</span></span><br><span class="line"><span class="comment"># 使用lxml来进行解析</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">&quot;lxml&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(soup.prettify())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;html&gt;</span></span><br><span class="line"><span class="string"> &lt;head&gt;</span></span><br><span class="line"><span class="string">  &lt;title&gt;</span></span><br><span class="line"><span class="string">   The Dormouse&#x27;s story</span></span><br><span class="line"><span class="string">  &lt;/title&gt;</span></span><br><span class="line"><span class="string"> &lt;/head&gt;</span></span><br><span class="line"><span class="string"> &lt;body&gt;</span></span><br><span class="line"><span class="string">  &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;</span></span><br><span class="line"><span class="string">   &lt;b&gt;</span></span><br><span class="line"><span class="string">    The Dormouse&#x27;s story</span></span><br><span class="line"><span class="string">   &lt;/b&gt;</span></span><br><span class="line"><span class="string">  &lt;/p&gt;</span></span><br><span class="line"><span class="string">  &lt;p class=&quot;story&quot;&gt;</span></span><br><span class="line"><span class="string">   Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;</span></span><br><span class="line"><span class="string">    &lt;!-- Elsie --&gt;</span></span><br><span class="line"><span class="string">   &lt;/a&gt;</span></span><br><span class="line"><span class="string">   ,</span></span><br><span class="line"><span class="string">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/lacie&quot; id=&quot;link2&quot;&gt;</span></span><br><span class="line"><span class="string">    Lacie</span></span><br><span class="line"><span class="string">   &lt;/a&gt;</span></span><br><span class="line"><span class="string">   and</span></span><br><span class="line"><span class="string">   &lt;a class=&quot;sister&quot; href=&quot;http://example.com/tillie&quot; id=&quot;link3&quot;&gt;</span></span><br><span class="line"><span class="string">    Tillie</span></span><br><span class="line"><span class="string">   &lt;/a&gt;</span></span><br><span class="line"><span class="string">   ;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.</span></span><br><span class="line"><span class="string">  &lt;/p&gt;</span></span><br><span class="line"><span class="string">  &lt;p class=&quot;story&quot;&gt;</span></span><br><span class="line"><span class="string">   ...</span></span><br><span class="line"><span class="string">  &lt;/p&gt;</span></span><br><span class="line"><span class="string"> &lt;/body&gt;</span></span><br><span class="line"><span class="string">&lt;/html&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="四个常用的对象"><a href="#四个常用的对象" class="headerlink" title="四个常用的对象"></a>四个常用的对象</h4><p>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种:</p><ol><li>Tag</li><li>NavigatableString</li><li>BeautifulSoup</li><li>Comment</li></ol><h5 id="1-Tag"><a href="#1-Tag" class="headerlink" title="1. Tag"></a>1. Tag</h5><p>Tag 通俗点讲就是 HTML 中的一个个标签。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string">&lt;body&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建 Beautiful Soup 对象</span></span><br><span class="line">soup = BeautifulSoup(html,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.title</span><br><span class="line"><span class="comment"># &lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.head</span><br><span class="line"><span class="comment"># &lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.a</span><br><span class="line"><span class="comment"># &lt;a class=&quot;sister&quot; href=&quot;http://example.com/elsie&quot; id=&quot;link1&quot;&gt;&lt;!-- Elsie --&gt;&lt;/a&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.p</span><br><span class="line"><span class="comment"># &lt;p class=&quot;title&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(soup.p)</span><br><span class="line"><span class="comment"># &lt;class &#x27;bs4.element.Tag&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>我们可以利用 soup 加标签名轻松地获取这些标签的内容，这些对象的类型是bs4.element.Tag。但是注意，它查找的是在所有内容中的第一个符合要求的标签。如果要查询所有的标签，后面会进行介绍。<br>对于Tag，它有两个重要的属性，分别是name和attrs。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> soup.name</span><br><span class="line"><span class="comment"># [document] #soup 对象本身比较特殊，它的 name 即为 [document]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.head.name</span><br><span class="line"><span class="comment"># head #对于其他内部标签，输出的值便为标签本身的名称</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.p.attrs</span><br><span class="line"><span class="comment"># &#123;&#x27;class&#x27;: [&#x27;title&#x27;], &#x27;name&#x27;: &#x27;dromouse&#x27;&#125;</span></span><br><span class="line"><span class="comment"># 在这里，我们把 p 标签的所有属性打印输出了出来，得到的类型是一个字典。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> soup.p[<span class="string">&#x27;class&#x27;</span>] <span class="comment"># soup.p.get(&#x27;class&#x27;)</span></span><br><span class="line"><span class="comment"># [&#x27;title&#x27;] #还可以利用get方法，传入属性的名称，二者是等价的</span></span><br><span class="line"></span><br><span class="line">soup.p[<span class="string">&#x27;class&#x27;</span>] = <span class="string">&quot;newClass&quot;</span></span><br><span class="line"><span class="built_in">print</span> soup.p <span class="comment"># 可以对这些属性和内容等等进行修改</span></span><br><span class="line"><span class="comment"># &lt;p class=&quot;newClass&quot; name=&quot;dromouse&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span></span><br></pre></td></tr></table></figure><h5 id="2-NavigableString"><a href="#2-NavigableString" class="headerlink" title="2. NavigableString"></a>2. NavigableString</h5><p>如果拿到标签后，还想获取标签中的内容。那么可以通过<code>tag.string</code>获取标签中的文字。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> soup.p.string</span><br><span class="line"><span class="comment"># The Dormouse&#x27;s story</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(soup.p.string)</span><br><span class="line"><span class="comment"># &lt;class &#x27;bs4.element.NavigableString&#x27;&gt;thon</span></span><br></pre></td></tr></table></figure><h5 id="3-BeautifulSoup"><a href="#3-BeautifulSoup" class="headerlink" title="3. BeautifulSoup"></a>3. BeautifulSoup</h5><p>BeautifulSoup 对象表示的是一个文档的全部内容，大部分时候，可以把它当作 Tag 对象。它支持 <strong>遍历文档树</strong> 和 <strong>搜索文档树</strong> 中描述的大部分的方法。</p><p>因为 <code>BeautifulSoup</code> 对象并不是真正的HTML或XML的tag,所以它没有name和attribute属性。但有时查看它的 <code>.name</code> 属性是很方便的，所以 BeautifulSoup 对象包含了一个值为 “[document]” 的特殊属性 <code>.name</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup.name</span><br><span class="line"><span class="comment"># &#x27;[document]&#x27;</span></span><br></pre></td></tr></table></figure><h5 id="4-Comment"><a href="#4-Comment" class="headerlink" title="4. Comment"></a>4. Comment</h5><p>Tag , NavigableString , BeautifulSoup 几乎覆盖了html和xml中的所有内容。但是还有一些特殊对象：文档的注释部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">markup = <span class="string">&quot;&lt;b&gt;&lt;!--Hey, buddy. Want to buy a used parser?--&gt;&lt;/b&gt;&quot;</span></span><br><span class="line">soup = BeautifulSoup(markup)</span><br><span class="line">comment = soup.b.string</span><br><span class="line"><span class="built_in">type</span>(comment)</span><br><span class="line"><span class="comment"># &lt;class &#x27;bs4.element.Comment&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>Comment 对象是一个特殊类型的 NavigableString 对象:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">comment</span><br><span class="line"><span class="comment"># &#x27;Hey, buddy. Want to buy a used parser&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="遍历文档树："><a href="#遍历文档树：" class="headerlink" title="遍历文档树："></a>遍历文档树：</h4><h5 id="1-contents和children"><a href="#1-contents和children" class="headerlink" title="1. contents和children"></a>1. contents和children</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">html_doc = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">&lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#x27;s story&lt;/title&gt;&lt;/head&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#x27;s story&lt;/b&gt;&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and</span></span><br><span class="line"><span class="string">&lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;;</span></span><br><span class="line"><span class="string">and they lived at the bottom of a well.&lt;/p&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;p class=&quot;story&quot;&gt;...&lt;/p&gt;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(html_doc,<span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line">head_tag = soup.head</span><br><span class="line"><span class="comment"># 返回所有子节点的列表</span></span><br><span class="line"><span class="built_in">print</span>(head_tag.contents)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回所有子节点的迭代器</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> head_tag.children:</span><br><span class="line">    <span class="built_in">print</span>(child)</span><br></pre></td></tr></table></figure><h5 id="2-strings-和-stripped-strings"><a href="#2-strings-和-stripped-strings" class="headerlink" title="2. strings 和 stripped_strings"></a>2. strings 和 stripped_strings</h5><p>如果tag中包含多个字符串 [2] ,可以使用 .strings 来循环获取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> string <span class="keyword">in</span> soup.strings:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">repr</span>(string))</span><br><span class="line">    <span class="comment"># u&quot;The Dormouse&#x27;s story&quot;</span></span><br><span class="line">    <span class="comment"># u&#x27;\n\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&quot;The Dormouse&#x27;s story&quot;</span></span><br><span class="line">    <span class="comment"># u&#x27;\n\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Once upon a time there were three little sisters; and their names were\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Elsie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;,\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Lacie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27; and\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Tillie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;;\nand they lived at the bottom of a well.&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;\n\n&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;...&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;\n&#x27;</span></span><br></pre></td></tr></table></figure><p>输出的字符串中可能包含了很多空格或空行,使用 .stripped_strings 可以去除多余空白内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> string <span class="keyword">in</span> soup.stripped_strings:</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">repr</span>(string))</span><br><span class="line">    <span class="comment"># u&quot;The Dormouse&#x27;s story&quot;</span></span><br><span class="line">    <span class="comment"># u&quot;The Dormouse&#x27;s story&quot;</span></span><br><span class="line">    <span class="comment"># u&#x27;Once upon a time there were three little sisters; and their names were&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Elsie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;,&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Lacie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;and&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;Tillie&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;;\nand they lived at the bottom of a well.&#x27;</span></span><br><span class="line">    <span class="comment"># u&#x27;...&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="搜索文档树"><a href="#搜索文档树" class="headerlink" title="搜索文档树"></a>搜索文档树</h4><h5 id="1-find和find-all方法："><a href="#1-find和find-all方法：" class="headerlink" title="1. find和find_all方法："></a>1. find和find_all方法：</h5><p>搜索文档树，一般用得比较多的就是两个方法，一个是<code>find</code>，一个是<code>find_all</code>。<code>find</code>方法是找到第一个满足条件的标签后就立即返回，只返回一个元素。<code>find_all</code>方法是把所有满足条件的标签都选到，然后返回回去。使用这两个方法，最常用的用法是出入<code>name</code>以及<code>attr</code>参数找出符合要求的标签。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(<span class="string">&quot;a&quot;</span>,attrs=&#123;<span class="string">&quot;id&quot;</span>:<span class="string">&quot;link2&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><p>或者是直接传入属性的的名字作为关键字参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(<span class="string">&quot;a&quot;</span>,<span class="built_in">id</span>=<span class="string">&#x27;link2&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="2-select方法："><a href="#2-select方法：" class="headerlink" title="2. select方法："></a>2. select方法：</h5><p>使用以上方法可以方便的找出元素。但有时候使用<code>css</code>选择器的方式可以更加的方便。使用<code>css</code>选择器的语法，应该使用<code>select</code>方法。以下列出几种常用的<code>css</code>选择器方法：</p><p><strong>（1）通过标签名查找：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;a&#x27;</span>))</span><br></pre></td></tr></table></figure><p><strong>（2）通过类名查找：</strong></p><p>通过类名，则应该在类的前面加一个<code>.</code>。比如要查找<code>class=sister</code>的标签。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;.sister&#x27;</span>))</span><br></pre></td></tr></table></figure><p><strong>（3）通过id查找：</strong></p><p>通过id查找，应该在id的名字前面加一个＃号。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&quot;#link1&quot;</span>))</span><br></pre></td></tr></table></figure><p><strong>（4）组合查找：</strong></p><p>组合查找即和写 class 文件时，标签名与类名、id名进行的组合原理是一样的，例如查找 p 标签中，id 等于 link1的内容，二者需要用空格分开：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&quot;p #link1&quot;</span>))</span><br></pre></td></tr></table></figure><p>直接子标签查找，则使用 &gt; 分隔：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&quot;head &gt; title&quot;</span>))</span><br></pre></td></tr></table></figure><p><strong>（5）通过属性查找：</strong></p><p>查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一节点，所以中间不能加空格，否则会无法匹配到。示例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(soup.select(<span class="string">&#x27;a[href=&quot;http://example.com/elsie&quot;]&#x27;</span>))</span><br></pre></td></tr></table></figure><p><strong>（6）获取内容</strong></p><p>以上的 select 方法返回的结果都是列表形式，可以遍历形式输出，然后用 <code>get_text()</code> 方法来获取它的内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(html, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> <span class="built_in">type</span>(soup.select(<span class="string">&#x27;title&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span> soup.select(<span class="string">&#x27;title&#x27;</span>)[<span class="number">0</span>].get_text()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> title <span class="keyword">in</span> soup.select(<span class="string">&#x27;title&#x27;</span>):</span><br><span class="line">    <span class="built_in">print</span> title.get_text()</span><br></pre></td></tr></table></figure><br><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><blockquote><p>爬取淘宝网首页所有链接</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.taobao.com/&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.212.400 QQBrowser/12.0.5443.400&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp = requests.get(url, headers = headers)</span><br><span class="line"><span class="comment"># print(resp.text)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#创建 Beautiful Soup 对象</span></span><br><span class="line"><span class="comment"># 使用lxml来进行解析</span></span><br><span class="line">soup = BeautifulSoup(resp.text, <span class="string">&quot;lxml&quot;</span>)</span><br><span class="line">a_list = soup.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(a_list))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><blockquote><p>大概看了些，个人觉得学这东西就是浪费生命！！！Over！！！</p></blockquote><br><h2 id="六-案例"><a href="#六-案例" class="headerlink" title="六 案例"></a>六 案例</h2><h3 id="案例一：百度贴吧图片下载"><a href="#案例一：百度贴吧图片下载" class="headerlink" title="案例一：百度贴吧图片下载"></a>案例一：百度贴吧图片下载</h3><ol><li><p>通过 <code>request</code> 拿到网页的源代码数据</p></li><li><p>通过 <code>lxml</code> 对源代码数据进行解析，拿到图片的url</p></li><li><p>依次对图片数据发送网络请求</p></li><li><p>将图片的原始内容写入【<code>.content</code> 表示多媒体二进制数据， <code>.text</code> 表示文本数据】</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可修改为根据用户输入去下载图片</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">index_url= <span class="string">&#x27;https://tieba.baidu.com/p/5475267611&#x27;</span></span><br><span class="line"></span><br><span class="line">response = requests.get(index_url).text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要观察标签特点</span></span><br><span class="line"><span class="comment"># print(response)</span></span><br><span class="line"></span><br><span class="line">selector = etree.HTML(response)</span><br><span class="line">imageURL = selector.xpath(<span class="string">&#x27;//img[@class=&quot;BDE_Image&quot;]/@src&#x27;</span>) <span class="comment">#!!!</span></span><br><span class="line"></span><br><span class="line">offset = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> imageURL:</span><br><span class="line">    <span class="built_in">print</span>(url)</span><br><span class="line">    image_content = requests.get(url).content;<span class="comment"># .content获取其二进制数据</span></span><br><span class="line">    <span class="comment"># 打开一个文件并写入，wb表示：写入|二进制</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;&#123;&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(offset),<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(image_content)</span><br><span class="line">    offset += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">https://imgsa.baidu.com/forum/w%3D580/sign=99114e38abec08fa260013af69ef3d4d/e549b13533fa828bc80c7764f61f4134960a5a85.jpg</span></span><br><span class="line"><span class="string">https://imgsa.baidu.com/forum/w%3D580/sign=f21cd344dcca7bcb7d7bc7278e086b3f/29b83bc79f3df8dc51126516c611728b46102885.jpg</span></span><br><span class="line"><span class="string">https://imgsa.baidu.com/forum/w%3D580/sign=cca7165d1ece36d3a20483380af23a24/f419ddc451da81cbaec9694a5966d01608243185.jpg</span></span><br><span class="line"><span class="string">https://imgsa.baidu.com/forum/w%3D580/sign=e03a58b6aa8b87d65042ab1737092860/edf88c5494eef01f690ff242ebfe9925bd317d91.jpg</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><br><h3 id="案例二：爬取酷狗音乐"><a href="#案例二：爬取酷狗音乐" class="headerlink" title="案例二：爬取酷狗音乐"></a>案例二：爬取酷狗音乐</h3><blockquote><p>爬虫的本质是模拟人类使用浏览器的动作，用于处理大量重复性工作，而已罢了！</p><p>还是需要人工去Chrome控制台寻找和鉴别：哪一个是获取歌单的地址，哪一个是发送的数据请求的地址</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载一首</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">m_url = &#x27;https://webfs.hw.kugou.com/202401131033/113f13ac59f205991e95a34cc1e9d028/v2/5ecc91e40b574f36402d3c855e5b759a/part/0/960139/G350/M04/6A/A7/clip_PpUEAGURylKAFN0_AElYWkYfMIA588.mp3&#x27;;</span></span><br><span class="line"><span class="string">headers = &#123;</span></span><br><span class="line"><span class="string">    &#x27;user-agent&#x27;: &#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.212.400 QQBrowser/12.0.5443.400&#x27;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">m_resp = requests.get(m_url, headers = headers)</span></span><br><span class="line"><span class="string">with open(&#x27;z.mp3&#x27;, &#x27;wb&#x27;) as f:</span></span><br><span class="line"><span class="string">        f.write(m_resp.content)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 批量下载</span></span><br><span class="line"><span class="comment"># 获取音乐列表</span></span><br><span class="line">url_list = <span class="string">&#x27;https://complexsearch.kugou.com/v2/search/song?callback=callback123&amp;srcappid=2919&amp;clientver=1000&amp;clienttime=1705114220066&amp;mid=ff6d51141ffbf050b7bad0a1be2407bb&amp;uuid=ff6d51141ffbf050b7bad0a1be2407bb&amp;dfid=08xpcQ0eWjfG2SxZwN4VFwfS&amp;keyword=%E5%A4%A7%E6%B5%B7&amp;page=1&amp;pagesize=30&amp;bitrate=0&amp;isfuzzy=0&amp;inputtype=0&amp;platform=WebFilter&amp;userid=0&amp;iscorrection=1&amp;privilege_filter=0&amp;filter=10&amp;token=&amp;appid=1014&amp;signature=6ba6aa4a32784248d329dfacdcb2e8a1&#x27;</span></span><br><span class="line">list_resp = requests.get(url_list, headers = headers)</span><br><span class="line"><span class="comment"># print(list_resp.text) # 发现不是json，需要处理</span></span><br><span class="line"><span class="comment"># print(json.loads(list_resp.text[12: -2])[&#x27;data&#x27;][&#x27;lists&#x27;])</span></span><br><span class="line">song_list = json.loads(list_resp.text[<span class="number">12</span>: -<span class="number">2</span>])[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;lists&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, song <span class="keyword">in</span> <span class="built_in">enumerate</span>(song_list):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;index + <span class="number">1</span>&#125;</span>---<span class="subst">&#123;song[<span class="string">&#x27;SongName&#x27;</span>]&#125;</span>----<span class="subst">&#123;song.get(<span class="string">&#x27;FileHash&#x27;</span>)&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line">num = <span class="built_in">input</span>(<span class="string">&#x27;请输入要下载第几首音乐：&#x27;</span>)</span><br><span class="line"><span class="comment"># 此处根据hash值获取音乐url，如“https://wwwapi.kugou.com/play/songinfo?srcappid=2919&amp;clientver=20000&amp;clienttime=1705116154067&amp;mid=ff6d51141ffbf050b7bad0a1be2407bb&amp;uuid=ff6d51141ffbf050b7bad0a1be2407bb&amp;dfid=08xpcQ0eWjfG2SxZwN4VFwfS&amp;appid=1014&amp;platid=4&amp;encode_album_audio_id=6wi1bhbc&amp;token=&amp;userid=0&amp;signature=96c4a29c065cf9d6e59cc866fa49c8d5”</span></span><br><span class="line"><span class="comment"># 需要自己寻找通过hash值获取url的请求地址</span></span><br><span class="line">info_url = <span class="string">f&#x27;https://wwwapi.kugou.com/play/songinfo?srcappid=2919&amp;clientver=20000&amp;clienttime=1705116154067&amp;mid=ff6d51141ffbf050b7bad0a1be2407bb&amp;uuid=ff6d51141ffbf050b7bad0a1be2407bb&amp;dfid=08xpcQ0eWjfG2SxZwN4VFwfS&amp;appid=1014&amp;platid=4&amp;userid=0&amp;signature=<span class="subst">&#123;song_list[<span class="built_in">int</span>(num) - <span class="number">1</span>].get(<span class="string">&quot;FileHash&quot;</span>)&#125;</span>&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(info_url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 之后进行请求就行了，但是现在酷狗做了加密应该是，所以该案例无法完成！</span></span><br><span class="line"><span class="comment"># 大致思路是这样的！</span></span><br></pre></td></tr></table></figure><br><h3 id="案例三：法拍网数据获取【无法运行】"><a href="#案例三：法拍网数据获取【无法运行】" class="headerlink" title="案例三：法拍网数据获取【无法运行】"></a>案例三：法拍网数据获取【无法运行】</h3><blockquote><p><a target="_blank" rel="noopener" href="https://www.rmfysszc.gov.cn/">人民法院诉讼资产网</a></p><p>任务：在项目中心中，拿取房屋信息</p><p>小工具Tip：浏览器中xpath插件</p></blockquote><p><img src="/.io//p3.png"></p><p>四个步骤：</p><ol><li>发送请求</li><li>查看结果</li><li>提取数据</li><li>处理数据</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要主要该网站的反爬策略，需要在headers中加入Refer</span></span><br><span class="line">url = <span class="string">&#x27;https://www1.rmfysszc.gov.cn/ProjectHandle.shtml&#x27;</span></span><br><span class="line">form_data = &#123;</span><br><span class="line">    <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;area&#x27;</span>: <span class="string">&#x27;北京市&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;city&#x27;</span>: <span class="string">&#x27;北京市&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;city1&#x27;</span>: <span class="string">&#x27;----&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;city2&#x27;</span>: <span class="string">&#x27;==请选择==&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;xmxz&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;state&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;money&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;money1&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;number&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fid1&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fid2&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;fid3&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;order&#x27;</span>: <span class="string">&#x27;0&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;page&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;include&#x27;</span>: <span class="string">&#x27;0&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.212.400 QQBrowser/12.0.5443.400&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www1.rmfysszc.gov.cn/projects.shtml?dh=3&amp;gpstate=1&amp;wsbm_slt=1&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">resp = requests.post(url, headers = headers, data = form_data)</span><br><span class="line"><span class="built_in">print</span>(resp.text)</span><br><span class="line"><span class="comment"># 此处发现，该网站的反爬机制很好，只能采用XPath来试试</span></span><br><span class="line"><span class="comment"># 此处一下均不可行...,仅是思路</span></span><br><span class="line">e = etree.HTML(resp.text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取数据</span></span><br><span class="line">title = e.xpath(<span class="string">&#x27;//div[@class = &quot;product&quot;]/div[@class = &quot;p_img&quot;]/a/@title&#x27;</span>)</span><br><span class="line">price1 = e.xpath(<span class="string">&#x27;//div[@class = &quot;product&quot;]/div[2]/p[1]/span/text()&#x27;</span>)</span><br><span class="line"><span class="comment"># </span></span><br></pre></td></tr></table></figure><br><h3 id="案例四：获取虎扑NBA数据"><a href="#案例四：获取虎扑NBA数据" class="headerlink" title="案例四：获取虎扑NBA数据"></a>案例四：获取虎扑NBA数据</h3><blockquote><p>此处提到过使用vscode调试python代码的过程中，可以使用 <code>#%%</code> 来包裹代码块，使得该部分选择性执行</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要主要该网站的反爬策略，需要在headers中加入Refer</span></span><br><span class="line">url = <span class="string">&#x27;https://nba.hupu.com/stats/players&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.212.400 QQBrowser/12.0.5443.400&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resp = requests.get(url, headers = headers)</span><br><span class="line"><span class="comment"># print(resp.text)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取数据【数据是类似于表格的数据】</span></span><br><span class="line">e = etree.HTML(resp.text)</span><br><span class="line"></span><br><span class="line">nos = e.xpath(<span class="string">&#x27;//table[@class = &quot;players_table&quot;]//tr/td[1]/text()&#x27;</span>)</span><br><span class="line">names = e.xpath(<span class="string">&#x27;//table[@class = &quot;players_table&quot;]//tr/td[2]/a/text()&#x27;</span>)</span><br><span class="line">teams = e.xpath(<span class="string">&#x27;//table[@class = &quot;players_table&quot;]//tr/td[3]/a/text()&#x27;</span>)</span><br><span class="line">scores = e.xpath(<span class="string">&#x27;//table[@class = &quot;players_table&quot;]//tr/td[4]/text()&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对应</span></span><br><span class="line">dataSet = []</span><br><span class="line"><span class="keyword">for</span> no, name, team, score <span class="keyword">in</span> <span class="built_in">zip</span>(nos, names, teams, scores):</span><br><span class="line">    dataSet.append([no, name, team, score])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;排名：<span class="subst">&#123;no&#125;</span>，球员：<span class="subst">&#123;name&#125;</span>,队名：<span class="subst">&#123;team&#125;</span>，得分：<span class="subst">&#123;score&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="comment"># 保存</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;NBA.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding = <span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> no, name, team, score <span class="keyword">in</span> <span class="built_in">zip</span>(nos, names, teams, scores):</span><br><span class="line">        f.write(<span class="string">f&#x27;排名：<span class="subst">&#123;no&#125;</span>，球员：<span class="subst">&#123;name&#125;</span>,队名：<span class="subst">&#123;team&#125;</span>，得分：<span class="subst">&#123;score&#125;</span>\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处可以使用爬pandas中的方法，将dataSet转为表格</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.DataFrame(dataSet[<span class="number">1</span>:], columns= [<span class="string">&#x27;排名&#x27;</span>,<span class="string">&#x27;球员&#x27;</span>,<span class="string">&#x27;队名&#x27;</span>,<span class="string">&#x27;得分&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><br><h2 id="七-Python连接数据库"><a href="#七-Python连接数据库" class="headerlink" title="七 Python连接数据库"></a>七 Python连接数据库</h2><p>用到第三方库：<code>mysql-connector</code>，其他的用到了再说</p><br><h2 id="八-爬虫框架【Scrapy】"><a href="#八-爬虫框架【Scrapy】" class="headerlink" title="八 爬虫框架【Scrapy】"></a>八 爬虫框架【Scrapy】</h2><blockquote><p>一套代码模板：获取数据【requests】、解析数据【BeautifulSoup】、提取数据、存储数据</p><p>常见爬虫框架：Scrapy、PySpider【Python环模型框架】、Crawley【侧重于提取数据的方式】、Portia【可视化】、Newspaper【提取新闻、文章等内容分析】</p><p>Scrapy使用pip安装，其安装之前需要先安装 <code>Twisted</code></p></blockquote><p>写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此<code>Scrapy</code>把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。</p><p><img src="/.io//p4.png"></p><h3 id="Scrapy框架模块功能："><a href="#Scrapy框架模块功能：" class="headerlink" title="Scrapy框架模块功能："></a>Scrapy框架模块功能：</h3><ol><li><code>Scrapy Engine（引擎）</code>：<code>Scrapy</code>框架的核心部分。负责在<code>Spider</code>和<code>ItemPipeline</code>、<code>Downloader</code>、<code>Scheduler</code>中间通信、传递数据等。</li><li><code>Spider（爬虫）</code>：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。</li><li><code>Scheduler（调度器）</code>：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。</li><li><code>Downloader（下载器）</code>：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。</li><li><code>Item Pipeline（管道）</code>：负责将<code>Spider（爬虫）</code>传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。</li><li><code>Downloader Middlewares（下载中间件）</code>：可以扩展下载器和引擎之间通信功能的中间件。</li><li><code>Spider Middlewares（Spider中间件）</code>：可以扩展引擎和爬虫之间通信功能的中间件。</li></ol><blockquote><p>省略具体的内容，因为大概率用不到。如果要用，要么找更好的教程，要么参考此<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1jM4y1j7rb/?p=148&spm_id_from=pageDriver&vd_source=ad866fe26d18693e4132a3c33f8fba36">视频教程</a></p></blockquote><br><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.2.0/dist/mindmap.min.css"></div><div class="reward-container"><div></div><button onclick='var qr=document.getElementById("qr");qr.style.display="none"===qr.style.display?"block":"none"'>打赏</button><div id="qr" style="display:none"><div style="display:inline-block"><img src="/images/wechatpay.png" alt="Moustache 微信支付"><p>微信支付</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"><strong>本文作者： </strong>Moustache</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://hammerzer.github.io/2024/01/09/python-crawler/" title="Python爬虫">https://hammerzer.github.io/2024/01/09/python-crawler/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i> Python</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2024/01/05/React-Hooks/" rel="prev" title="React-Hooks"><i class="fa fa-chevron-left"></i> React-Hooks</a></div><div class="post-nav-item"><a href="/2024/03/11/cpp-base-1/" rel="next" title="CPP-BASE-1">CPP-BASE-1 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="valine-comments"></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#CONTENT-OUTLINE"><span class="nav-number">1.</span> <span class="nav-text">CONTENT OUTLINE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E3%80%87-%E7%9B%AE%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">〇 目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80-%E7%88%AC%E8%99%AB%E5%89%8D%E5%A5%8F"><span class="nav-number">3.</span> <span class="nav-text">一 爬虫前奏</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9A"><span class="nav-number">3.1.</span> <span class="nav-text">什么是网络爬虫：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB%E5%92%8C%E8%81%9A%E7%84%A6%E7%88%AC%E8%99%AB"><span class="nav-number">3.2.</span> <span class="nav-text">通用爬虫和聚焦爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8Python%E5%86%99%E7%88%AC%E8%99%AB%E7%A8%8B%E5%BA%8F"><span class="nav-number">3.3.</span> <span class="nav-text">为什么用Python写爬虫程序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E5%85%B7"><span class="nav-number">3.4.</span> <span class="nav-text">准备工具</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C-http%E5%8D%8F%E8%AE%AE%E5%92%8CChrome%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7"><span class="nav-number">4.</span> <span class="nav-text">二 http协议和Chrome抓包工具</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFhttp%E5%92%8Chttps%E5%8D%8F%E8%AE%AE%EF%BC%9A"><span class="nav-number">4.1.</span> <span class="nav-text">什么是http和https协议：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9C%A8%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E5%8F%91%E9%80%81%E4%B8%80%E4%B8%AAhttp%E8%AF%B7%E6%B1%82%E7%9A%84%E8%BF%87%E7%A8%8B%EF%BC%9A"><span class="nav-number">4.2.</span> <span class="nav-text">在浏览器中发送一个http请求的过程：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#url%E8%AF%A6%E8%A7%A3"><span class="nav-number">4.3.</span> <span class="nav-text">url详解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E8%AF%B7%E6%B1%82%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">4.3.1.</span> <span class="nav-text">常用的请求方法：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0%EF%BC%9A"><span class="nav-number">4.3.2.</span> <span class="nav-text">请求头常见参数：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E5%93%8D%E5%BA%94%E7%8A%B6%E6%80%81%E7%A0%81%EF%BC%9A"><span class="nav-number">4.3.3.</span> <span class="nav-text">常见响应状态码：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Chrome%E6%8A%93%E5%8C%85%E5%B7%A5%E5%85%B7%EF%BC%9A"><span class="nav-number">4.3.4.</span> <span class="nav-text">Chrome抓包工具：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89-urllib%E5%BA%93"><span class="nav-number">5.</span> <span class="nav-text">三 urllib库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#urlopen%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="nav-number">5.1.</span> <span class="nav-text">urlopen函数：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlretrieve%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="nav-number">5.2.</span> <span class="nav-text">urlretrieve函数：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlencode%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="nav-number">5.3.</span> <span class="nav-text">urlencode函数：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse-qs%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="nav-number">5.4.</span> <span class="nav-text">parse_qs函数：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#urlparse%E5%92%8Curlsplit%EF%BC%9A"><span class="nav-number">5.5.</span> <span class="nav-text">urlparse和urlsplit：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#request-Request%E7%B1%BB%EF%BC%9A"><span class="nav-number">5.6.</span> <span class="nav-text">request.Request类：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ProxyHandler%E5%A4%84%E7%90%86%E5%99%A8%EF%BC%88%E4%BB%A3%E7%90%86%E8%AE%BE%E7%BD%AE%EF%BC%89"><span class="nav-number">5.7.</span> <span class="nav-text">ProxyHandler处理器（代理设置）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFcookie%EF%BC%9A"><span class="nav-number">5.8.</span> <span class="nav-text">什么是cookie：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#cookie%E7%9A%84%E6%A0%BC%E5%BC%8F%EF%BC%9A"><span class="nav-number">5.8.1.</span> <span class="nav-text">cookie的格式：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8cookielib%E5%BA%93%E5%92%8CHTTPCookieProcessor%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%EF%BC%9A"><span class="nav-number">5.9.</span> <span class="nav-text">使用cookielib库和HTTPCookieProcessor模拟登录：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#http-cookiejar%E6%A8%A1%E5%9D%97%EF%BC%9A"><span class="nav-number">5.9.1.</span> <span class="nav-text">http.cookiejar模块：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%99%BB%E5%BD%95%E4%BA%BA%E4%BA%BA%E7%BD%91%EF%BC%9A"><span class="nav-number">5.9.2.</span> <span class="nav-text">登录人人网：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98cookie%E5%88%B0%E6%9C%AC%E5%9C%B0%EF%BC%9A"><span class="nav-number">5.9.3.</span> <span class="nav-text">保存cookie到本地：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BDcookie%EF%BC%9A"><span class="nav-number">5.9.4.</span> <span class="nav-text">从本地加载cookie：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B-requests%E5%BA%93"><span class="nav-number">6.</span> <span class="nav-text">四 requests库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%92%8C%E6%96%87%E6%A1%A3%E5%9C%B0%E5%9D%80%EF%BC%9A"><span class="nav-number">6.1.</span> <span class="nav-text">安装和文档地址：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E9%80%81GET%E8%AF%B7%E6%B1%82%EF%BC%9A"><span class="nav-number">6.2.</span> <span class="nav-text">发送GET请求：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E9%80%81POST%E8%AF%B7%E6%B1%82%EF%BC%9A"><span class="nav-number">6.3.</span> <span class="nav-text">发送POST请求：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%EF%BC%9A"><span class="nav-number">6.4.</span> <span class="nav-text">使用代理：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cookie%EF%BC%9A"><span class="nav-number">6.5.</span> <span class="nav-text">cookie：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#session%EF%BC%9A"><span class="nav-number">6.6.</span> <span class="nav-text">session：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E4%B8%8D%E4%BF%A1%E4%BB%BB%E7%9A%84SSL%E8%AF%81%E4%B9%A6%EF%BC%9A"><span class="nav-number">6.7.</span> <span class="nav-text">处理不信任的SSL证书：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94-%E9%A1%B5%E9%9D%A2%E8%A7%A3%E6%9E%90%E5%92%8C%E6%95%B0%E6%8D%AE%E6%8F%90%E5%8F%96"><span class="nav-number">7.</span> <span class="nav-text">五 页面解析和数据提取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%B8%A4%E7%A7%8D%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="nav-number">7.1.</span> <span class="nav-text">1 两种数据处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-XPath%E8%AF%AD%E6%B3%95%E5%92%8Clxml%E6%A8%A1%E5%9D%97"><span class="nav-number">7.2.</span> <span class="nav-text">2 XPath语法和lxml模块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-XPath%E8%AF%AD%E6%B3%95"><span class="nav-number">7.3.</span> <span class="nav-text">3 XPath语法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E5%8F%96%E8%8A%82%E7%82%B9%EF%BC%9A"><span class="nav-number">7.3.1.</span> <span class="nav-text">选取节点：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%93%E8%AF%AD%EF%BC%9A"><span class="nav-number">7.3.2.</span> <span class="nav-text">谓语：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%9A%E9%85%8D%E7%AC%A6"><span class="nav-number">7.3.3.</span> <span class="nav-text">通配符</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%80%89%E5%8F%96%E5%A4%9A%E4%B8%AA%E8%B7%AF%E5%BE%84%EF%BC%9A"><span class="nav-number">7.3.4.</span> <span class="nav-text">选取多个路径：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%90%E7%AE%97%E7%AC%A6%EF%BC%9A"><span class="nav-number">7.3.5.</span> <span class="nav-text">运算符：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-lxml%E5%BA%93"><span class="nav-number">7.4.</span> <span class="nav-text">4 lxml库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%EF%BC%9A"><span class="nav-number">7.4.1.</span> <span class="nav-text">基本使用：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E6%96%87%E4%BB%B6%E4%B8%AD%E8%AF%BB%E5%8F%96html%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="nav-number">7.4.2.</span> <span class="nav-text">从文件中读取html代码：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9C%A8lxml%E4%B8%AD%E4%BD%BF%E7%94%A8XPath%E8%AF%AD%E6%B3%95%EF%BC%9A"><span class="nav-number">7.4.3.</span> <span class="nav-text">在lxml中使用XPath语法：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-BeautifulSoup4-%E5%BA%93"><span class="nav-number">7.5.</span> <span class="nav-text">5 BeautifulSoup4 库</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E5%92%8C%E6%96%87%E6%A1%A3%EF%BC%9A"><span class="nav-number">7.5.1.</span> <span class="nav-text">安装和文档：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%87%A0%E5%A4%A7%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94%EF%BC%9A"><span class="nav-number">7.5.2.</span> <span class="nav-text">几大解析工具对比：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%EF%BC%9A"><span class="nav-number">7.5.3.</span> <span class="nav-text">简单使用：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%9B%E4%B8%AA%E5%B8%B8%E7%94%A8%E7%9A%84%E5%AF%B9%E8%B1%A1"><span class="nav-number">7.5.4.</span> <span class="nav-text">四个常用的对象</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-Tag"><span class="nav-number">7.5.4.1.</span> <span class="nav-text">1. Tag</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-NavigableString"><span class="nav-number">7.5.4.2.</span> <span class="nav-text">2. NavigableString</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-BeautifulSoup"><span class="nav-number">7.5.4.3.</span> <span class="nav-text">3. BeautifulSoup</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-Comment"><span class="nav-number">7.5.4.4.</span> <span class="nav-text">4. Comment</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%81%8D%E5%8E%86%E6%96%87%E6%A1%A3%E6%A0%91%EF%BC%9A"><span class="nav-number">7.5.5.</span> <span class="nav-text">遍历文档树：</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-contents%E5%92%8Cchildren"><span class="nav-number">7.5.5.1.</span> <span class="nav-text">1. contents和children</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-strings-%E5%92%8C-stripped-strings"><span class="nav-number">7.5.5.2.</span> <span class="nav-text">2. strings 和 stripped_strings</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%90%9C%E7%B4%A2%E6%96%87%E6%A1%A3%E6%A0%91"><span class="nav-number">7.5.6.</span> <span class="nav-text">搜索文档树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-find%E5%92%8Cfind-all%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">7.5.6.1.</span> <span class="nav-text">1. find和find_all方法：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-select%E6%96%B9%E6%B3%95%EF%BC%9A"><span class="nav-number">7.5.6.2.</span> <span class="nav-text">2. select方法：</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B"><span class="nav-number">7.5.7.</span> <span class="nav-text">案例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AD-%E6%A1%88%E4%BE%8B"><span class="nav-number">8.</span> <span class="nav-text">六 案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E4%B8%80%EF%BC%9A%E7%99%BE%E5%BA%A6%E8%B4%B4%E5%90%A7%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD"><span class="nav-number">8.1.</span> <span class="nav-text">案例一：百度贴吧图片下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E4%BA%8C%EF%BC%9A%E7%88%AC%E5%8F%96%E9%85%B7%E7%8B%97%E9%9F%B3%E4%B9%90"><span class="nav-number">8.2.</span> <span class="nav-text">案例二：爬取酷狗音乐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E4%B8%89%EF%BC%9A%E6%B3%95%E6%8B%8D%E7%BD%91%E6%95%B0%E6%8D%AE%E8%8E%B7%E5%8F%96%E3%80%90%E6%97%A0%E6%B3%95%E8%BF%90%E8%A1%8C%E3%80%91"><span class="nav-number">8.3.</span> <span class="nav-text">案例三：法拍网数据获取【无法运行】</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E5%9B%9B%EF%BC%9A%E8%8E%B7%E5%8F%96%E8%99%8E%E6%89%91NBA%E6%95%B0%E6%8D%AE"><span class="nav-number">8.4.</span> <span class="nav-text">案例四：获取虎扑NBA数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%83-Python%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-number">9.</span> <span class="nav-text">七 Python连接数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%AB-%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6%E3%80%90Scrapy%E3%80%91"><span class="nav-number">10.</span> <span class="nav-text">八 爬虫框架【Scrapy】</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E6%A8%A1%E5%9D%97%E5%8A%9F%E8%83%BD%EF%BC%9A"><span class="nav-number">10.1.</span> <span class="nav-text">Scrapy框架模块功能：</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Moustache" src="/images/180-180.png"><p class="site-author-name" itemprop="name">Moustache</p><div class="site-description" itemprop="description">我是小胡子</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">78</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">34</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/hammerzer" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hammerzer" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:stellar_lzu@163.com" title="E-Mail → mailto:stellar_lzu@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div class="cc-license motion-element" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Chase</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">1.9m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">29:01</span></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script size="300" alpha="0.4" zindex="-1" src="//cdn.jsdelivr.net/gh/theme-next/theme-next-canvas-ribbon@1/canvas-ribbon.js"></script><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'JRehDoQ6pHXV1zKg09AMNLFt-gzGzoHsz',
      appKey     : 'cRAt4W15KiQdrIuHlQrRrtIl',
      placeholder: "Just go go",
      avatar     : 'wavatar',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});</script></body></html>